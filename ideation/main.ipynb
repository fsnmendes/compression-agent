{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (0.52.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anthropic) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Collecting exa_py\n",
      "  Downloading exa_py-1.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from exa_py) (0.28.1)\n",
      "Collecting openai>=1.48 (from exa_py)\n",
      "  Downloading openai-1.82.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from exa_py) (2.11.5)\n",
      "Collecting pytest-mock>=3.14.0 (from exa_py)\n",
      "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting requests>=2.32.3 (from exa_py)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from exa_py) (4.13.2)\n",
      "Requirement already satisfied: anyio in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx>=0.28.1->exa_py) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx>=0.28.1->exa_py) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx>=0.28.1->exa_py) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpx>=0.28.1->exa_py) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.28.1->exa_py) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from openai>=1.48->exa_py) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from openai>=1.48->exa_py) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from openai>=1.48->exa_py) (1.3.1)\n",
      "Collecting tqdm>4 (from openai>=1.48->exa_py)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic>=2.10.6->exa_py) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic>=2.10.6->exa_py) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pydantic>=2.10.6->exa_py) (0.4.1)\n",
      "Collecting pytest>=6.2.5 (from pytest-mock>=3.14.0->exa_py)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting iniconfig (from pytest>=6.2.5->pytest-mock>=3.14.0->exa_py)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: packaging in /Users/javierportet/dev/compression/compression-agent/.venv/lib/python3.13/site-packages (from pytest>=6.2.5->pytest-mock>=3.14.0->exa_py) (25.0)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=6.2.5->pytest-mock>=3.14.0->exa_py)\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.3->exa_py)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.3->exa_py)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading exa_py-1.13.1-py3-none-any.whl (28 kB)\n",
      "Downloading openai-1.82.0-py3-none-any.whl (720 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: urllib3, tqdm, pluggy, iniconfig, charset-normalizer, requests, pytest, pytest-mock, openai, exa_py\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10/10\u001b[0m [exa_py] 8/10\u001b[0m [openai]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed charset-normalizer-3.4.2 exa_py-1.13.1 iniconfig-2.1.0 openai-1.82.0 pluggy-1.6.0 pytest-8.3.5 pytest-mock-3.14.0 requests-2.32.3 tqdm-4.67.1 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "!pip install exa_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"EXA_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEATOR_PROMPT = \"\"\"\n",
    "You are a highly esteemed expert researcher and visionary innovator, specializing in information theory, machine learning, and data compression. A groundbreaking research paper, \"Understanding is Compression,\" has just been published, introducing the LMCompress framework. Your task is to deeply analyze this breakthrough and propose 3-5 specific, actionable, and high-impact novel ideas that could exploit its core principles.\n",
    "Breakthrough Summary (Paper: \"Understanding is Compression\"):\n",
    "The paper presents LMCompress, a novel lossless compression framework. Unlike traditional methods, LMCompress uses large language models (LLMs) as data understanding engines rather than direct encoders. Inspired by Solomonoff induction (a theoretically optimal but uncomputable prediction method based on understanding), the core argument is that superior understanding leads to superior compression.\n",
    "LMCompress Mechanism:\n",
    "Tokenization: Input data (text, image, video, audio) is tokenized.\n",
    "LLM Probability Estimation: A generative LLM estimates the probability distribution for each token.\n",
    "Arithmetic Coding: Probabilities from the LLM are used for arithmetic coding.\n",
    "Domain Specialization: Models fine-tuned on specific domains (e.g., iGPT for images, LLaMA for text/audio) enhance \"understanding\" and compression for that data type.\n",
    "Key Breakthrough Aspects:\n",
    "Exceeding Shannon Limits: LMCompress surpasses traditional Shannon-based compression limits by leveraging LLMs to approximate uncomputable data patterns.\n",
    "Superior Performance: It consistently outperforms state-of-the-art methods (doubling/quadrupling compression ratios for images vs. JPEG-XL, audio vs. FLAC, video vs. H.264, text vs. bz2).\n",
    "Kolmogorov Paradigm Shift: This marks a shift from frequency-based, computable compression to model-based, semantic compression. The essence is understanding and modeling the source, not just statistical redundancy removal.\n",
    "Implications: Significant potential for future communication systems (e.g., 6G) by mitigating bandwidth limits through deeper data understanding at both ends.\n",
    "Your Task: Generate Novel Ideas\n",
    "Based on this breakthrough, please suggest 3-5 novel and important ideas. For each idea, provide:\n",
    "Idea Title: A concise, descriptive title.\n",
    "Core Concept: A clear explanation of the idea (1-2 paragraphs).\n",
    "Exploitation of Breakthrough: Specifically detail how this idea leverages one or more core principles of LMCompress (e.g., semantic understanding, domain-specific LLMs, exceeding Shannon limits, the Kolmogorov paradigm).\n",
    "Potential Impact & Importance: Describe the potential benefits, why this idea is important, and what significant problems it could solve or new opportunities it could create.\n",
    "Key Challenges/Research Questions: Briefly outline potential hurdles, open research questions, or necessary next steps for realizing this idea.\n",
    "Focus on ideas that are:\n",
    "Truly Novel: Go beyond obvious next steps. Think about cross-disciplinary applications or entirely new use-cases.\n",
    "High-Impact: Could lead to significant advancements, new products/services, or solve major existing problems.\n",
    "Actionable (Conceptually): While potentially ambitious, the core concept should be clear enough that one could envision a research or development path.\n",
    "Consider the implications of \"understanding as compression\" in diverse fields. Think about how LLM-driven semantic compression could revolutionize areas beyond simple data storage and transmission.\n",
    "\n",
    "For example, one idea could be to create \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITIQUER_PROMPT = \"\"\"You are a highly critical and discerning research program evaluator with deep expertise in AI, compression, and information theory. You have been presented with several research ideas that aim to exploit the recent breakthrough described in the \"Understanding is Compression\" paper (summarized below).\n",
    "\n",
    "**Breakthrough Summary (LMCompress):**\n",
    "The paper \"Understanding is Compression\" introduces LMCompress, a lossless compression framework using LLMs as data understanding engines. It tokenizes data, uses a generative LLM to estimate token probabilities, and applies arithmetic coding. By fine-tuning LLMs on specific domains, it significantly outperforms traditional methods (JPEG-XL, FLAC, H.264, bz2), suggesting a shift to a \"Kolmogorov paradigm\" where understanding and modeling the source, not just redundancy removal, is key. This has major implications for fields like 6G communication by mitigating bandwidth limits through deeper data understanding.\n",
    "\n",
    "Your task is to critically evaluate each proposed research idea against the following specific metrics. For each idea, provide a structured evaluation:\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "1.  **NOVELTY (Score 1-5, 5 being highly novel):**\n",
    "    *   How original is this idea?\n",
    "    *   Does it propose a genuinely new application, methodology, or theoretical extension of the LMCompress breakthrough?\n",
    "    *   Or, is it an obvious next step, a minor tweak to existing work, or a rehash of old concepts merely relabeled with \"LLM compression\"?\n",
    "    *   **Justification required.**\n",
    "\n",
    "2.  **IMPORTANCE (Score 1-5, 5 being critically important):**\n",
    "    *   What is the potential impact of this idea if successfully realized?\n",
    "    *   Would it solve a significant problem, open up major new capabilities, or substantially advance our understanding or application of semantic compression?\n",
    "    *   Consider its practical, theoretical, or societal relevance.\n",
    "    *   **Justification required.**\n",
    "\n",
    "3.  **FEASIBILITY & VERIFIABILITY (Combined Score 1-5, 5 being highly feasible & verifiable):**\n",
    "    *   **Feasibility:** Can a proof-of-concept (PoC) or a minimal viable demonstration for this idea be reasonably implemented with current or near-future resources and technology (assuming the LMCompress capability exists)?\n",
    "    *   **Verifiability:** Can the success or failure of this idea be clearly demonstrated and validated *quickly*?\n",
    "        *   This means an output that can be checked by code (e.g., a compression ratio achieved, a task completed, a specific measurable outcome).\n",
    "        *   OR, by a human inspecting a tangible result (e.g., a decoded image's quality, a summarized text's accuracy, a correctly derived mathematical step, a functioning compressed communication channel) within minutes.\n",
    "        *   Is the proposed verification method robust and unambiguous?\n",
    "    *   **Justification required.**\n",
    "\n",
    "4.  **RELIANCE ON SEMANTIC COMPRESSION (Score 1-5, 5 meaning critically reliant):**\n",
    "    *   Is the *efficient compression of semantic data* (as enabled by LMCompress) the *core* enabler or the *primary bottleneck* that this idea addresses?\n",
    "    *   In other words, was this idea largely impractical, significantly less effective, or even impossible *before* a breakthrough like LMCompress, specifically due to the inability to efficiently compress or leverage the semantic understanding of data like text, images, audio, or complex symbolic structures?\n",
    "    *   If the idea could have been pursued almost as effectively with older compression techniques or non-compression-focused AI, it scores lower here.\n",
    "    *   **Justification required.**\n",
    "\n",
    "**Overall Assessment:**\n",
    "Based on the individual scores, provide a brief overall recommendation for each idea (e.g., \"Highly Promising,\" \"Worth Exploring,\" \"Needs Refinement,\" \"Low Priority\").\n",
    "\n",
    "**Input Format:**\n",
    "You will be given a list of research ideas.\n",
    "\n",
    "**Output Format:**\n",
    "For each idea, provide:\n",
    "Idea #X: [Name of Idea, if provided, or a brief title you assign]\n",
    "-   Novelty: [Score]/5. Justification: [...]\n",
    "-   Importance: [Score]/5. Justification: [...]\n",
    "-   Feasibility & Verifiability: [Score]/5. Justification: [...]\n",
    "-   Reliance on Semantic Compression: [Score]/5. Justification: [...]\n",
    "-   Overall Assessment: [...]\n",
    "---\n",
    "\n",
    "Please proceed to evaluate the ideas provided.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "\n",
    "from exa_py import Exa\n",
    "exa = Exa(api_key = os.environ[\"EXA_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-opus-4-0\",\n",
    "        max_tokens=4000,\n",
    "        temperature=1.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": IDEATOR_PROMPT\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critique(ideas: str):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-sonnet-4-0\",\n",
    "        max_tokens=4000,\n",
    "        temperature=1.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": CRITIQUER_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here are the results you should evaluate: {ideas}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(output: any):\n",
    "    try:\n",
    "        return Markdown(output[0].text)\n",
    "    except:\n",
    "        return Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(citations=None, text='## Evaluation of Research Ideas Based on \"Understanding is Compression\"\\n\\n**Idea #1: Semantic Time Capsules: Ultra-Long-Term Data Preservation Through Understanding**\\n\\n- **Novelty: 4/5.** Justification: This is a genuinely novel application that goes beyond obvious compression use cases. While digital preservation exists, the concept of storing hierarchical semantic representations as a \"Rosetta Stone\" for future AI systems is original and non-trivial. It\\'s not just \"apply LLM compression to archives\" but a fundamental rethinking of preservation methodology.\\n\\n- **Importance: 5/5.** Justification: Addresses the critical \"digital dark age\" problem that threatens human knowledge preservation. The potential to preserve cultural artifacts, scientific data, and historical records in a format interpretable across technological evolution is of paramount importance to civilization. This could fundamentally solve format obsolescence.\\n\\n- **Feasibility & Verifiability: 2/5.** Justification: While a proof-of-concept could be built (compress data with semantic layers, attempt reconstruction), the core promise‚Äîinterpretability across \"vastly different future AI architectures\"‚Äîis inherently unverifiable in reasonable timeframes. The true test requires decades/centuries. Current validation methods would be inadequate proxies for the actual use case.\\n\\n- **Reliance on Semantic Compression: 5/5.** Justification: This idea is entirely dependent on semantic compression capabilities. Traditional compression preserves bits, not meaning. Without LMCompress-style semantic understanding, the concept of storing \"understanding\" alongside data and creating cross-generational interpretability is impossible. This was not feasible before semantic compression breakthroughs.\\n\\n- **Overall Assessment: Worth Exploring** (despite verification challenges, the importance and novelty justify research investment)\\n\\n---\\n\\n**Idea #2: Compression-Based Anomaly Detection and Security (CADS)**\\n\\n- **Novelty: 3/5.** Justification: While the specific application of compression ratios as anomaly indicators has some novelty, compression-based anomaly detection exists in literature. The insight about \"understanding\" leading to compression differentials is valuable but somewhat incremental. The domain-specific fine-tuning angle adds originality but doesn\\'t constitute a breakthrough conceptual leap.\\n\\n- **Importance: 4/5.** Justification: Anomaly detection is critical for cybersecurity, industrial monitoring, and fraud detection. A unified framework that adapts to evolving patterns without explicit programming could have significant practical impact. However, the space is already well-served by existing ML approaches, limiting revolutionary potential.\\n\\n- **Feasibility & Verifiability: 5/5.** Justification: Highly feasible‚Äîcan be implemented with current resources by training domain-specific models and measuring compression ratios on normal vs. anomalous data. Verification is straightforward: inject known anomalies, measure detection rates, compare compression ratios. Results are immediately quantifiable and interpretable.\\n\\n- **Reliance on Semantic Compression: 3/5.** Justification: While semantic compression enhances this approach, anomaly detection via compression ratios was possible with traditional methods. The domain-specific understanding adds value but isn\\'t the primary bottleneck. Classical compression + ML already enabled similar approaches, though less effectively.\\n\\n- **Overall Assessment: Worth Exploring** (practical value and feasibility outweigh limited novelty)\\n\\n---\\n\\n**Idea #3: Differential Privacy Through Semantic Compression (DPSC)**\\n\\n- **Novelty: 4/5.** Justification: This represents a novel intersection of semantic compression and privacy preservation. The insight that semantic understanding can distinguish between compressible patterns and incompressible personal identifiers is creative and non-obvious. It\\'s not merely applying existing differential privacy techniques but proposing a fundamentally new mechanism.\\n\\n- **Importance: 4/5.** Justification: Privacy-preserving data sharing is increasingly critical in healthcare, finance, and research. A mechanism that naturally preserves utility while protecting privacy could enable breakthrough collaborations. However, formal privacy guarantees remain unclear, limiting immediate impact.\\n\\n- **Feasibility & Verifiability: 3/5.** Justification: A proof-of-concept is feasible‚Äîtrain models to compress semantic content while treating personal identifiers as noise. However, verification is complex: proving privacy guarantees requires formal analysis, and measuring utility preservation vs. privacy protection involves subjective assessments. Quick validation of privacy claims is challenging.\\n\\n- **Reliance on Semantic Compression: 4/5.** Justification: The core insight‚Äîdistinguishing between generalizable semantic patterns and individual-specific information through compression‚Äîrequires semantic understanding capabilities. Traditional compression couldn\\'t meaningfully separate semantic utility from personal identifiers. This approach was largely impractical before semantic compression.\\n\\n- **Overall Assessment: Worth Exploring** (novel approach to important problem, despite verification challenges)\\n\\n---\\n\\n**Idea #4: Cognitive Bandwidth Optimization for Brain-Computer Interfaces**\\n\\n- **Novelty: 5/5.** Justification: Applying semantic compression to neural signals represents a genuinely novel and ambitious extension. The concept of compressing \"thoughts themselves\" through understanding rather than treating neural activity as raw signals is highly original. This pushes the Kolmogorov paradigm into uncharted territory.\\n\\n- **Importance: 5/5.** Justification: Could revolutionize BCIs, enabling high-fidelity brain-computer communication with transformative implications for prosthetics, paralysis treatment, and human-computer interaction. The bandwidth limitations in current BCIs are a fundamental bottleneck that this could address.\\n\\n- **Feasibility & Verifiability: 2/5.** Justification: Extremely challenging feasibility due to limited, noisy neural data and poor understanding of neural encoding. While a basic proof-of-concept might compress simple neural patterns, demonstrating \"semantic compression of thoughts\" is far beyond current capabilities. Verification requires neural recording infrastructure and subjective experience assessment that\\'s difficult to quantify quickly.\\n\\n- **Reliance on Semantic Compression: 5/5.** Justification: Entirely dependent on semantic compression capabilities. Traditional compression of neural signals treats them as raw data without understanding their semantic content. The proposed compression of \"thoughts and intentions\" is impossible without semantic understanding of neural patterns.\\n\\n- **Overall Assessment: Needs Refinement** (extremely high importance and novelty, but feasibility concerns require more realistic near-term milestones)\\n\\n---\\n\\n**Idea #5: Cross-Modal Semantic Bridges (CMSB)**\\n\\n- **Novelty: 4/5.** Justification: The concept of cross-modal compression through shared semantic spaces is novel and creative. While cross-modal representation learning exists, using it specifically for compression that \"transcends original data format\" represents an original application of the semantic compression principle.\\n\\n- **Importance: 4/5.** Justification: Could revolutionize bandwidth-constrained communication and accessibility technology. Enabling efficient cross-modal information transfer has significant applications in space communication, disaster scenarios, and assistive technology. However, the practical impact depends heavily on semantic fidelity preservation.\\n\\n- **Feasibility & Verifiability: 4/5.** Justification: Feasible with current multimodal AI capabilities‚Äîtrain models to create shared semantic representations across modalities, compress through cross-modal encoding, measure reconstruction quality. Verification is straightforward: compress visual scenes to text-like representations, reconstruct as images, measure fidelity through both automated metrics and human evaluation.\\n\\n- **Reliance on Semantic Compression: 4/5.** Justification: Critically relies on semantic understanding to create meaningful cross-modal mappings. Traditional compression operates within single modalities and cannot achieve the proposed cross-modal compression. The semantic understanding of content across different representations is essential and was not effectively possible before semantic compression breakthroughs.\\n\\n- **Overall Assessment: Highly Promising** (strong scores across all metrics with good feasibility)\\n\\n---\\n\\n**Summary Ranking:**\\n1. **Highly Promising:** Cross-Modal Semantic Bridges\\n2. **Worth Exploring:** Semantic Time Capsules, CADS, DPSC\\n3. **Needs Refinement:** Cognitive Bandwidth Optimization (feasibility concerns despite high potential)', type='text')]\n"
     ]
    }
   ],
   "source": [
    "ideas = generate()\n",
    "eval = critique(ideas)\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Novel Ideas Based on \"Understanding is Compression\"\n",
       "\n",
       "### 1. **Semantic Time Capsules: Ultra-Long-Term Data Preservation Through Understanding**\n",
       "\n",
       "**Core Concept:**\n",
       "Traditional long-term data storage faces format obsolescence and media degradation. By leveraging LMCompress principles, we can create \"semantic time capsules\" that store not just compressed data, but the semantic understanding of that data. Instead of preserving raw bits that may become unreadable as formats evolve, we preserve the high-level semantic representation that can be reconstructed by future AI systems, even if the original encoding schemes are forgotten. The system would compress data using multiple layers of understanding - from raw signal to conceptual meaning - creating a hierarchy of semantic representations that future systems could access at various levels of abstraction.\n",
       "\n",
       "**Exploitation of Breakthrough:**\n",
       "This directly leverages LMCompress's core insight that understanding enables superior compression. By storing the semantic model alongside compressed data, we create a self-describing archive that carries its own \"Rosetta Stone.\" The domain-specific understanding principle means we could create specialized preservation models for different cultural artifacts, scientific data, or historical records. This transcends Shannon limits by focusing on meaning preservation rather than bit-perfect reproduction.\n",
       "\n",
       "**Potential Impact & Importance:**\n",
       "This could revolutionize digital preservation for libraries, museums, and scientific institutions. Critical human knowledge could be preserved for millennia in a format that remains interpretable even as technology evolves. It addresses the \"digital dark age\" problem where future generations might lose access to digital heritage due to format obsolescence.\n",
       "\n",
       "**Key Challenges/Research Questions:**\n",
       "- How to ensure semantic models remain interpretable across vastly different future AI architectures?\n",
       "- What level of semantic abstraction provides optimal balance between compression and future interpretability?\n",
       "- How to validate preservation quality when the reconstruction might occur centuries later?\n",
       "\n",
       "### 2. **Compression-Based Anomaly Detection and Security (CADS)**\n",
       "\n",
       "**Core Concept:**\n",
       "Since LMCompress achieves compression through deep understanding, deviations from expected compression ratios reveal anomalies in data that the model doesn't \"understand.\" This principle can be weaponized for security: a domain-specific LLM trained on normal system behavior would compress typical logs, network traffic, or sensor data efficiently. Anomalous events - whether from cyberattacks, system failures, or data poisoning - would compress poorly, creating a sensitive, real-time anomaly detection system. The compression ratio itself becomes a continuous health/security metric.\n",
       "\n",
       "**Exploitation of Breakthrough:**\n",
       "This exploits the fundamental connection between understanding and compression. The Kolmogorov complexity insight means that data conforming to learned patterns compresses well, while outliers resist compression. Domain specialization allows fine-tuned models for specific systems, making the approach highly sensitive to context-specific anomalies.\n",
       "\n",
       "**Potential Impact & Importance:**\n",
       "This could provide a unified framework for anomaly detection across cybersecurity, industrial IoT, healthcare monitoring, and financial fraud detection. Unlike traditional rule-based systems, it would automatically adapt to evolving normal behavior patterns and detect novel attack vectors without explicit programming.\n",
       "\n",
       "**Key Challenges/Research Questions:**\n",
       "- How to distinguish between benign evolution of normal behavior and true anomalies?\n",
       "- What's the optimal balance between model complexity and real-time compression performance?\n",
       "- How to prevent adversaries from crafting inputs that appear normal to the compression model?\n",
       "\n",
       "### 3. **Differential Privacy Through Semantic Compression (DPSC)**\n",
       "\n",
       "**Core Concept:**\n",
       "LMCompress's ability to capture semantic understanding while discarding noise can be repurposed for privacy preservation. By training domain-specific models to understand and compress only the non-private semantic content of data, we can create a compression layer that naturally strips personally identifiable information while preserving utility. The model would learn to maximize compression of semantically relevant patterns while treating personal identifiers as incompressible noise, effectively creating a learned differential privacy mechanism.\n",
       "\n",
       "**Exploitation of Breakthrough:**\n",
       "This leverages the semantic understanding principle to distinguish between generalizable patterns (compressible) and individual-specific information (incompressible). The domain specialization aspect allows tailored privacy models for different data types - medical records, financial transactions, or behavioral data.\n",
       "\n",
       "**Potential Impact & Importance:**\n",
       "This could enable unprecedented data sharing for research and analytics while preserving privacy. Healthcare institutions could share compressed medical data that retains diagnostic patterns but removes patient identities. Tech companies could analyze user behavior patterns without storing personal data.\n",
       "\n",
       "**Key Challenges/Research Questions:**\n",
       "- How to formally prove privacy guarantees of semantic compression?\n",
       "- Can adversaries exploit model understanding to reconstruct private information?\n",
       "- How to balance utility preservation with privacy protection across different domains?\n",
       "\n",
       "### 4. **Cognitive Bandwidth Optimization for Brain-Computer Interfaces**\n",
       "\n",
       "**Core Concept:**\n",
       "As brain-computer interfaces (BCIs) advance, bandwidth limitations between neural signals and digital systems become critical. By applying LMCompress principles with models trained on neural patterns, we can achieve unprecedented compression of brain signals by understanding their semantic content rather than treating them as raw data. The system would learn individual neural \"languages\" and compress thoughts, intentions, and sensory experiences at the semantic level, dramatically increasing effective BCI bandwidth.\n",
       "\n",
       "**Exploitation of Breakthrough:**\n",
       "This pushes the Kolmogorov paradigm to its limit - compressing not just data but thoughts themselves through deep understanding. Domain specialization would involve personalized models that learn individual neural patterns. It transcends Shannon limits by leveraging the semantic structure of neural activity rather than its raw signal properties.\n",
       "\n",
       "**Potential Impact & Importance:**\n",
       "This could enable real-time, high-fidelity brain-computer communication, revolutionizing prosthetics, communication for paralyzed individuals, and eventually direct brain-to-brain communication. It could make the difference between BCIs that transmit simple commands versus those that convey complex thoughts and emotions.\n",
       "\n",
       "**Key Challenges/Research Questions:**\n",
       "- How to train models on the limited and noisy neural data currently available?\n",
       "- What ethical frameworks govern compression of human thoughts?\n",
       "- How to ensure neural compression preserves subjective experience fidelity?\n",
       "\n",
       "### 5. **Cross-Modal Semantic Bridges (CMSB)**\n",
       "\n",
       "**Core Concept:**\n",
       "Since LMCompress demonstrates that understanding enables compression across different modalities (text, image, audio, video), we can create \"semantic bridges\" that compress data from one modality into the semantic space of another. This would enable revolutionary bandwidth-efficient communication where, for example, complex visual scenes are compressed into text-like semantic representations that can be transmitted efficiently and reconstructed as images at the destination. The key insight is that cross-modal understanding allows compression that transcends the original data format.\n",
       "\n",
       "**Exploitation of Breakthrough:**\n",
       "This exploits LMCompress's multi-modal capability and the principle that semantic understanding transcends specific data representations. It leverages domain-specific models to create shared semantic spaces between modalities, enabling compression ratios impossible within single-modality constraints.\n",
       "\n",
       "**Potential Impact & Importance:**\n",
       "This could revolutionize communication for bandwidth-constrained scenarios like deep space missions, submarine communications, or disaster zones. It could enable new forms of accessible technology where visual information is efficiently encoded for blind users or audio for deaf users, all while maintaining semantic fidelity.\n",
       "\n",
       "**Key Challenges/Research Questions:**\n",
       "- How to maintain semantic fidelity across radical modality transformations?\n",
       "- What universal semantic primitives exist across different sensory modalities?\n",
       "- How to handle culturally-specific semantic interpretations in cross-modal compression?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ideas[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_papers(query: str):\n",
    "    result = exa.search_and_contents(\n",
    "        query,\n",
    "        text = True,\n",
    "        type = \"neural\",\n",
    "        category = \"research paper\",\n",
    "        summary = True,\n",
    "        num_results = 10)\n",
    "    res = \"\"\n",
    "    for r in result.results:\n",
    "        res += f\"## {r.title}\\n{r.summary}\\n\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Understanding is Compression\n",
       "The paper \"Understanding is Compression\" (arXiv:2407.07723) introduces LMCompress, a new lossless data compression method based on large language models. It achieves significantly better compression ratios than existing methods, doubling the ratios of JPEG-XL for images, FLAC for audios, and H264 for videos, and tripling or quadrupling the ratio of bz2 for texts. The core idea is that better understanding of data by large models leads to better compression.\n",
       "\n",
       "\n",
       "## \n",
       "This paper \"Language Modeling is Compression\" explores the connection between large language models and lossless compression. It demonstrates that large language models can be powerful general-purpose predictors and compressors, even outperforming domain-specific compressors on data types they weren't explicitly trained on (e.g., compressing ImageNet patches and LibriSpeech samples). The paper also discusses how the prediction-compression equivalence allows using any compressor to build a conditional generative model and provides insights into scaling laws, tokenization, and in-context learning through the lens of compression. The use of Transformer-based compression with arithmetic coding is highlighted, building upon recent state-of-the-art results in both online and offline settings.\n",
       "\n",
       "\n",
       "## Modality-Agnostic Variational Compression of Implicit Neural Representations\n",
       "The paper \"Modality-Agnostic Variational Compression of Implicit Neural Representations\" introduces a new modality-agnostic neural compression algorithm (VC-INR) using Implicit Neural Representations (INRs). It bridges latent coding and sparsity, creating compact latent representations mapped to a soft gating mechanism for subnetwork selection. The algorithm optimizes the rate/distortion trade-off in a modality-agnostic space using neural compression. Experiments demonstrate strong performance across diverse modalities (images, climate data, 3D shapes/scenes, audio, video), outperforming codecs like JPEG 2000, MP3, and AVC/HEVC in their respective domains. The authors advocate for modality-agnosticism in compression, suggesting a single algorithmic workbench for various data types represented by coordinate and feature spaces.\n",
       "\n",
       "\n",
       "## Meta-Learning Sparse Compression Networks\n",
       "The paper \"Meta-Learning Sparse Compression Networks\" introduces advancements in data compression using Implicit Neural Representations (INRs). It addresses scalability issues by employing network sparsification techniques and introducing a method for sparsification within meta-learning algorithms. The approach demonstrates improved compression and reduced computational cost for learning INRs across various data modalities, including images, manifolds, 3D shapes, and scenes. The core idea involves adapting a meta-learned initialization sparsely, encoding data items with a small subset of changes, thus reducing compression cost and compression time.\n",
       "\n",
       "\n",
       "## Multi-Modality Deep Network for Extreme Learned Image Compression\n",
       "This paper introduces a multi-modality deep network for extreme learned image compression. It addresses the limitations of single-modality compression methods at extremely low bitrates, which often suffer from blur and semantic loss. The proposed method uses text as prior information to guide image compression, leveraging semantic information for improved performance. The network incorporates image-text attention and image-request complement modules for better feature fusion and employs a multimodal semantic-consistent loss function. Experiments, including a user study, demonstrate that the method achieves visually pleasing results at very low bitrates, comparable to or better than state-of-the-art methods operating at 2x to 4x higher bitrates. The approach is inspired by text-to-image synthesis and aims to leverage the semantic information in text to aid image compression, particularly at low bitrates where traditional methods struggle.\n",
       "\n",
       "\n",
       "## \n",
       "AMGC presents a new algorithm for compressing reads in FASTQ files, addressing the pressing need for efficient storage of genomic data generated by Next-Generation Sequencing (NGS) technologies. The algorithm focuses on optimizing the compression of matching positions of reads, mismatched base positions, and matching failed reads. Experiments show AMGC outperforms existing methods, achieving an average 81.23% gain in compression ratio compared to the next best compressor.\n",
       "\n",
       "\n",
       "## Generalized Octave Convolutions for Learned Multi-Frequency Image Compression\n",
       "This paper introduces a new learned image compression approach using generalized octave convolutions to factorize latent image representations into high and low frequencies. The method aims to improve compression rate and reconstruction quality by representing low frequencies at lower resolutions and enabling communication between high and low frequencies. The proposed scheme reportedly outperforms standard codecs and learning-based methods in PSNR and MS-SSIM metrics, establishing a new state-of-the-art for learned image compression. It builds upon recent advances in learned image compression that have shown potential to outperform standard codecs by using context-adaptive entropy approaches.\n",
       "\n",
       "\n",
       "## Random-Access Neural Compression of Material Textures\n",
       "This NVIDIA Research publication introduces a novel neural compression technique for material textures called \"Random-Access Neural Compression of Material Textures.\" It achieves low bitrate compression (16x more texels) with better image quality than AVIF and JPEG XL, while also enabling on-demand, real-time decompression with random access, similar to block texture compression on GPUs. The method compresses multiple material textures and their mipmap chains together, using a small neural network optimized for each material to decompress them. The research was presented at SIGGRAPH 2023 and received an honorable mention.\n",
       "\n",
       "\n",
       "## Ultra High Fidelity Image Compression with ùìÅ‚àû-constrained Encoding and Deep Decoding\n",
       "The article \"Ultra High Fidelity Image Compression with ùìÅ‚àû-constrained Encoding and Deep Decoding\" discusses a new image compression system of ‚àû-constrained encoding and deep soft decoding (‚àû-ED 2 ) that achieves major progress in ‚àû-constrained image coding after two decades, by developing a novel CNN-based soft ‚àû -constrained decoding method. The new method uses a restoration CNN called the ‚àû -SDNet to map a conventionally decoded image to the latent image, enforcing a tight error bound on a per pixel basis. The ‚àû -ED 2 approach beats the best of existing lossy image compression methods (e.g., BPG, WebP, etc.) not only in ‚àû but also in 2 error metric and perceptual quality, for bit rates near the threshold of perceptually transparent reconstruction. The new compression system has a low-complexity real-time encoder and a cascade decoder consisting of a fast initial decoder and an optional CNN soft decoder.\n",
       "\n",
       "## \n",
       "This paper introduces a new learned lossless image compression method based on interpolation, which aims to improve upon existing scale-based auto-regressive models. The method achieves comparable or better compression performance while using significantly fewer neural network parameters and requiring less computational complexity for encoding and decoding. Key innovations include sharing interpolator neural networks across different scales, using separate neural networks for different parameters of the probability distribution model, and processing in the YCoCg-R color space. The method is available at https://github.com/metu-kamisli/LLICTI.\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(retrieve_papers(\"new compression breakthroughs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = exa.search_and_contents(\n",
    "  \"new compression breakthroughs\",\n",
    "  text = True,\n",
    "  type = \"neural\",\n",
    "  category = \"research paper\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(url='https://arxiv.org/abs/2407.07723', id='https://arxiv.org/abs/2407.07723', title='Understanding is Compression', score=0.4048426151275635, published_date='2024-08-21T02:45:36.000Z', author='Li; Ziguang; Huang; Chao; Wang; Xuliang; Hu; Haibo; Wyeth; Cole; Bu; Dongbo; Yu; Quan; Gao; Wen; Xingwu; Ming', image=None, favicon=None, subpages=None, extras=None, text=\"\\n View PDF \\n HTML (experimental) \\nWe have previously shown all understanding or learning are compression, under reasonable assumptions. In principle, better understanding of data should improve data compression. Traditional compression methodologies focus on encoding frequencies or some other computable properties of data. Large language models approximate the uncomputable Solomonoff distribution, opening up a whole new avenue to justify our theory.\\n Under the new uncomputable paradigm, we present LMCompress based on the understanding of data using large models. LMCompress has significantly better lossless compression ratios than all other lossless data compression methods, doubling the compression ratios of JPEG-XL for images, FLAC for audios and H264 for videos, and tripling or quadrupling the compression ratio of bz2 for texts. The better a large model understands the data, the better LMCompress compresses.\\n \\n \\n Submission history From: Xingwu Liu [ view email] [v1] \\n Mon, 24 Jun 2024 03:58:11 UTC (250 KB) \\n ||||I|||| Skip to main content\\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\\n > cs > arXiv:2407.07723\\n\\n Help | Advanced Search\\n\\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\\n Search\\n GO\\n\\n quick links\\n\\n * Login\\n * Help Pages\\n * About\\n\\n Computer Science > Information Theory\\n\\n arXiv:2407.07723 (cs)\\n [Submitted on 24 Jun 2024]\\n\\n Title: Understanding is Compression\\n\\n Authors: Ziguang Li, Chao Huang, Xuliang Wang, Haibo Hu, Cole Wyeth, Dongbo Bu, Quan Yu, Wen Gao, Xingwu Liu, Ming Li\\n View a PDF of the paper titled Understanding is Compression, by Ziguang Li and 9 other authors\\n View PDF HTML (experimental)\\n Abstract: We have previously shown all understanding or learning are compression, under reasonable assumptions. In principle, better understanding of data should improve data compression. Traditional compression methodologies focus on encoding frequencies or some other computable properties of data. Large language models approximate the uncomputable Solomonoff distribution, opening up a whole new avenue to justify our theory.\\n Under the new uncomputable paradigm, we present LMCompress based on the understanding of data using large models. LMCompress has significantly better lossless compression ratios than all other lossless data compression methods, doubling the compression ratios of JPEG-XL for images, FLAC for audios and H264 for videos, and tripling or quadrupling the compression ratio of bz2 for texts. The better a large model understands the data, the better LMCompress compresses.\\n Subjects: Information Theory (cs.IT) ; Artificial Intelligence (cs.AI)\\n Cite as: arXiv:2407.07723 [cs.IT] \\n (or arXiv:2407.07723v1 [cs.IT] for this version) \\n \\n\\n Submission history\\n\\n From: Xingwu Liu [view email]\\n [v1] Mon, 24 Jun 2024 03:58:11 UTC (250 KB)\\n Full-text links:\\n\\n Access Paper:\\n\\n View a PDF of the paper titled Understanding is Compression, by Ziguang Li and 9 other authors\\n * View PDF\\n * HTML (experimental)\\n * TeX Source\\n * Other Formats\\n view license\\n Current browse context:\\n cs.IT\\n < prev | next >\\n new | recent | 2024-07\\n Change to browse by:\\n cs\\n cs.AI\\n math\\n math.IT\\n\\n References & Citations\\n\\n * NASA ADS\\n * Google Scholar\\n * Semantic Scholar\\n a export BibTeX citation Loading...\\n\\n BibTeX formatted citation\\n\\n √ó\\n loading...\\n Data provided by:\\n\\n Bookmark\\n\\n Bibliographic Tools\\n\\n Bibliographic and Citation Tools\\n\\n Bibliographic Explorer Toggle\\n Bibliographic Explorer (What is the Explorer?)\\n Litmaps Toggle\\n Litmaps (What is Litmaps?)\\n scite.ai Toggle\\n scite Smart Citations (What are Smart Citations?)\\n Code, Data, Media\\n\\n Code, Data and Media Associated with this Article\\n\\n Links to Code Toggle\\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\\n DagsHub Toggle\\n DagsHub (What is DagsHub?)\\n GotitPub Toggle\\n Gotit.pub (What is GotitPub?)\\n Links to Code Toggle\\n Papers with Code (What is Papers with Code?)\\n ScienceCast Toggle\\n ScienceCast (What is ScienceCast?)\\n Demos\\n\\n Demos\\n\\n Replicate Toggle\\n Replicate (What is Replicate?)\\n Spaces Toggle\\n Hugging Face Spaces (What is Spaces?)\\n Spaces Toggle\\n TXYZ.AI (What is TXYZ.AI?)\\n Related Papers\\n\\n Recommenders and Search Tools\\n\\n Link to Influence Flower\\n Influence Flower (What are Influence Flowers?)\\n Connected Papers Toggle\\n Connected Papers (What is Connected Papers?)\\n Core recommender toggle\\n CORE Recommender (What is CORE?)\\n About arXivLabs\\n\\n arXivLabs: experimental projects with community collaborators\\n\\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\\n * About\\n * Help\\n * Click here to contact arXiv Contact\\n * Click here to subscribe Subscribe\\n * Copyright\\n * Privacy Policy\\n * Web Accessibility Assistance\\n\\n * arXiv Operational Status\\n Get status notifications via email or slack\", highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://arxiv.org/pdf/2309.10668', id='https://arxiv.org/pdf/2309.10668.pdf', title='', score=0.3979397714138031, published_date='2023-11-16T00:00:00.000Z', author='', image=None, favicon=None, subpages=None, extras=None, text='Published as a conference paper at ICLR 2024\\nLANGUAGE MODELING IS COMPRESSION\\nGr√©goire Del√©tang‚àó1 Anian Ruoss‚àó1 Paul-Ambroise Duquenne2 Elliot Catt1\\nTim Genewein1 Christopher Mattern1 Jordi Grau-Moya1 Li Kevin Wenliang1\\nMatthew Aitchison1 Laurent Orseau1 Marcus Hutter1 Joel Veness1\\nABSTRACT\\nIt has long been established that predictive models can be transformed into lossless\\ncompressors and vice versa. Incidentally, in recent years, the machine learning\\ncommunity has focused on training increasingly large and powerful self-supervised\\n(language) models. Since these large language models exhibit impressive predictive\\ncapabilities, they are well-positioned to be strong compressors. In this work, we\\nadvocate for viewing the prediction problem through the lens of compression and\\nevaluate the compression capabilities of large (foundation) models. We show\\nthat large language models are powerful general-purpose predictors and that the\\ncompression viewpoint provides novel insights into scaling laws, tokenization, and\\nin-context learning. For example, Chinchilla 70B, while trained primarily on text,\\ncompresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their\\nraw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%),\\nrespectively. Finally, we show that the prediction-compression equivalence allows\\nus to use any compressor (like gzip) to build a conditional generative model.\\n1 INTRODUCTION\\nInformation theory and machine learning are inextricably linked and have even been referred to as\\n‚Äútwo sides of the same coin‚Äù (MacKay, 2003). One particularly elegant connection is the essential\\nequivalence between probabilistic models of data and lossless compression. The source coding\\ntheorem (Shannon, 1948) is the fundamental theorem describing this idea, i.e., the expected message\\nlength in bits of an optimal entropy encoder is equal to the negative log2-likelihood of the statistical\\nmodel. In other words, maximizing the log2-likelihood (of the data) is equivalent to minimizing the\\nnumber of bits required per message. Indeed, lossless compression with a probabilistic model can\\nbe achieved in a variety of different ways, including Huffman coding (Huffman, 1952), arithmetic\\ncoding (Pasco, 1977; Rissanen, 1976), and asymmetric numeral systems (Duda, 2009).\\nArithmetic coding, in particular, is known to be optimal in terms of coding length, meaning that the\\noverall compression performance depends on the capabilities of the probabilistic model (see Fig. 1\\nfor an overview of arithmetic coding). Incidentally, in recent years, large pre-trained Transform\\x02ers (Vaswani et al., 2017), so-called foundation models (Bommasani et al., 2021), have proven to be\\nhighly successful across a wide range of predictive tasks (Bubeck et al., 2023; Rae et al., 2021) and\\nare thus promising candidates for use with arithmetic coding. Indeed, Transformer-based compression\\nwith arithmetic coding has produced state-of-the-art results both in the online (Bellard, 2021; Mao\\net al., 2022) and offline settings (Valmeekam et al., 2023). In the online setting, a pseudo-randomly\\ninitialized model is directly trained on the stream of data that is to be compressed, while the offline\\nsetting, which we consider in our work, trains the model on an external dataset before employing it to\\ncompress a (potentially different) data stream. Consequently, offline compression is performed in\\x02context, with a fixed set of model parameters. Transformers have demonstrated impressive in-context\\nlearning abilities (Laskin et al., 2023; Brown et al., 2020; Wei et al., 2022; Genewein et al., 2023)\\nand are thus ideally suited for offline compression.\\n*Equal contribution. 1Google DeepMind. 2Meta AI & Inria. Correspondence to {gdelt, anianr}@google.com.\\n1\\narXiv:2309.10668v2 [cs.LG] 18 Mar 2024\\nPublished as a conference paper at ICLR 2024\\n0\\n0.5\\n1\\nb0\\nb1\\nInput (4 bytes)\\nOutput (7 bit)\\nb0\\n0\\n0.25\\nb00\\nb01\\nb0?\\n0.125\\n0.25\\nb001\\nb010\\nb010\\n0.3125\\nb0100\\nb0101\\nb0101010\\nb0101010\\n0\\n0.45\\n0.75\\n1\\nP(A)=0.45\\nP(I)=0.3\\nP(X)=0.25\\nA\\n0\\n0.09\\n0.36\\n0.45\\nP(A|A)=0.2\\nP(I|A)=0.6\\nP(X|A)=0.2\\nI\\n0.09\\n0.144\\n0.266\\n0.36\\nP(A|AI)=0.2\\nP(I|AI)=0.45\\nP(X|AI)=0.35\\nX\\n0.266\\n0.322\\n0.341\\n0.36\\nP(A|AIX)=0.6\\nP(I|AIX)=0.2\\nP(X|AIX)=0.2\\nI\\nFigure 1: Arithmetic encoding of ‚ÄòAIXI‚Äô with a probabilistic model P (blue) resulting in the binary\\ncode ‚Äòb0101010‚Äô (green). We iteratively divide the real interval I = [0, 1) according to the model‚Äôs\\n(conditional) probabilities and select the sub-interval corresponding to the observed symbol (e.g.,\\nI = [0, 0.45) for P(A)). We further refine I for each input symbol (indicated by the arrows), e.g.,\\nI = [0.09, 0.36) for P(I|A). To determine the encoded output, we iteratively split [0, 1) in half and\\nassign a binary code to each sub-interval (shaded red areas). At every step we can output the binary\\ncode if I is fully contained in the corresponding binary interval (e.g., ‚Äòb0‚Äô for ‚ÄòA‚Äô, but not for ‚ÄòAI‚Äô as\\nit could be ‚Äòb00‚Äô or ‚Äòb01‚Äô). At the end of the input, the code is ‚Äòb0101‚Äô, which cannot be uniquely\\ndecoded (P(A|AIX), P(I|AIX), P(X|AIX) all overlap with ‚Äòb0101‚Äô). Thus, we further refine\\nthe binary code until its binary interval is fully contained in I (all calculations in Appendix A).\\nThe context length is a key limiting factor in offline compression, as it dictates the maximum\\nnumber of bytes a model can compress at a time. Transformers can only compress a few kilobytes\\n(each ‚Äútoken‚Äù being coded with 2 or 3 bytes), while requiring a lot of compute. Correspondingly,\\nmany challenging predictive tasks (e.g., algorithmic reasoning or long-term memory) require long\\ncontexts (Del√©tang et al., 2023), and thus extending these models‚Äô context lengths is a key challenge\\nwhich is gaining increased attention (Zaheer et al., 2020; Guo et al., 2022; Bulatov et al., 2023). The\\nin-context compression view provides insights into the failure modes of current foundation models.\\nThis Work We advocate for using (lossless) compression to study foundation models. To that end,\\nwe conduct an extensive empirical investigation of the offline (in-context) compression capabilities\\nof large language models, with the rationale that they have become readily available (Touvron et al.,\\n2023a;b) and can thus be used for compression without the training overhead. We empirically demon\\x02strate that these models, while (meta-)trained primarily on text, achieve competitive compression\\nrates across different data modalities, outperforming domain-specific standard compressors (not\\naccounting for model parameter size). Moreover, we shed new light on scaling laws (Kaplan et al.,\\n2020), showing that they also hold true for compression but that measuring the adjusted compression\\nrates instead of the log loss adds a twist: Scaling beyond a certain point will deteriorate the compres\\x02sion performance since the model parameters need to be accounted for in the compressed output.\\nFinally, we advocate for framing (self-supervised) prediction through the lens of compression as it\\nencompasses generalization: a model that compresses well generalizes well (Hutter, 2006).\\nContributions We empirically study the lossless compression capabilities of foundation models:\\n‚Ä¢ We review how to compress with predictive models via arithmetic coding and call attention\\nto the connection between current language modeling research and compression.\\n‚Ä¢ We show that large language models achieve impressive compression rates (disregarding\\nmodel parameter size) on modalities other than text. For example, Chinchilla 70B achieves\\ncompression rates of 43.4% on ImageNet patches and 16.4% on LibriSpeech samples,\\nbeating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.\\n2\\nPublished as a conference paper at ICLR 2024\\n‚Ä¢ We revisit scaling laws, showing that the dataset size provides a hard limit on model size in\\nterms of compression performance and that model scaling is not a silver bullet.\\n‚Ä¢ We leverage the compression-prediction equivalence to employ compressors as generative\\nmodels and visually illustrate the performance of the underlying compressor.\\n‚Ä¢ We demonstrate that tokenization, which can be viewed as a pre-compression, does, in gen\\x02eral, not improve compression performance, but allows models to increase the information\\ncontent in their context and is thus generally employed to improve prediction performance.\\n2 BACKGROUND\\nIn this section, we review the necessary background on information theory and its relation to likelihood\\nmaximization. To that end, we consider streams of data x1:n := x1x2 . . . xn ‚àà X n of length n from\\na finite set of symbols X . We write x‚â§j = x 0 (i.e., encoding xk), we first partition the previous\\ninterval Ik‚àí1 = [lk‚àí1, uk‚àí1) into N sub-intervals ÀúIk(x1),\\nÀúIk(x2), . . . , one for each letter from X =\\n{x1, x2, . . . , xN }. The size of sub-interval ÀúIk(y) that represents letter y is (uk‚àí1 ‚àílk‚àí1)¬∑œÅ(y | x', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://export.arxiv.org/pdf/2301.09479v3.pdf', id='https://export.arxiv.org/pdf/2301.09479v3.pdf', title='Modality-Agnostic Variational Compression of Implicit Neural Representations', score=0.4188191592693329, published_date='2023-04-07T11:29:26.000Z', author='Jonathan Richard Schwarz,Jihoon Tack,Yee Whye Teh,Jae-Ho Lee,Jinwoo Shin', image=None, favicon=None, subpages=None, extras=None, text='Jonathan Richard Schwarz \\nJihoon Tack \\nYee Whye Teh \\nJaeho Lee \\nJinwoo Shin \\nModality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nWe introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modalityagnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate data, 3D shapes and scenes as well as audio and video, introducing VC-INR as the first INR-based method to outperform codecs as well-known and diverse as JPEG 2000, MP3 and AVC/HEVC on their respective modalities.\\n\\nIntroduction\\n\\nData compression has become a critical problem in the modern era, as vast amounts of data is added to and transmitted through computer networks (Clissa, 2022) at previously unimaginable rates. While momentous progress has been made compared to naive representations, custom compression techniques are still developed for each modality at hand, carefully introducing inductive biases into new algorithms. While being an undoubtedly successful approach, it has limited the transfer of algorithmic ideas between techniques * Equal contribution 1 DeepMind 2 University College London 3 KAIST 4 POSTECH. Correspondence to: Jonathan Richard Schwarz. designed for different forms of data. More importantly, in certain engineering or scientific problems, vast amounts of data may be collected for which no generally accepted compression technique may be available (e.g. the AR/VR domain (Yang et al., 2022), point clouds, remote sensing or climate data), inhibiting progress in such fields.\\n\\nIn this paper, we join a recent group of researchers (e.g. Dupont et al., 2021;2022b;Schwarz & Teh, 2022) in arguing for a paradigm shift: Making modality-agnosticism a key guiding principle, we advocate for a single algorithmic workbench on which methods applicable to any type of data represented by a coordinate and feature space are developed. This would allow research effort to be pooled and any jointly developed model or learning improvement to benefit multiple downstream compression applications at once.\\n\\nA promising approach towards realising this idea is the use of Implicit Neural Representations (INRs) or Neural Fields (e.g. Tancik et al., 2020;Sitzmann et al., 2020). An INR relies on a functional interpretation of data, specifically as a mapping from coordinates to features (e.g. (x, y) ‚Üí (r, g, b) for images), which is parameterised a neural network. INRs offer various attractive properties, including upsampling to arbitrary resolution (Chen et al., 2021) or a pathway to new approaches to applications such as generative modeling or classification (Dupont et al., 2022a). For our purpose, the most intriguing property of the INR approach is its inherent modality-agnosticism, as any data point can in theory be represented provided it is expressed as a coordinate to feature mapping and thus learnable. Consequently, a learned INR is simply an encoding of the data point within the weights of a neural network, the efficient storage of which has received much attention at a time of ever increasing model capacity. We can thus state the second guiding principle of the work at hand: Data-as model-compression.\\n\\nThis second principle distinguishes our ideas from much of the existing work on Neural Compression (e.g. Ball√© et al., 2017;Cheng et al., 2020b), which directly encodes a given data point into a codespace, hence relying on carefully designed modality-specific encoding and decoding networks (often called analysis/synthesis transforms). Throughout the manuscript, we will highlight how we overcome this arXiv:2301.09479v3 [stat.ML] 7 Apr 2023 limitation while building on rather than replacing the work from this community.\\n\\nAmong the recent work on compression with INRs on the other hand, various ideas for the efficient storage of INRs have been explored. So far, proposed compression and quantisation algorithms are relatively simple (e.g. Uniform Quantisation) or rely on a separate per-signal optimisation process (Str√ºmpler et al., 2022), hence significantly increasing runtime. In addition, much of this work relies on ideas borrowed from Meta-Learning (Finn et al., 2017) to decrease encoding times, which opens up various questions about the best trade-off between compact parameterisation and (Meta-) Learning algorithms. Therefore, despite significant efforts, a substantial gap still exists between INR-based compression and the hand-designed compression methods for certain modalities (e.g. JPEG 2000 for images, MP3 for audio).\\n\\nIn this paper, we improve INR-based compression in a twofold approach (i) We experiment with advanced conditioning techniques resulting in better signal-reconstruction prequantisation (ii) We overcome limitations of previously used quantisation techniques and introduce a learned quantiser, allowing us to maintain significantly higher reconstruction quality at lower file sizes post-quantisation. Both directions of investigation adhere to the guiding principles of modalityagnosticism and the view of data as model compression. This presentation is not accidental, as we can think of the two axes of investigation as orthogonal algorithmic considerations. Indeed, any improvement in (i) increases the upper bound of performance maintained in the quantisation and entropy coding steps in (ii), while any improvement in (ii) reduces the gap between upper bound and actually realised performance.\\n\\n\\nContributions:\\n\\n‚Ä¢ Improved conditioning: We propose a middle ground between recent sparsity and latent coding approaches to compact representations. The proposed technique introduces a non-linear mapping from a latent codes to a low-rank soft gating matrix per layer, selecting a sub-network to represent a data item in an underlying INR. This is shown to learn more efficiently and result in better reconstructions compared to previous approaches. Our interpretation and experimental analysis shines new lights onto related ideas explored in other contexts.\\n\\n‚Ä¢ Improved compression: We introduce a learned compressor pre-trained on compact latent codes representing training data. As such latent codes may be extracted from any modality, our proposed compressor operates fully modality-agnostic while making use of the same algorithmic insights previously only applicable to specific modalities. (Panayotov et al., 2015) respectively. In addition, we outperform MP3 on Librispeech by 5.6 dB and HEVC on Videos by 8.8 dB.\\n\\nThroughout this paper, we express a given data point x as a set of coordinates c ‚àà C and real-valued features y ‚àà Y and its corresponding INR representation as œÜ ‚àà R D . Whenever appropriate, we distinguish between N data points using superscripts, i.e.,\\n{(x i , œÜ i )} N i=1\\nand individual coordinate/feature pairs using subscripts, i.e.\\nx i := {(c j , y j )} M i=1 .\\n\\nRelated work\\n\\nINRs are neural networks approximating the functional mapping from coordinate to feature space. INRs are effective methods for modeling complex continuous signals, such as as 2D images (Chen et al., 2021), 3D scenes (Park et al., 2019), videos (Kim et al., 2022, and are even applicable for modeling discrete data, e.g. graphs (Grattarola & Vandergheynst, 2022). To this end, several architectures have been proposed to capture high-frequency signal details, examples being sinusoidal activations (Sitzmann et al., 2020), positional encodings (Mildenhall et al., 2020), and Fourier features (Tancik et al., 2020). In practice, INRs are often specialised to each data item by fine-tuning from a shared initialisation (Tancik et al., 2021), drastically cutting the number of optimisation iterations.\\n\\nNeural compression is an end-to-end autoencoder-based lossy compression framework aiming to directly minimise the inherent rate/distortion trade-off. This is based on a transform-coding approach (Goyal, 2001) shown in Figure 1a, where a data item x is transformed into a latent code z through an analysis transform g a . During training, quantisation is simulated through uniform noise (U) resulting in a noisy z and a corresponding reconstruction x = g s ( z) through the synthesis transform g s . At test time, z is quantised (and entropy coded), resulting in codes and reconstructions·∫ë,x respectively. Taking g a , g s to be deep neural (a) networks, the neural compression paradigm was introduced in (Ball√© et al., 2017;Theis et al., 2017), who make theoretical connections to variational inference. Recently, much of the recent work has focused on advanced designs of the entropy model, e.g. by using auto-regressive priors (Minnen et al., 2018a) or various forms of a hierarchical priors (Ball√© et al., 2018), such as Gaussian mixture models (GMM) in (Minnen et al., 2018b) \\n\\n\\nVariational Compression of INRs\\n\\n\\nOverview\\n\\nIn contrast to the two approaches discussed in the previous section, we now present a computational framework which maintains modality-agnosticism while allowing the use of deep entropy coding. We show a high-level overview in Figure 1c: The method can be best understood as an application of the non-linear transform coding paradigm ( Figure 1a) in the compact representation space of the INR appro', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://export.arxiv.org/pdf/2205.08957v2.pdf', id='https://export.arxiv.org/pdf/2205.08957v2.pdf', title='Meta-Learning Sparse Compression Networks', score=0.4046110212802887, published_date='2022-08-08T08:09:25.000Z', author='Jonathan Richard Schwarz,Yee Whye Teh', image=None, favicon=None, subpages=None, extras=None, text='Jonathan Richard Schwarz schwarzjn@google.com\\nDeepMind University College London\\nYee Whye Teh ywteh@google.com\\nDeepMind University College London\\nMeta-Learning Sparse Compression Networks\\nPublished in Transactions on Machine Learning Research (08/2022) DeepMind Reviewed on OpenReview: https: // openreview. net/ forum? id= Cct7kqbHK6\\nRecent work in DeepLearning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation. Recent work on such Implicit Neural Representations (INRs) has shown that -following careful architecture search -INRs can outperform established compression methods such as JPEG (e.g.Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ stateof-the-art network sparsification techniques to drastically improve compression. Secondly, introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.Full network:Figure 1: Overview of MSCN as a compression method. In order to compress a data item, we perform sparse adaptation of a meta-learned initialisation, leading to a small subset of changes Œ¥Œ∏ encoding the item. Sparsity reduces compression cost and avoids costly architecture search. Meta-Learning drastically cuts compression time. Œ¥Œ∏ can be subsequently compressed and encoded using standard techniques.In this work, we specifically focus on improving the suitability of INRs as a compression method by tackling the aforementioned problems. First, we recognise insights of recent deep learning studies (e.g. Frankle & Carbin, 2018) which show how only a small subset of parameters encode the predictive function. We thus employ recent state-of-the-art sparsity techniques(Louizos et al., 2017)to explicitly optimise INRs using as few parameters as possible, drastically improving their compression cost.. Model-agnostic meta-learning for fast adaptation of deep -learning. arXiv preprint arXiv:2109.04504, 2021.Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.\\nIntroduction\\nAn emerging sub-field of Deep Learning has started to re-imagine the representation of data items: While traditionally, we might represent an image or 3D shape as a multi-dimensional array, continuous representations of such data appear to be a more natural choice for the underlying signal. This can be achieved by defining a functional representation: mapping from spatial coordinates (x, y) to (r, g, b) values in the case of an image. The problem of learning such a function is then simply a supervised learning task, for which we may employ a neural network -an idea referred to as Implicit Neural Representations (INRs). An advantage of this strategy is that the algorithms for INRs are data agnostic -we may simply re-define the coordinate system and target signal values for other modalities and readily apply the same procedure. Moreover, the learned function can be queried at any point, allowing for the signal to be represented at higher resolutions once trained. Finally, the size of the network representation can be chosen by an expert or as we propose in this work, an algorithmic method to be lower than the native dimensionality of the array representation. Thus, this perspective provides a compelling new avenue into the fundamental problem of data compression. INRs are particularly attractive in cases where array representations scale poorly with the discretisation level (e.g. 3D shapes) or the underlying signal is inherently continuous such as in neural radiance fields (NerF) or when discretisation is non-trivial, for example when data lies on a manifold.\\nSo far, the difficulty of adopting INRs as a compression strategy has been a trade-off between network size and approximation quality requiring architecture search (e.g. Dupont et al., 2021) or strong inductive biases (e.g. Chan et al., 2021;Mehta et al., 2021). Furthermore, the cost of fitting a network to a single data point vastly exceeds the computational cost of standard compression methods such as JPEG', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://export.arxiv.org/pdf/2304.12319v1.pdf', id='https://export.arxiv.org/pdf/2304.12319v1.pdf', title='LVQAC: Lattice Vector Quantization Coupled with Spatially Adaptive Companding for Efficient Learned Image Compression', score=0.39728468656539917, published_date='2023-03-25T23:34:15.000Z', author='Xi Zhang,Xiaolin Wu', image=None, favicon=None, subpages=None, extras=None, text='Xi Zhang \\nShanghai Jiao Tong University\\nMcMaster University\\n\\n\\nXiaolin Wu \\nShanghai Jiao Tong University\\nMcMaster University\\n\\n\\nLVQAC: Lattice Vector Quantization Coupled with Spatially Adaptive Companding for Efficient Learned Image Compression\\n\\nRecently, numerous end-to-end optimized image compression neural networks have been developed and proved themselves as leaders in rate-distortion performance. The main strength of these learnt compression methods is in powerful nonlinear analysis and synthesis transforms that can be facilitated by deep neural networks. However, out of operational expediency, most of these end-to-end methods adopt uniform scalar quantizers rather than vector quantizers, which are information-theoretically optimal. In this paper, we present a novel Lattice Vector Quantization scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature dependencies than scalar uniform quantization while being computationally almost as simple as the latter. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive companding (AC) mapping with LVQ. The resulting LVQAC design can be easily embedded into any end-to-end optimized image compression system. Extensive experiments demonstrate that for any end-to-end CNN image compression models, replacing uniform quantizer by LVQAC achieves better rate-distortion performance without significantly increasing the model complexity.\\n\\nIntroduction\\n\\nIn the past five years, the research on end-to-end CNN image compression has made steady progress and led to the birth of a new class of image compression methods [1,3,4,8,21,25,26,[34][35][36][40][41][42][43][44]. The CNN compression can now match and even exceed the rate-distortion performance of the previous best image compression methods [6,33,37,45], which operate in the traditional paradigm of linear transform, quantization and entropy coding.\\n\\nThe advantages of the CNN approach of data compression come from the nonlinearity of its analysis and synthesis transforms of the autoencoder architecture, the endto-end joint optimization of the nonlinear transforms, uni-form quantization of the latent space and conditional entropy coding (context-based arithmetic coding) of quantized features.\\n\\nApparently, using uniform scalar quantizer in the above CNN image compression framework is motivated by operational expediency more than other considerations. Only at very high bit rate uniform quantization can approach the rate-distortion optimality [15]. It is very difficult to directly adopt and optimize a vector quantizer (VQ) in the end-to-end CNN architecture design for data compression, because VQ is a discrete decision process and it is not compatible with variational backpropagation that is necessary to the end-to-end CNN training. In [1], Agustsson et al. tried to circumvent the difficulty by a so-called soft-to-hard vector quantization scheme. Their technique is a soft (continuous) relaxation of discrete computations of VQ and entropy so that their effects can be approximated in the end-to-end training. However, in [1] the quantization centers are optimized along with the other modules, which make the whole system quite cumbersome and more difficult to train. In this paper, we p ropose a novel Lattice Vector Quantization scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature dependencies than scalar uniform quantization while being computationally almost as simple as the latter. Even if the features to be compressed are statistically independent, LVQ is still a more efficient coding strategy than scalar uniform quantization. This is because the former offers a more efficient covering of high-dimensional space than the latter, as proven by the theory of sphere packings, lattices and groups [11]. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive companding mapping with LVQ. The resulting LVQAC design is computationally as simple and as amenable to the end-toend training of the CNN compression model as in the originally proposed framework of [3].\\n\\nConsequently, for any end-to-end CNN image compression models, replacing uniform quantizer by LVQAC achieves better rate-distortion performance without significantly increasing the model complexity; the simpler the context-sensitive entropy model, the greater the performance gain. For instance, the checkerboard context model [18] has recently been proposed to speed up the decoding process, but it hurts the R-D performance slightly. By incorporating LVQAC into the end-to-end compression CNN with a checkerboard context model, we can achieve the same performance as the counterpart CNN using a far more expensive auto-regressive context model. This is because uniform scalar quantization needs a more complex model to compensate for its coding inefficiency, whereas LVQAC does not.\\n\\n\\nOverview of Learned Image Compression and Related Work\\n\\nBall√© et al. [3] pioneered the end-to-end CNN model for image compression. Their network design is still shaped by the legacy of classical three-step signal compression algorithms; namely, transform, quantization and entropy coding. The greatest advantage of the CNN approach over the classical one comes from replacing linear transforms of the latter by more powerful and complex nonlinear transforms, which is the distinctive prowess of deep neural networks. Specifically, in the end-to-end image compression CNNs, an image vector x ‚àà R N is mapped to a latent code space via a parametric nonlinear analysis transform, y = g a (x; Œ∏ g ). Then this representation is quantized, yielding a discrete-valued vector≈∑ = Q(y) ‚àà Z M which can be losslessly compressed using entropy coding algorithms such as arithmetic coding [31,38] and transmitted as a sequence of bits. On the other side, the decoder recovers≈∑ from the compressed codes, and subjects it to a parametric nonlinear synthesis transform g s (≈∑; œï g ) to build the reconstructed imagex. The nonlinear parametric transform g a (x; Œ∏ g ) and g s (x; œï g ) are implemented by convolutional neural networks and the parameters Œ∏ g and œï g are learned over a large amount of natural images. The training goal is to minimize the expected length of the bitstream as well as the expected distortion of the reconstructed image with respect to the original image, giving rise to a rate-distortion optimization problem:\\nminimize R + Œª ‚ãÖ D R = E x‚àºp x [‚àílog 2 p≈∑(Q(g a (x)))] D = Œª ‚ãÖ E x‚àºp x [d(x, g s (Q(g a (x))))](1)\\nwhere Œª is the Lagrange multiplier that determines the desired rate-distortion trade-off, p x is the (unknown) distribution of source images and p≈∑ is a discrete entropy model. Q(‚ãÖ) represents quantization, implemented as rounding to the nearest integer in most of the end-to-end compression papers. The first term (representing rate) corresponds to the cross entropy between the marginal distribution of the latent and the learned entropy model, which is minimized when the two distributions are identical. The second term (distortion) corresponds to the reconstructed error between the original image and the reconstructed image.\\n\\nIn most of the published end-to-end image compression CNNs, uniform scalar quantizer instead of vector quantizer (VQ) is adopted as the quantization function Q(‚ãÖ) in Eq. 1, That is,≈∑\\ni = Q(y i ) = round(y i )(2)\\nwhere index i runs over all elements of the vectors, including channels and spatial locations. Note that both terms in Eq. 1 depend on the quantized values, and the derivatives of the quantization function are zero almost everywhere, causing gradient descent problem. To allow optimization via stochastic gradient descent, a relaxation technique is used to replace the quantizer with an additive i.i.d. uniform noise ‚àÜy i , which has the same width as the quantization bins (one):·ªπ\\ni = y i + ‚àÜy i , ‚àÜy i ‚àà U(‚àí 1 2 , 1 2 ).(3)\\nVery few articles [1,46] studied vector quantization in end-to-end image compression networks.\\n\\nAgustsson et al. [1] proposed a so-called soft-to-hard vector quantization module to bridge the nearest neighbor decision process of VQ and variational backpropagation that is necessary for the end-to-end CNN training. Their technique is a soft (continuous) relaxation of branching operation of VQ so that the VQ effects can be approximated in the end-toend training. Zhu et al. [46] proposed a probabilistic vector quantization with cascaded estimation to estimate means and covariances. However, the optimization goal of [46] only contains the distortion term, ignoring the vital design requirement of coding rates.\\n\\nAfter the pioneering work of Ball√© et al. [3], many other end-to-end compression methods have been proposed to further improve the R-D performances by introducing more complex nonlinear transforms and more efficient context entropy models.\\n\\nRippel et al. [2,30] proposed to learn the distribution of images using adversarial training to achieve better perceptual quality at extremely low bit rate. Johnston et al. [19] published a spatially adaptive bit allocation algorithm that efficiently uses a limited number of bits to encode visually complex image regions. Li et al. [23] developed a method to allocate the content-aware bit rate under the guidance of a content-weighted importance map. Some papers focused on investigating the adaptive context model for entropy estimation to achieve a better trade-off between reconstruction errors and required bits (entropy), including [4,21,25,26], among which the CNN methods of [21,26] are the first to outperform BPG in PSNR. Choi et al. [9] published a novel variable-rate learned image compression framework with a conditional auto-encoder. Cheng et al. [8] proposed to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes and achieved a more accurate and flexible entropy model. Lin et al. [24] proposed a novel spati', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://export.arxiv.org/pdf/2305.16025v1.pdf', id='https://export.arxiv.org/pdf/2305.16025v1.pdf', title='NVTC: Nonlinear Vector Transform Coding', score=0.4043610095977783, published_date='2023-05-25T13:06:38.000Z', author='Runsen Feng,Zongyu Guo,Weiping Li,Zhibo Chen', image=None, favicon=None, subpages=None, extras=None, text='Runsen Feng \\nUniversity of Science and Technology of China\\n\\n\\nZongyu Guo \\nUniversity of Science and Technology of China\\n\\n\\nWeiping Li \\nUniversity of Science and Technology of China\\n\\n\\nZhibo Chen chenzhibo@ustc.edu.cn \\nUniversity of Science and Technology of China\\n\\n\\nNVTC: Nonlinear Vector Transform Coding\\n\\nIn theory, vector quantization (VQ) is always better than scalar quantization (SQ) in terms of rate-distortion (R-D) performance[34]. Recent state-of-the-art methods for neural image compression are mainly based on nonlinear transform coding (NTC) with uniform scalar quantization, overlooking the benefits of VQ due to its exponentially increased complexity. In this paper, we first investigate on some toy sources, demonstrating that even if modern neural networks considerably enhance the compression performance of SQ with nonlinear transform, there is still an insurmountable chasm between SQ and VQ. Therefore, revolving around VQ, we propose a novel framework for neural image compression named Nonlinear Vector Transform Coding (NVTC). NVTC solves the critical complexity issue of VQ through (1) a multi-stage quantization strategy and (2) nonlinear vector transforms. In addition, we apply entropy-constrained VQ in latent space to adaptively determine the quantization boundaries for joint rate-distortion optimization, which improves the performance both theoretically and experimentally. Compared to previous NTC approaches, NVTC demonstrates superior rate-distortion performance, faster decoding speed, and smaller model size. Our code is available at https://github.com/\\n\\nIntroduction\\n\\nRecent works based on nonlinear transform coding (NTC) [5] have achieved remarkable success in neural image compression [12,35]. Unlike these traditional image codecs that employ linear transform such as discrete cosine transform (DCT), NTC is constructed with the nonlinear transform layers and optimized with data-driven techniques, where the modern neural networks present excellent capability in both encoding/decoding transform and entropy estimation [12,20,36]. Most NTC methods apply scalar quantization (SQ) to discretize the latent variables and use the additive uniform noise to approximate the quantization error during training [6]. However, in the era of 1990s, it has already been known that vector quantization, in spite of it exponentially increased complexity, is always better than SQ in terms of rate-distortion (RD) performance [34]. It inspires us to design a novel neural image compression model to fully leverages vector quantization.\\n\\nVector quantization (VQ) [18] is designed to map a continuous source distribution to a set of discrete vectors. The discrete nature of VQ has been successfully applied in generative models to avoid the \"posterior collapse\" issue, including these well-known image synthesis models such as VQVAE [42], VQGAN [16] and text-to-image synthesis models such as DALLE [37], latent diffusion [39]. However, if we go back to the basic requirement of quantization, we will find that VQ offers unique advantages in terms of rate-distortion performance, particularly the space-filling advantage and the memory advantage [34].\\n\\nGiven a source distribution, the goal of quantization (no matter SQ or VQ) is to determine the quantization centers and boundaries, and then assign indices to denote these separated quantization regions/cells. Combining these regions fills the whole space of source distribution. The spacefilling advantage of VQ against SQ is related to the sphere\\n\\n\\nIsotropic Gaussian\\n\\nBanana Boomerang NTC ECVQ Figure 2. Quantization results for NTC (SQ with nonlinear transform) and ECVQ (entropy-constrained VQ) on 2-d distributions. Blue lines represent quantization boundaries and orange points represent quantization centers (codewords). \"Isotropic Gaussian\" refers to a 2-d isotropic Gaussian distribution. \"Banana\" and \"Boomerang\" are two different 2-d distributions. It is observed that NTC cannot achieve the space-filling advantage (i.e. learning hexagon-like quantization cells for 2-d sources) even on an isotropic gaussian distribution. Moreover, NTC\\'s decorrelation capability is insufficient as source correlation becomes more complex. For example, quantization boundaries collide in the red circle of \"Boomerang\", leading to a performance drop. The corresponding BD-PSNR results are shown in Table 2. packing problem in geometry [14,21,43]. If we compare the quantization results of SQ and VQ, as shown in Figure 2, we will find that even for a simple isotropic Gaussian distribution, SQ with nonlinear transform cannot learn to approximate hexagon-like quantization cells, where the hexagon is the polytope with the best space-filling properties in 2-d space. Under the high-rate assumption, the gain of space-filling advantage is about 1.53 dB as the dimension approaches infinity [15,34]. Following this conclusion, we experimentally provide the BD-PSNR results by comparing SQ with nonlinear transform to VQ on isotropic Gaussian distributions in Table 1. In addition, to reduce the redundancies of data distributions, existing NTC methods (SQ with nonlinear transform) rely on highly expensive nonlinear transform [12,46,47] and context-based autoregressive entropy models [20,36]. However, different from NTC methods, VQ has superior decorrelation ability, which is known as the memory advantage of vector quantizers. This advantage is more obvious when quantizing complex source distributions, such as the Boomerang distribution in Figure 2 (especially in the red circle area).\\n\\nIn this paper, we build a novel framework that applies modern neural networks to leverage the space-filling advantages and memory advantages of VQ for image compression. We propose nonlinear vector transform coding (NVTC), which achieves encouraging rate-distortion performance with relatively low coding complexity. Specifically, as shown in Figure 3, we introduce three key points to design a practical VQ, including 1) a multi-stage product VQ rather than a single-stage VQ to reduce the exponentially increased complexity, 2) nonlinear vector transform rather than scalar transform to remove redundancy between sub-vectors with fewer parameters, and 3) entropyconstrained VQ rather than unconstrained VQ to achieve superior R-D optimality and joint optimization of latent-space VQ models.\\n\\nFor the first point, many well-known VQ variants have been proposed in recent decades, such as product VQ [23,40], multi-stage VQ [24], tree-structured VQ [11] and lattice VQ [17]. Although tree-structured and lattice VQ offer fast encoding speeds, they do not reduce the storage complexity of codebooks or entropy-coding frequency tables In . For transform complexity (middle), we use vector transform instead of scalar transform to remove inter-vector redundancy. For RD optimality (right), we find that ECVQ [13] is essential for the joint rate-distortion optimization, which is neglected in previous works [2,32,42,45] . this paper, we suggest a hybrid VQ structure that incorporates both product VQ and multi-stage VQ, as shown in the left column of Figure 3. The quantization procedure comprises multiple stages, and each stage employs multiple independent low-dimension quantizers to compress the subvectors of the input vector. As the number of stages and subvectors increases, the proposed multi-stage product VQ exhibits a significant decrease in complexity.\\n\\nWhile the intra-vector redundancy (i.e. the redundancy inside each subvector) can be removed by vector quantization, the inter-vector redundancy (i.e. the redundancy between subvectors) is still overlooked. Therefore, our second point focuses on efficiently eliminating inter-vector redundancy. Transform VQ [26] introduces a linear transform for decorrelation and performs product quantization on the transformed coefficients. Similar coding structures are observed in recent learning-based VQ methods [2,45], which are improved by learnable nonlinear transform with superior decorrelation capabilities. However, the transform used in these works is designed to decorrelate scalar components, which is computationally inefficient for vector decorrelation. The intra-vector redundancy, which is intended to be removed by VQ, might be partially reduced in advance by the scalar transform. Therefore, certain parts of the scalar transform could be eliminated to improve com-putational efficiency. Motivated by the linear vector transform [29,31,32], we propose a new VT variant that decouples a fully-connected scalar transform into two lightweight parts: intra-transform and inter-transform. In the middle of Figure 3, we provide a simple comparison between the scalar transform and the proposed vector transform. We further stack the single-layer VT to build a powerful nonlinear vector transform. The differences between our VT and the linear VT are discussed in Section 4.1.\\n\\nRegarding the third point, we emphasize that the quantization process (either SQ or VQ) used in most previous methods [2,5,32,42,45] (including VQVAE) is not entropy-constrained, which is theoretically suboptimal for rate-distortion performance. In the right of Figure 3, we provide a quantization illustration of unconstrained VQ and entropy-constrained VQ (ECVQ [13]), where unconstrained VQ determines the quantization boundaries (blue line) using the nearest neighbor search. ECVQ introduces an additional rate bias ‚àí log pi Œª in the quantization process, which shifts the quantization boundaries from the highprobability region to the low-probability region. In other words, ECVQ search the codewords with the best RD performance, instead of just the neighboring codewords. ECVQ provides an optimal VQ encoding process described in Section 2. With the help of latent-space ECVQ, we de-sign a training strategy for joint RD optimization. Instead of manually controlling the RD trade-off by varying codebook size [45], our model can learn layer-adaptive bit allocation.\\n\\nOur contributions c', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://arxiv.org/pdf/2304.13583.pdf', id='https://arxiv.org/pdf/2304.13583.pdf', title='Multi-Modality Deep Network for Extreme Learned Image Compression', score=0.39811983704566956, published_date='2023-04-26T14:22:59.000Z', author='Xuhao Jiang,Weimin Tan,Tian Tan,Bo Yan,Liquan Shen', image=None, favicon=None, subpages=None, extras=None, text=\"Multi-Modality Deep Network for Extreme Learned Image Compression\\n\\n\\nXuhao Jiang \\nSchool of Computer Science\\nShanghai Key Laboratory of Intelligent Information Processing\\nShanghai Collaborative Innovation Center of Intelligent Visual Computing\\nFudan University\\nShanghaiChina\\n\\nWeimin Tan wmtan@fudan.edu.cn \\nSchool of Computer Science\\nShanghai Key Laboratory of Intelligent Information Processing\\nShanghai Collaborative Innovation Center of Intelligent Visual Computing\\nFudan University\\nShanghaiChina\\n\\nTian Tan \\nBo Yan byan@fudan.edu.cn \\nSchool of Computer Science\\nShanghai Key Laboratory of Intelligent Information Processing\\nShanghai Collaborative Innovation Center of Intelligent Visual Computing\\nFudan University\\nShanghaiChina\\n\\nLiquan Shen \\nSchool of Computer Science\\nShanghai Key Laboratory of Intelligent Information Processing\\nShanghai Collaborative Innovation Center of Intelligent Visual Computing\\nFudan University\\nShanghaiChina\\n\\nSchool of Communication\\nShanghai University\\nShanghaiChina\\n\\nMulti-Modality Deep Network for Extreme Learned Image Compression\\n\\nImage-based single-modality compression learning approaches have demonstrated exceptionally powerful encoding and decoding capabilities in the past few years , but suffer from blur and severe semantics loss at extremely low bitrates. To address this issue, we propose a multimodal machine learning method for text-guided image compression, in which the semantic information of text is used as prior information to guide image compression for better compression performance. We fully study the role of text description in different components of the codec, and demonstrate its effectiveness. In addition, we adopt the image-text attention module and image-request complement module to better fuse image and text features, and propose an improved multimodal semantic-consistent loss to produce semantically complete reconstructions. Extensive experiments, including a user study, prove that our method can obtain visually pleasing results at extremely low bitrates, and achieves a comparable or even better performance than state-of-the-art methods, even though these methods are at 2√ó to 4√ó bitrates of ours.\\n\\nIntroduction\\n\\nDuring the past decades, image data on the Internet shows an explosive growth, bringing huge challenges for data storage and transmission. To meet this ever-increasing requirements, low-bitrate lossy image compression is a promising way to save storage and transmission bandwidth. Traditional image compression algorithms, e.g., Better Portable Graphics (BPG) (Bellard 2015) and Versatile Video Coding (VVC) (ITU-T and ISO/IEC 2020), are widely used in practice. However, they will cause serious blocking artifacts due to block-based processing at low bitrates. Therefore, exploring better methods for extreme image compression is urgently needed.\\n\\nRecently, many single-modality learned methods (Xie, Cheng, and Chen 2021;Mentzer et al. 2020) have been proposed. However, they also fail to reconstruct satisfactory results at extremely low bitrates. Specifically, they may generate blurry results due to limited bits, or utilize Generative Adversarial Networks (GAN) to produce sharp results Figure 1: Visual comparisons of the proposed TGIC and some state-of-the-art methods (HiFiC (Mentzer et al. 2020), VVC (ITU-T and ISO/IEC 2020), BPG (Bellard 2015) and the work of Xie et al. (Xie, Cheng, and Chen 2021)). whose textures may not be semantically consistent with the original image, as shown in Fig. 1. The text-to-image synthesis task is currently receiving a lot of attention, which generates semantically consistent images from text descriptions. Inspired by this task, multi-modality image compression may have great advantages at low bitrates. The corresponding text provides the high-level image semantic information, which can be used as prior information to assist image compression. Specifically, the text describes a rough content of the image and its local features, such as, color, location, shape, etc. This semantic information can be used to assist in reconstructing images, which can help save image bits. In addition, under the guidance of text semantic information, the multi-modality model can generate visually pleasing results that are more semantically consistent with the original image, as shown in Fig. 1. Note that the text description occupies very few bits and can be transmitted to the decoder side at marginal bandwidth cost. Even for low bitrates (0.2 bpp), text uses less than one-twentieth of the bits used by images on datasets (Wah et al. 2011;Nilsback and Zisserman 2008).\\n\\nIn this paper, a text-guided image compression (TGIC) generative adversarial network is proposed, in which the text description is utilized as prior information to assist in image compression. TGIC can produce better results compared with other methods, even if we use a much lower bitrate. For the image encoding, based on the image-text attention (ITA) module, text information is introduced into the codec to guide the generation of compact feature representations. In the image decoding stage, we design an image-request complement (IRC) module to adaptive fuse the text and image information for better reconstructions. Besides, an improved multimodal semantic-consistent loss is designed to further improve the perceptual quality of reconstructions. The main contributions are as follows:\\n\\n‚Ä¢ We propose a novel codec framework for image compression, which utilizes the semantic information of the text description to improve coding performance. To the best of our knowledge, this is the first attempt that uses text semantic information as prior information to guide image compression. ‚Ä¢ We fully study the role of text description in different components of the codec, and demonstrate its effectiveness for image compression. In particular, we adopt ITA to fuse image and text features, and propose IRC that allows the network to adaptively learn the much-needed guidance knowledge from text. ‚Ä¢ The experiments (including a user study) show the outstanding perceptual performance of our TGIC in comparison with the existing learned image compression methods and traditional compression codecs.\\n\\n\\nRelated Work Lossy Image Compression\\n\\nLossy image compression has received significant attention from both academia and industry due to its huge practical value. Traditional compression standards, such as JPEG (Wallace 1992), JPEG2000 (Rabbani 2002), BPG (HEVC-Intra) (Bellard 2015) and VVC (ITU-T and ISO/IEC 2020) (the latest traditional codec), reply on handcrafted module design. However, they ignore spatial correlations between image blocks, which results in image discontinuities at block boundaries. Recently, many learned methods have been proposed to tackle the problem of image compression, and achieve promising results. Some early methods (Toderici et al. 2015(Toderici et al. , 2017 utilize the recurrent neural network to recursively compress the residual information, but they cannot directly optimize the rate in the training phase. The subsequent works are mainly based on variational autoencoder, and significant advances have been made progressively (Rippel and Bourdev 2017;Zhang et al. 2019;Chen et al. 2021;Ball√©, Laparra, and Simoncelli 2017;Agustsson et al. 2017;Theis et al. 2017;Ball√© et al. 2018;Lee, Cho, and Beack 2018;Hu et al. 2021;Cheng et al. 2020). Recognizing the huge potential of the hyperprior model ), many follow-up methods improve the entropy estimation techniques based on hyperprior design, such as coarse-to-fine model Liu 2020), joint model (Minnen, Ball√©, andToderici 2018) and 3D context entropy model (Guo et al. 2020). Besides, some methods Mentzer et al. 2020; and Lucic 2019) based on GAN (Goodfellow et al. 2014) have been proposed for image compression at low bitrates. Mentzer et al. (Mentzer et al. 2020) investigate Figure 2: Overview of existing learned compression methods and the proposed TGIC. f E (¬∑) and f G (¬∑) represent the encoder and decoder of codec, respectively, and q(¬∑) represents the quantization function. f HE (¬∑) and f HG (¬∑) denote the encoder and decoder of the auxiliary autoencoder (entropy model), respectively. f D (¬∑) and f T (¬∑) denote the discriminator and the text encoder, respectively. normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses, and propose HiFiC which shows impressive performance.\\n\\nHowever, the above-mentioned algorithms all show poor performance at extremely low bitrates, and even the excellent GAN-based method HiFiC shows general performance in this situation. The main reason is that it is impossible to faithfully reconstruct the entire content of the uncompressed image with extreme limited bits, and the GAN-based methods cannot generate images with realistic textures due to the lack of guidance from additional prior information. Therefore, benefiting from the semantic information provided by the text description, image compression based on multimodal machine learning may have a greater possibility to obtain better compression performance.\\n\\n\\nMultimodal Machine Learning\\n\\nThe multimodal machine learning has recently become a very hot topic due to its powerful advantages in the field of computer vision, such as text-to-image synthesis and image captioning. The text-to-image synthesis task aims to produce a high-quality image from a described text, such as (Zhang et al. 2017;Xu et al. 2018;Reed et al. 2016). For example, given the text description, AttnGAN (Xu et al. 2018) employs attention mechanism to produce images with photorealistic details. Contrary to the text-to-image synthesis task, the image captioning task (Shi et al. 2020;Anderson et al. 2018;Xu et al. 2015) is to generate a corresponding text description for a given image. Inspired by these works, we propose text-guided image compression, which uses the semantic information of text description as prior information to improve coding performance. Figure. 2 provides an overview of learned image compression in the transform coding approach (Goyal 2001). As shown in Fig. 2 (a), the baseline model can be expressed Figure 3: Architecture of our text-guided image compression (TGIC) model. C3 √ó 3 √ó 64 s2 is a convolution with 64 channels, with 3√ó3 filters and stride 2. ‚Üë 2 indicates the nearest neighbor upsampling. In addition, AE and AD are arithmetic encoding and decoding, and Q is for quantization. ITA is introduced to fuse image features and text features based on attention mechanism, and IRC is designed to adaptively use the text features for the image semantic complement. Resblock and ResModule are based on (He et al. 2016).\\n\\n\\nMethod Preliminaries\\nas y = q(f E (x)), thenx = f G (y),(1)\\nwhere x,x and y are raw images, reconstructed images and compressed codes, respectively. Specifically, x is encoded by an encoder f E (¬∑), and then quantized by q(¬∑) to obtain y. y is then losslessly compressed into a bitstream by using entropy coding like arithmetic coding (Rissanen and Langdon 1981). For decoding, the decoder f G (¬∑) transforms y to obtain thex. Here, the widely used quantization function proposed by (Ball√©, Laparra, and Simoncelli 2017) is employed in our method. The corresponding rate R of y is estimated by a fully factorized density model p y|Œ∏ during training, which is formulated by\\nR = E[‚àílog 2 p y|Œ∏ (y|Œ∏)].\\n(2) Balle et al. first proposes a hyperprior model , which utilizes the side information z to capture the spatial dependence of y, as shown in Fig. 2 (b). This idea is realized by introducing an additional entropy model. The side information z can be calculated by z = q(f HE (y)), where f HE (¬∑) denotes the encoder of the entropy model. Then z is transformed by the decoder of the auxiliary autoencoder f HD (¬∑), which is used to estimate the distribution of y. Then the rate R is formulated by\\nR = E[‚àílog 2 p y|z (y|z)] + E[‚àílog 2 p z|Œ∏ (z|Œ∏)].\\n(3) As shown in Fig. 2 (c), compared with the hyperprior model, the GAN-based model has one more discriminator, which generates the reconstructed images with fine-grained details through adversarial training.\\n\\nBased on the previous works, we propose a new codec framework (as shown in Fig. 2 (d)), which utilizes the text information to assist in image compression. We will describe the proposed framework in detail in the following section.\\n\\n\\nProposed Method\\n\\nThe architecture design of TGIC is shown in Fig. 3, which describes the details of the Fig. 2 (d). Specifically, the main body of TGIC is composed of four components: encoder, decoder, entropy model and discriminator. The image encoder and text encoder are employed to map the image and the text into a common semantic feature space. Firstly, the text features are extracted from the text description by using the text encoder. Then these text features are introduced into TGIC to assist image compression for better image reconstruction based on the image-text attention (ITA) module. In particular, we design an image-request complement module (IRC) in the decoder, so as to realize adaptive selection of text semantic information for the image semantic feature enhancement. Besides, we also design an improved multimodal semantic-consistency loss, which considers the semantic consistency between the reconstructions and the texts, as well as the uncompressed images.\\n\\n\\nText-Guided Feature Representation\\n\\nIn our TGIC, the text encoder is a bi-direction Long Short-Term Memory (LSTM) (Schuster and Paliwal 1997), which extracts the semantic features of texts. The corresponding calculation can be defined as\\nt = f T (text),(4)\\nwhere t denotes the text semantic features and f T (¬∑) represents the text encoder. Then t is input to the encoder and the entropy model respectively to achieve a compact feature representation.\\n\\nText guidance in the encoder. ITA is adopted to calculate the correlation between the text and image features, and then fuse these two features, as shown in Fig. 4 (a). Inspired by (Xu et al. 2018;Li, Tan, and Yan 2021), ITA uses multiscale residual structure to further extract image features, and use matrix multiplication to calculate correlation, where W 1 , W 2 and W 3 represent the convolutional operations with different filter size, and W 4 is used to adjust the text feature dimension. The previous works Duan, Chen, and Gu 2020) have verified that the semantic segmentation maps can improve coding performance. Considering that text can provide high-level semantic information of images, we utilize the high correlation between image and text features to achieve more compact image features. Therefore, y can be defined as y = q(f E (x, t)), where f E (¬∑) represents the encoder of TGIC.\\n\\nText guidance in the entropy model. The hyperprior model ) improves the coding performance by introducing the side information, whose essence lies in predicting the distribution of latent features. The distribution of latent features is closely related to the content of the image. Considering that the text description can offer the image semantic information, the text information may help to predict the distribution of latent features. Inspired by (Li, Li, and Lu 2021), the text features are introduced to f HG (¬∑) of the entropy model. Similarly, this operation is also based on ITA. Then the entropy model can use the semantic relevance of text and image to predict the distribution of latent features more accurately. Under the guidance of the text features, the estimated distribution is converted from p y|z to p y|z,t , which better parameterizes the distributions of latent codes and improves the entropy model performance.\\n\\n\\nText-Guided Image Reconstruction\\n\\nDue to the quantization operation, the image features in the decoder inevitably lose some information. Considering that the text description contains some semantic information of the image, the semantic features of the text is considered as prior information to enhance the image features. Then, the reconstructed image can be defined asx = f G (y, t), where f G (¬∑) represents the decoder of TGIC. Aiming to make better use of text features for image feature enhancement, we propose the IRC to adaptively fuse the text information and image information. The architecture of IRC is shown in Fig. 4 (b). Firstly, IRC predict an attention map A based on the correlation between the input image features V 2 and text features, which is realized by the matrix multiplication of feature maps and is calculated as Figure 5: Projecting multi-modal embedding into the semantic feature space.\\nA = sof tmax(W 5 ((V 2 t)(V 2 t) T ))(5)\\nwhere W 5 represents the convolutional operation. Then we use the obtained A to weight and add the text features. Finally, the adaptive selected text features and the input image features are fused to obtain the enhanced features V 2 by using ITA. This process can be expressed as V 2 = IT A(V 2 , At + t).\\n\\n\\nText-Guided Adversarial Training\\n\\nConditional GAN (Mirza and Osindero 2014) is designed to learn a generative model of a conditional distribution, and shows excellent performance in many tasks, such as image super-resolution . Considering the high correlation between the image and its corresponding text, TGIC uses text features as conditional information in the discriminator for better adversarial training. In the discriminator, the text features are first reshaped, then convolved and upsampled, so that these features can be concatenated with the image. Under the guidance of the text features, the discriminator maps an input (x,t) to the probability P x|t .\\n\\n\\nMultimodal Semantic-Consistent Loss\\n\\nThe proposed multimodal semantic-consistent loss is designed to constrain the semantic consistency of the reconstructed image and the original image as well as the text. AttnGAN (Xu et al. 2018) suggests to map the image features and text features into a common semantic space with the image encoder and text encoder, and calculates the negative log posterior probability to make the reconstructed image and the corresponding text semantically consistent. The corresponding loss is defined as\\nL IT = ‚àí(logP (t|f I (x)) + logP (f I (x)|t)),(6)\\nwhere f I (¬∑) represents the image encoder. Although L IT can constrain the semantics of the text description and reconstructions to be consistent, the images that conform to the text description are diverse. Therefore, we also add the constraint between the reconstructions and the uncompressed images, which is shown in Fig. 5. The multimodal semantic-consistent loss can be calculated as\\nL M = L IT + L II , where L II = Œ≤||f I (x) ‚àí f I (x)|| 2 ,(7)\\nwhere Œ≤ is a hyper-parameter, and L II is the loss function which makes the reconstructions and the uncompressed images semantic-consistent. Specifically, as shown in Fig. 5, the text does not contain the description of the bird's eyes, and the reconstruction quality of the eye part in the result generated only by L IT is poor. However, using the improved L M , the subjective quality of the reconstruction result is significantly improved.\\n\\nIn addition to L M , we also use other four loss functions to optimize TGIC. Among them, the reconstruction loss L R , GAN loss L G and rate loss L Rate are commonly used in the GAN-base codec. Since the text description takes up very few bits and its value is a constant, we do not consider it in the rate loss, but we take it into account when calculating the final bitrates. Here, these three loss functions are defined as (10) where f D (¬∑) represents the discriminator. Following (Mentzer et al. 2020), the perceptual loss L P based on a pretrained AlexNet (Krizhevsky, Sutskever, and Hinton 2012) is also adopted, which is defined as\\nL R = ||x ‚àí x|| 2 ,(8)L G = E[logf D (x, t)] + E[log(1 ‚àí f D (x, t))],(9)L Rate = E[‚àílog 2 p y|z,t (y|z, t)] + E[‚àílog 2 p z|Œ∏ (z|Œ∏)],L p = ||œÜ(x) ‚àí œÜ(x)|| 2 ,(11)\\nwhere œÜ(¬∑) is the function of the pretrained AlexNet.\\n\\nFinally, the global loss function of TGIC is defined as L = ŒªL Rate + k 1 L R + k 2 L G + k 3 L P + k 4 L M (12) where the k 1 , k 2 , k 3 , and k 4 are hyper-parameters, and the Œª is a trade-off parameter to balance rate and distortion. Following (Mentzer et al. 2020), we adopt the constrained rate. The constrained rate introduces a 'rate target' hyperparameter r t and two hyper-parameter including Œª a and Œª b . When the calculated rate is larger than r t , Œª=Œª a . Otherwise, Œª=Œª b . We can obtain a model with an average bitrate close to r t by setting Œª a Œª b\\n\\n\\nTraining Implementation\\n\\nFollowing AttnGAN (Xu et al. 2018), the image encoder and text encoder are pretrained to map the image features and text features into a common semantic space. Then the weights of the image encoder and text encoder are fixed while training the proposed TGIC. We adopt Pytorch as the training toolbox, and use the Adam optimization algorithm (Kingma and Ba 2014) with a mini-batch of 4 to optimize the model parameters. All the experiments are conducted on a NVIDIA GeForce RTX 1080 Ti. Our model is optimized for 300 epochs with a learning rate of 1 √ó 10 ‚àí4 . The hyper-parameters k 1 , k 2 , k 3 , k 4 and Œ≤ of the global loss function are empirically set as 0.075 √ó 2 ‚àí5 , 0.15, 5, 0.005 and 40, respectively. The Œª b is set as 2 ‚àí4 , and the Œª a is set as (2 3 , 2 2 , 2 1 ) to adapt to different bitrates.\\n\\n\\nExperiments Datasets and Evaluation\\n\\nFollowing the previous multimodal machine learning-based works (Zhang et al. 2017), the widely used datasets including CUB (Wah et al. 2011) and Oxford-102 (Nilsback and Zisserman 2008) The proposed TGIC aims at improving the codec performance at extremely low bitrates. Thus, we set the rate target below 0.25bpp for TGIC. Note that we use the bit sum of the image and the text to calculate the bitrates for TGIC, while we only utilize the bits of the image to calculate the bitrates for other methods. The bitarate of text are defined as R text = Sizetext√ó8 H√óW , where Size text denotes the file size of the text in bytes, H and W denote the hight and width of the image, respectively. Following (Mentzer et al. 2020;Yang, Van Gool, and Timofte 2021), we use LPIPS , KID (Bi≈Ñkowski et al. 2018), FID (Heusel et al. 2017) to evaluate our TGIC and other compared methods, which are highly consistent with human perception of images. In addition, we also use Peak Signal-to-Noise Ratio (PSNR) to measure the fidelity of our results.\\n\\n\\nComparison against SOTA Methods\\n\\nIn this part, TGIC is compared with state-of-the-art (SOTA) methods, including BPG (Bellard 2015), VVC (ITU-T and ISO/IEC 2020), HiFiC (Mentzer et al. 2020), and the works of Ball√© et al. Toderici 2018), Cheng et al. (Cheng et al. 2020) and Xie et al. (Xie, Cheng, and Chen 2021). Among these methods, BPG and VVC are the traditional methods, and others belong to the deep learning-based methods. Especially, HiFiC is a GAN-based model, aiming to produce results with better subjective quality. We use the BPG software (YCbCr 4:4:4) to test the images. For VVC, we use the VVC Official Test Model VTM 10.0 (YCbCr 4:4:4) of official version with an intra-profile configuration to test on images. We draw the rate-distortion (RD) curves to compare the coding performance of different methods.\\n\\nQuantitative results. Fig. 6 shows the RD curve comparison on the CUB and Oxford-102 datasets. It can be observed that Our TGIC shows much better performance compared to other methods in terms of LPIPS, FID and KID. Especially, TGIC achieves a comparable or even better performance than other methods, even though these methods are at 2x to 4x bitrates of ours. In addition, we evaluate the fidelity of TGIC in terms of PSNR. As shown in Fig. 7, TGIC reaches competitive PSNR values compared with HiFiC. This verifies that TGIC can maintain an acceptable fidelity when compressing images towards subjective quality.\\n\\nQualitative results. Fig. 8 shows the visualization results of TGIC and other SOTA methods at similar bitrates. It can be found that the results of TGIC show better subjective quality than other algorithms. For VVC and BPG, their results have obvious blocking artifacts. The results of Xie et al. are blurry due to the MSE-based training. In addition, HiFiC utilizes GAN to obtain the hallucinated details, and produces visually pleasing reconstructions. Unfortunately, the hallucinated content may be inconsistent with the original image content, resulting in obvious artifacts. Under the guidance of the text description, TGIC can produce satisfied reconstructions with photorealistic details, which are semanticconsistent with the original images.\\n\\nUser Study. A user study is further conducted with 32 participants. Given a pair of reconstructed images, the user is asked to judge which one owns a higher perceptual quality and is more consistent with the corresponding text description. We randomly select 50 images from CUB and 50 images from Oxford-102. Note that the bitrates of their compressed results by different methods are required to be similar. TGIC is compared with the state-of-the-art methods including VVC, HiFiC and the work of Xie et al.. This user study requires 9600 comparisons in total. Each participant needs to spend 40 minutes to complete the subjective test. As shown in Fig. 9, it can be found that the results of TGIC gain more preference than that of other methods, and all exceed 70%. This subjective comparisons are consistent with the quantitative results in Fig. 6, which verifies that our TGIC is superior to other algorithms. \\n\\n\\nAblation Study\\n\\nIn this part, we study and analyze the contributions of text guidance and different loss functions to our algorithm. Since the resulting bitrates do not exactly match the setting value, we try our best to compare performance of different models at the similar bitrates. We first remove L P , L M and all the text guidance including text-guided feature representation (TGFR), text-guided image reconstruction (TGIR) and textguided adversarial training (TGAT) from our TGIC, and re- gard this model as the baseline model. As shown in Table 1, our TGIC obtains much better performance than baseline, which confirms the performance gain of introducing text. Case 1: Effectiveness of text. We conduct the ablation study on the text effectiveness. We test the performances of TGIC without TGFR (w/o TGFR), TGIC without TGIR (w/o TGIR) and TGIC without TGAT (w/o TGAT). As shown in Table 1, the performances of three models all decrease compared with TGIC. Note that w/o TGIR has the largest performance drop and w/o TGAT has the smallest drop. This means that the text has the greatest effect in the decoder and the least effect in the discriminator.\\n\\nCase 2: Effectiveness of L P and L M . Since L R , L Rate and L G are common in GAN-based codec, we explore the effectiveness of L P and L M . As shown in Table 1, both have performance gains for our TGIC.\\n\\nCase 3: Effectiveness of IRC and L II . We also take the ablation study to verify the effectiveness of the proposed IRC and L II . As shown in Table 1, the performances both decrease when removing IRC and L II . This tells that IRC and L II are benefical to our model.\\n\\n\\nText-guided Image Compression in Real-World Scenarios\\n\\nIn real-world scenarios, the image content is diverse and complex. To explore more possibilities of our TGIC, we also conduct experiments on COCO (Lin et al. 2014) with 80 types of objects. To further verify the generation performance, we also conduct the cross-dataset experiments on Kodak (Franzen 1999). Since Kodak does not have corresponding text descriptions, we consider using the image captions methods (e.g. OFA (Wang et al. 2022)) to generate texts for our experiments. The generated texts are of high quality and highly semantically with the images. We compare our TGIC with BPG, VVC and HiFiC, and the visualization results are shown in Table 2 and Fig. 10. We can find that our TGIC produces better results compared to other methods, which confirms the great potential of our TGIC.\\n\\n\\nConclusion\\n\\nIn this paper, we propose a text-guided adversarial generation network for image compression (TGIC). We adopt the image-text attention module to introduce text information into the codec as prior information. Specifically, the text description can help the codec achieve a compact features representation, and can also be used for the image feature enhancement. In addition, we design an image-request complement module to adaptively learn the much-needed guidance knowledge of text information for feature enhancement. Moreover, a new multimodal semantic-consistent loss is well-designed that constrains the semantic consistency between the reconstructions, the texts and the uncompressed images. Experimental results demonstrate that TGIC outperforms the SOTA methods. Especially, even at bitrates below 0.1 bpp, the TGIC can produce appealing visual results.\\n\\n\\nThe Details of Some Modules\\n\\nThe details of the ResBlock (He et al. 2016) in the proposed TGIC are shown in Fig. 11(a). The size of the filter is 3 √ó 3 and the stride is 1. The channel number is the same as the number of the previous convolutional layer. As shown in Fig. 11(b), the ResModule consists of four ResBlock modules. The details of the discriminator are shown in Fig. 11(c). The Conv3 √ó 3 √ó 6 s1 means that the convolutional layer of size 3 √ó 3, stride of 1, and kernel number of 6. t represents the text features andx denotes the image. Figure 11: Architectures of the ResBlock, the ResModule and the discriminator. (a) and (b) show the detail structure of the ResBlock and the ResModule. (c) represents the discriminator. The Conv3 √ó 3 √ó 6 s1 means that the convolutional layer of size 3 √ó 3, stride of 1, and kernel number of 6.\\n\\n\\nVisualization Results\\n\\nResults on CUB and Oxford-102 Datasets We add many visualization results on CUB (Wah et al. 2011) and Oxford-102 (Nilsback and Zisserman 2008) datasets. The reconstructed results of models, including the proposed TGIC, BPG (Bellard 2015), VVC (ITU-T and ISO/IEC 2020), HiFiC (Mentzer et al. 2020) and the work of Xie et al. (Xie, Cheng, and Chen 2021), can be seen in Figs. 12, 13. In particular, for each image, we adopt these models to encode and decode images at similar bitrates. Note that we use the bit sum of the image and the text to calculate the bitrates for TGIC. As shown in Figs. 12, 13, we can find that the results of our TGIC have better subjective quality compared to other methods at similar bitrates.\\n\\n\\nResults on COCO Dataset\\n\\nTo explore more possibilities of our TGIC, we also conduct experiments on COCO (Lin et al. 2014) datatset, which consists of 80 types of objects. We add a large number of visualization results. The reconstructed results of the proposed TGIC and VVC (ITU-T and ISO/IEC 2020) (the latest commercial codec) can be seen in Fig. 14. We also adopt these models to encode and decode images at similar bitrates for each test image. As shown in Fig. 14, we can find that the subjective qualities of the results generated by our TGIC are higher than that of VVC. \\n\\nFigure 4 :\\n4Architectures of ITA and IRC. C means concatenation. V 1 , V 2 , V 1 and V 2 are the image features.\\n\\nFigure 6 :\\n6Performance evaluation on CUB and Oxford-102 datasets in terms of LPIPS, FID and KID.\\n\\nFigure 7 :\\n7The PSNR results on CUB and Oxford-102.\\n\\nFigure 8 :\\n8Visual comparisons with SOTA approaches on CUB and Oxford-102 datasets. Above each line of the images is the corresponding text description. Better zoom in.\\n\\nFigure 10 :\\n10Visual comparisons with VVC on COCO (first row) and Kodak (second row). Better zoom in.\\n\\nFigure 12 :\\n12Visual comparisons on CUB dataset of our TGIC and the state-of-the-art methods including BPG, VVC, HiFiC and the work of Xie et al.. The first, second, third, fourth and fifth columns show the reconstructed results of these five methods, respectively. The sixth column shows the original images. Better zoom in.\\n\\nFigure 13 :\\n13Visual comparisons on Oxford-102 dataset of our TGIC and the state-of-the-art methods including BPG, VVC, HiFiC and the work of Xie et al.. The first, second, third, fourth and fifth columns show the reconstructed results of these five methods, respectively. The sixth column shows the original images. Better zoom in.\\n\\nFigure 14 :\\n14Visual comparisons on COCO dataset of our TGIC and the state-of-the-art method VVC. The first and fourth columns show the reconstructed results of VVC. The second and fifth columns show the reconstructed results of our TGIC. The third and sixth columns show the original images. Better zoom in.\\n\\nTable 1 :\\n1Performance Comparisons between variations of our TGIC on CUB dataset. The best results are boldfaced.Figure 9: User study results. The reported value indicates \\nthe performance rate of the proposed TGIC against the other \\nSOTA methods, respectively. \\n\\n\\n\\nTable 2 :\\n2Performance comparisons of our TGIC and VVC on COCO and Kodak datasets. Models are trained on training set of COCO, and tested on Kodak and 1000 images of testing set of COCO. The best results are boldfaced.\\nAppendix\\nSoftto-Hard Vector Quantization for End-to-End Learning Compressible Representations. E Agustsson, F Mentzer, M Tschannen, L Cavigelli, R Timofte, L Benini, L J Van Gool, Advances in Neural Information Processing Systems. Agustsson, E.; Mentzer, F.; Tschannen, M.; Cavigelli, L.; Timofte, R.; Benini, L.; and Van Gool, L. J. 2017. Soft- to-Hard Vector Quantization for End-to-End Learning Com- pressible Representations. In Advances in Neural Informa- tion Processing Systems.\\n\\nGenerative adversarial networks for extreme learned image compression. E Agustsson, M Tschannen, F Mentzer, R Timofte, L V Gool, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionAgustsson, E.; Tschannen, M.; Mentzer, F.; Timofte, R.; and Gool, L. V. 2019. Generative adversarial networks for ex- treme learned image compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 221-231.\\n\\nBottom-up and top-down attention for image captioning and visual question answering. P Anderson, X He, C Buehler, D Teney, M Johnson, S Gould, L Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down at- tention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, 6077-6086.\\n\\nEnd-to-end optimized image compression. J Ball√©, V Laparra, E P Simoncelli, 5th International Conference on Learning Representations. Ball√©, J.; Laparra, V.; and Simoncelli, E. P. 2017. End-to-end optimized image compression. In 5th International Confer- ence on Learning Representations, ICLR 2017.\\n\\nVariational image compression with a scale hyperprior. J Ball√©, D Minnen, S Singh, S J Hwang, N Johnston, International Conference on Learning Representations. Ball√©, J.; Minnen, D.; Singh, S.; Hwang, S. J.; and John- ston, N. 2018. Variational image compression with a scale hyperprior. In International Conference on Learning Repre- sentations.\\n\\nF Bellard, BPG Image format. Bellard, F. 2015. BPG Image format. https:// bel- lard.org/bpg/.\\n\\n. M Bi≈Ñkowski, D J Sutherland, M Arbel, A Gretton, arXiv:1801.01401Demystifying mmd gans. arXiv preprintBi≈Ñkowski, M.; Sutherland, D. J.; Arbel, M.; and Gret- ton, A. 2018. Demystifying mmd gans. arXiv preprint arXiv:1801.01401.\\n\\nEnd-to-End Learnt Image Compression via Non-Local Attention Optimization and Improved Context Modeling. T Chen, H Liu, Z Ma, Q Shen, X Cao, Y Wang, IEEE Transactions on Image Processing. 30Chen, T.; Liu, H.; Ma, Z.; Shen, Q.; Cao, X.; and Wang, Y. 2021. End-to-End Learnt Image Compression via Non- Local Attention Optimization and Improved Context Mod- eling. IEEE Transactions on Image Processing, 30: 3179- 3191.\\n\\nLearned image compression with discretized gaussian mixture likelihoods and attention modules. Z Cheng, H Sun, M Takeuchi, J Katto, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionCheng, Z.; Sun, H.; Takeuchi, M.; and Katto, J. 2020. Learned image compression with discretized gaussian mix- ture likelihoods and attention modules. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7939-7948.\\n\\nS Duan, H Chen, J Gu, arXiv:2005.12810JPAD-SE: High-Level Semantics for Joint Perception-Accuracy-Distortion Enhancement in Image Compression. arXiv preprintDuan, S.; Chen, H.; and Gu, J. 2020. JPAD-SE: High- Level Semantics for Joint Perception-Accuracy-Distortion Enhancement in Image Compression. arXiv preprint arXiv:2005.12810.\\n\\nKodak lossless true color image suite. R Franzen, 4Franzen, R. 1999. Kodak lossless true color image suite. source: http://r0k. us/graphics/kodak, 4(2).\\n\\n. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\\n\\nAdvances in neural information processing systems. V K Goyal, IEEE Signal Processing Magazine. 275Theoretical foundations of transform codingGenerative adversarial nets. Advances in neural in- formation processing systems, 27. Goyal, V. K. 2001. Theoretical foundations of transform coding. IEEE Signal Processing Magazine, 18(5): 9-21.\\n\\n3-d context entropy model for improved practical image compression. Z Guo, Y Wu, R Feng, Z Zhang, Z Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsGuo, Z.; Wu, Y.; Feng, R.; Zhang, Z.; and Chen, Z. 2020. 3- d context entropy model for improved practical image com- pression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 116- 117.\\n\\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid- ual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, 770-778.\\n\\nGans trained by a two time-scale update rule converge to a local nash equilibrium. M Heusel, H Ramsauer, T Unterthiner, B Nessler, S Hochreiter, Advances in neural information processing systems. 30Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by a two time-scale up- date rule converge to a local nash equilibrium. Advances in neural information processing systems, 30.\\n\\nCoarse-to-fine hyperprior modeling for learned image compression. Y Hu, W Yang, J Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Hu, Y.; Yang, W.; and Liu, J. 2020. Coarse-to-fine hyper- prior modeling for learned image compression. In Proceed- ings of the AAAI Conference on Artificial Intelligence, vol- ume 34, 11013-11020.\\n\\nLearning end-toend lossy image compression: A benchmark. Y Hu, W Yang, Z Ma, J Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. ITU-T; and ISO/IEC. 2020. Versatile Video CodingHu, Y.; Yang, W.; Ma, Z.; and Liu, J. 2021. Learning end-to- end lossy image compression: A benchmark. IEEE Trans- actions on Pattern Analysis and Machine Intelligence. ITU-T; and ISO/IEC. 2020. Versatile Video Coding. ITU-T Rec. H.266 and ISO/IEC 23090-3.\\n\\nAdam: A method for stochastic optimization. D P Kingma, J Ba, A Krizhevsky, I Sutskever, G E Hinton, arXiv:1412.6980Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems. 25arXiv preprintKingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im- agenet classification with deep convolutional neural net- works. Advances in neural information processing systems, 25.\\n\\nContext-adaptive Entropy Model for End-to-end Optimized Image Compression. J Lee, S Cho, S.-K Beack, International Conference on Learning Representations. Lee, J.; Cho, S.; and Beack, S.-K. 2018. Context-adaptive Entropy Model for End-to-end Optimized Image Compres- sion. In International Conference on Learning Representa- tions.\\n\\nDeep contextual video compression. J Li, B Li, Y Lu, Advances in Neural Information Processing Systems. 34Li, J.; Li, B.; and Lu, Y. 2021. Deep contextual video com- pression. Advances in Neural Information Processing Sys- tems, 34: 18114-18125.\\n\\nPerceptual Variousness Motion Deblurring With Light Global Context Refinement. J Li, W Tan, Yan , B , Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLi, J.; Tan, W.; and Yan, B. 2021. Perceptual Variousness Motion Deblurring With Light Global Context Refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 4116-4125.\\n\\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll√°r, C L Zitnick, European conference on computer vision. SpringerLin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra- manan, D.; Doll√°r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, 740-755. Springer.\\n\\nConditional Probability Models for Deep Image Compression. F Mentzer, E Agustsson, M Tschannen, R Timofte, L Van Gool, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEEMentzer, F.; Agustsson, E.; Tschannen, M.; Timofte, R.; and Van Gool, L. 2018. Conditional Probability Models for Deep Image Compression. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4394-4402. IEEE.\\n\\nHigh-Fidelity Generative Image Compression. Advances in Neural Information Processing Systems. F Mentzer, G D Toderici, M Tschannen, E Agustsson, D Minnen, J Ball√©, G Toderici, NeurIPS. 33Joint Autoregressive and Hierarchical Priors for Learned Image CompressionMentzer, F.; Toderici, G. D.; Tschannen, M.; and Agustsson, E. 2020. High-Fidelity Generative Image Compression. Ad- vances in Neural Information Processing Systems, 33. Minnen, D.; Ball√©, J.; and Toderici, G. 2018. Joint Autore- gressive and Hierarchical Priors for Learned Image Com- pression. In NeurIPS.\\n\\nM Mirza, S Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMirza, M.; and Osindero, S. 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.\\n\\nAutomated flower classification over a large number of classes. M.-E Nilsback, A Zisserman, Sixth Indian Conference on Computer Vision, Graphics & Image Processing. IEEENilsback, M.-E.; and Zisserman, A. 2008. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 722-729. IEEE.\\n\\nJPEG2000: Image compression fundamentals, standards and practice. M Rabbani, Journal of Electronic Imaging. 112286Rabbani, M. 2002. JPEG2000: Image compression funda- mentals, standards and practice. Journal of Electronic Imag- ing, 11(2): 286.\\n\\nGenerative adversarial text to image synthesis. S Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, PMLRInternational Conference on Machine Learning. Reed, S.; Akata, Z.; Yan, X.; Logeswaran, L.; Schiele, B.; and Lee, H. 2016. Generative adversarial text to image syn- thesis. In International Conference on Machine Learning, 1060-1069. PMLR.\\n\\nReal-time adaptive image compression. O Rippel, L Bourdev, PMLRInternational Conference on Machine Learning. Rippel, O.; and Bourdev, L. 2017. Real-time adaptive im- age compression. In International Conference on Machine Learning, 2922-2930. PMLR.\\n\\nUniversal modeling and coding. J Rissanen, G Langdon, IEEE Transactions on Information Theory. 271Rissanen, J.; and Langdon, G. 1981. Universal modeling and coding. IEEE Transactions on Information Theory, 27(1): 12-23.\\n\\nBidirectional recurrent neural networks. M Schuster, K K Paliwal, IEEE transactions on Signal Processing. 4511Schuster, M.; and Paliwal, K. K. 1997. Bidirectional recur- rent neural networks. IEEE transactions on Signal Process- ing, 45(11): 2673-2681.\\n\\nImproving Image Captioning with Better Use of Caption. Z Shi, X Zhou, X Qiu, X Zhu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsShi, Z.; Zhou, X.; Qiu, X.; and Zhu, X. 2020. Improving Im- age Captioning with Better Use of Caption. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, 7454-7464.\\n\\nL Theis, W Shi, A Cunningham, F Husz√°r, arXiv:1703.00395Lossy image compression with compressive autoencoders. arXiv preprintTheis, L.; Shi, W.; Cunningham, A.; and Husz√°r, F. 2017. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395.\\n\\nG Toderici, S M O&apos;malley, S J Hwang, D Vincent, D Minnen, S Baluja, M Covell, R Sukthankar, arXiv:1511.06085Variable rate image compression with recurrent neural networks. arXiv preprintToderici, G.; O'Malley, S. M.; Hwang, S. J.; Vincent, D.; Minnen, D.; Baluja, S.; Covell, M.; and Sukthankar, R. 2015. Variable rate image compression with recurrent neural networks. arXiv preprint arXiv:1511.06085.\\n\\nFull resolution image compression with recurrent neural networks. G Toderici, D Vincent, N Johnston, S Hwang, D Minnen, J Shor, M Covell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionToderici, G.; Vincent, D.; Johnston, N.; Jin Hwang, S.; Min- nen, D.; Shor, J.; and Covell, M. 2017. Full resolution im- age compression with recurrent neural networks. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5306-5314.\\n\\nDeep generative models for distribution-preserving lossy compression. M Tschannen, E Agustsson, M Lucic, Advances in Neural Information Processing Systems. 31Tschannen, M.; Agustsson, E.; and Lucic, M. 2019. Deep generative models for distribution-preserving lossy com- pression. Advances in Neural Information Processing Sys- tems 31, 5929-5940.\\n\\nThe caltech-ucsd birds-200-2011 dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, CNS-TR-2011-001California Institute of TechnologyTechnical ReportWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie, S. 2011. The caltech-ucsd birds-200-2011 dataset. Techni- cal Report CNS-TR-2011-001, California Institute of Tech- nology.\\n\\nThe JPEG still picture compression standard. G K Wallace, IEEE transactions on consumer electronics. 381Wallace, G. K. 1992. The JPEG still picture compression standard. IEEE transactions on consumer electronics, 38(1): xviii-xxxiv.\\n\\nOFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. P Wang, A Yang, R Men, J Lin, S Bai, Z Li, J Ma, C Zhou, J Zhou, Yang , H , arXiv:2202.03052arXiv preprintWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma, J.; Zhou, C.; Zhou, J.; and Yang, H. 2022. OFA: Uni- fying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.\\n\\nEnhanced invertible encoding for learned image compression. Y Xie, K L Cheng, Q Chen, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on MultimediaXie, Y.; Cheng, K. L.; and Chen, Q. 2021. Enhanced invert- ible encoding for learned image compression. In Proceed- ings of the 29th ACM International Conference on Multime- dia, 162-170.\\n\\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, J Ba, R Kiros, K Cho, A Courville, R Salakhudinov, R Zemel, Y Bengio, PMLRInternational conference on machine learning. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi- nov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and tell: Neural image caption generation with visual at- tention. In International conference on machine learning, 2048-2057. PMLR.\\n\\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. T Xu, P Zhang, Q Huang, H Zhang, Z Gan, X Huang, X He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; and He, X. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1316-1324.\\n\\nR Yang, L Van Gool, R Timofte, arXiv-2109Perceptual Video Compression with Recurrent Conditional GAN. arXiv e-prints. Yang, R.; Van Gool, L.; and Timofte, R. 2021. Perceptual Video Compression with Recurrent Conditional GAN. arXiv e-prints, arXiv-2109.\\n\\nStackgan: Text to photorealistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Wang, X Huang, D N Metaxas, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZhang, H.; Xu, T.; Li, H.; Zhang, S.; Wang, X.; Huang, X.; and Metaxas, D. N. 2017. Stackgan: Text to photo- realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international confer- ence on computer vision, 5907-5915.\\n\\nThe unreasonable effectiveness of deep features as a perceptual metric. R Zhang, P Isola, A A Efros, E Shechtman, O Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586-595.\\n\\nSingle image super-resolution with enhanced Laplacian pyramid network via conditional generative adversarial learning. X Zhang, H Song, K Zhang, J Qiao, Q Liu, Neurocomputing. 398Zhang, X.; Song, H.; Zhang, K.; Qiao, J.; and Liu, Q. 2020. Single image super-resolution with enhanced Lapla- cian pyramid network via conditional generative adversarial learning. Neurocomputing, 398: 531-538.\\n\\nResidual non-local attention networks for image restoration. Y Zhang, K Li, K Li, B Zhong, Y Fu, arXiv:1903.10082arXiv preprintZhang, Y.; Li, K.; Li, K.; Zhong, B.; and Fu, Y. 2019. Resid- ual non-local attention networks for image restoration. arXiv preprint arXiv:1903.10082.\", highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://arxiv.org/pdf/2002.03482v1.pdf', id='https://arxiv.org/pdf/2002.03482v1.pdf', title='Ultra High Fidelity Image Compression with ùìÅ‚àû-constrained Encoding and Deep Decoding', score=0.4021560251712799, published_date='2020-02-10T00:33:39.000Z', author='Xi Zhang,Xiaolin Wu', image=None, favicon=None, subpages=None, extras=None, text='Xi Zhang \\nFellow, IEEEXiaolin Wu \\nMANUSCRIPT SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Ultra High Fidelity Image Compression with ‚àû -constrained Encoding and Deep Decoding\\n\\nIn many professional fields, such as medicine, remote sensing and sciences, users often demand image compression methods to be mathematically lossless. But lossless image coding has a rather low compression ratio (around 2:1 for natural images). The only known technique to achieve significant compression while meeting the stringent fidelity requirements is the methodology of ‚àû -constrained coding that was developed and standardized in nineties. We make a major progress in ‚àûconstrained image coding after two decades, by developing a novel CNN-based soft ‚àû -constrained decoding method. The new method repairs compression defects by using a restoration CNN called the ‚àû -SDNet to map a conventionally decoded image to the latent image. A unique strength of the ‚àû -SDNet is its ability to enforce a tight error bound on a per pixel basis. As such, no small distinctive structures of the original image can be dropped or distorted, even if they are statistical outliers that are otherwise sacrificed by mainstream CNN restoration methods.More importantly, this research ushers in a new image compression system of ‚àû -constrained encoding and deep soft decoding ( ‚àû -ED 2 ). The ‚àû -ED 2 approach beats the best of existing lossy image compression methods (e.g., BPG, WebP, etc.) not only in ‚àû but also in 2 error metric and perceptual quality, for bit rates near the threshold of perceptually transparent reconstruction. Operationally, the new compression system is practical, with a low-complexity real-time encoder and a cascade decoder consisting of a fast initial decoder and an optional CNN soft decoder.\\n\\nI. INTRODUCTION\\n\\nIn many professional applications of computer vision, such as medicine, remote sensing, sciences and precision engineering, high spatial and spectral resolutions of images are always of paramount importance. As the achievable resolutions of modern imaging technologies steadily increase, users are inundated by the resulting astronomical amount of image data. For example, a single pathology image generated by digital pathology slide scanner can easily reach the size of 1GB or larger. For the sake of operability and cost-effectiveness, images have to be compressed for storage and communication in practical systems.\\n\\nUnlike in consumer applications, such as smartphones and social media, where users are mostly interested in image esthetics, professionals of many technical fields are more concerned with the fidelity of decompressed images. Ideally, they want mathematically lossless image compression, that is, X the compression is an invertible coding scheme that can decode back to the original image, bit for bit identical. Although the mathematically lossless image coding is the ultimate gold standard, its compression performance is too limited. Despite years of research [1]- [5], typical lossless compression ratios for medical and remote sensing images are only around 2:1, which fall far short of the requirements of most imaging and vision systems.\\n\\nIn order to meet the stringent fidelity requirements while still achieving significant compression ratio, the ‚àûconstrained (or colloquially called near-lossless) image coding methodology was developed [6]- [9] and standardized by ISO/JPEG [1], [10]. The distinction between the lossy and near-lossless compression methods is that the latter guarantees that at each pixel the absolute value of compression error is bounded by œÑ , œÑ being a user specified error tolerance. The tight per-sample error bound can only be realized by the minmax ‚àû error criterion. The ubiquitous 2 error metric, which is adopted by consumer-grade lossy image compression methods, such as JPEG, JPEG 2000, WebP, etc., measures the average distortion over all pixels. The 2 compression is unable to preserve distinct image details that are statistical outliers but nevertheless vital to image semantics. Such cases are common in machine vision applications; for examples, one is searching in a big ocean for a small boat, or a small lesion in a large organ. When constrained by bit budget, an 2 -based lossy compression method tends to override such small structures by whatever dominant patterns in the background: ocean waves in the first example and liver textures in the second example. In order to avert such risks users (e.g., doctors, scientists and engineers) in many professions have to forego the 2 -based lossy compression widely used in consumer applications, and adopt the more conservative ‚àû metric to keep compression error tolerance at a necessary minimum.\\n\\nSince the standardization of the JPEG-LS nearlossless made in 1993, very little progress has been made in techniques for ‚àû nearlossless image compression. Scheuch et al. and Zhou et al. realized that the per-pixel ‚àû error bound offers much stronger and useful information than in the 2 -based compression, and used it to mitigate compression noises in a process called soft decoding [11]. Soft decoding of the from adopting a more versatile and precise statistical model for the inverse problem, whatever complex and defying analytical tools it might turn out to be. The methodology of data-driven deep learning opens up such possibilities, as it can function as highly non-linear implicit statistical models.\\n\\nIndeed, a large number of machine learning methods have been published recently for various image restoration tasks, including the reduction of compression noises [12]- [14]. However, these deep learning based methods for soft decoding are apparently motivated by consumer and Internet applications and heavily influenced by mainstream lossy image/video compression methods. They adopt ubiquitous 2 or 1 error metrics in the cost function of the restoration convolutional neural networks (CNN). This average fidelity design criterion tends to smooth out rare distinct image features. To counter the smoothing side effects, researchers widely adopt the technique of generative adversary neural network (GAN) for the task of compression artifacts removal. With an emphasis on pleasing visual appearances rather than high objective fidelity, GAN has a well-known tendency to fabricate \"realistic\" looking but false details in the reconstructed images. But any deletions and fabrications of image features are detrimental and should be absolutely forbidden in the professional fields of medicine, space, remote sensing, sciences, precision engineering and the alike.\\n\\nOne way to prevent the above identified side effects of minimizing the MSE loss and the GAN adversarial loss is the use of the ‚àû loss function, when training the restoration CNN for compression artifact reduction or soft decoding. Unlike MSE and GAN losses, the ‚àû loss imposes a tight error bound on each single pixel; therefore, it can preserve distinct and subtle structures of the original image even if they are statistical outliers. However, training the soft decoding CNN for minimum ‚àû loss (called the ‚àû -SDNet hereafter) may have convergence difficulties, if the image compressor is designed for 2 minimization as in current practice. For the feasibility of the ‚àû -SDNet, we need to control the compression distortions at the source. The above reviewed ‚àû -constrained image coding approach serves our purpose perfectly. The strict ‚àû constraint of the encoder ensures each decoded pixel to have a given error bound œÑ , and consequently offers the ‚àû -SDNet much needed strong priors to infer the inverse mapping of soft decoding.\\n\\nThe above reasoning leads to a novel ultra high-fidelity image compression system of ‚àû -constrained encoding and deep soft decoding ( ‚àû -ED 2 ), which is the main contribution of this work. In the ‚àû -ED 2 system, an image is ‚àû coded by a sequential prediction-quantization encoder; the decoder is a cascade of the conventional (hard) decoder and a deep soft decoder that is the ‚àû -SDNet as outlined in the previous paragraph. As the strict non-differentiable ‚àû loss does not permit backpropagation, we replace it with a differentiable quasi-‚àû loss term when optimizing the ‚àû -SDNet; also we modify the activation function of the network and make it respond to the ‚àû criterion, expediting the quasi-‚àû training process. In addition, we use the dilated convolution in the ‚àû -SDNet to achieve larger receptive field with fewer layers. This makes the ‚àû -SDNet shallower and more practical, while reducing risk of overfitting. The soft decoding network ‚àû -SDNet outperforms the best of existing lossy image compression methods such as BPG, WebP, J2K, not only in ‚àû but also in 2 error metric, for bit rates near the threshold of perceptually transparent reconstruction; in effect, it reduces the critical bandwidth for perceptually lossless image compression. The ‚àû loss term contributes to the improved perceptual quality as ‚àû penalizes the blurring of sharp edges more heavily than 2 .\\n\\nFinally, we stress the practical significance of this research. The proposed ‚àû -ED 2 image compression system not only raises the bar for achievable rate-distortion performance in ‚àû , 2 error metrics and in perceptual quality, but more importantly it can realize the compression gain in real time encoding. This is simply because the ‚àû -ED 2 system uses the traditional low complexity predictive encoder. Granted the CNN soft decoder ‚àû -SDNet is more expensive than conventional predictive decoder, but it is an optional refinement after the quick hard decoding. The asymmetric complexity characteristic of the ‚àû -ED 2 system gives it a distinct operational advantage over the recently researched end-to-end pure CNN compression approach [15]- [18], as the former has a lower encoding complexity than the latter by order of magnitude. Before a real-time CNN encoder of optimal rate-distortion performance can be economically implemented on end devices, such as cell phones, th', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://export.arxiv.org/pdf/2212.13243v1.pdf', id='https://arxiv.org/pdf/2212.13243.pdf', title='', score=0.39589086174964905, published_date='2023-10-26T00:00:00.000Z', author='', image=None, favicon=None, subpages=None, extras=None, text='1\\nLearned Lossless Image Compression Through\\nInterpolation With Low Complexity\\nFatih Kamisli\\nAbstract‚ÄîWith the increasing popularity of deep learning in\\nimage processing, many learned lossless image compression meth\\x02ods have been proposed recently. One group of algorithms that\\nhave shown good performance are based on learned pixel-based\\nauto-regressive models, however, their sequential nature prevents\\neasily parallelized computations and leads to long decoding times.\\nAnother popular group of algorithms are based on scale-based\\nauto-regressive models and can provide competitive compression\\nperformance while also enabling simple parallelization and much\\nshorter decoding times. However, their major drawback are the\\nused large neural networks and high computational complexity.\\nThis paper presents an interpolation based learned lossless\\nimage compression method which falls in the scale-based auto\\x02regressive models group. The method achieves better than or\\non par compression performance with the recent scale-based\\nauto-regressive models, yet requires more than 10x less neural\\nnetwork parameters and encoding/decoding computation com\\x02plexity. These achievements are due to the contributions/findings\\nin the overall system and neural network architecture design,\\nsuch as sharing interpolator neural networks across different\\nscales, using separate neural networks for different parameters of\\nthe probability distribution model and performing the processing\\nin the YCoCg-R color space instead of the RGB color space.\\nIndex Terms‚ÄîImage compression, Artificial neural networks,\\nEntropy coding, Lossless compression\\nI. INTRODUCTION\\nMost image and video communication applications use\\nlossy compression, however, there are also applications that\\nrequire lossless compression. Lossless compression allows the\\noriginal data to be perfectly reconstructed from the compressed\\nbitstream without any loss of information. For example, med\\x02ical imaging, satellite imaging, professional photography and\\ndigital cinema are applications where lossless image or video\\ncompression is used. This paper presents a novel learned\\nlossless image compression method.\\nClassical lossless image compression methods are based on\\npixel [1], [2], [3], [4] or block based prediction [5], [6], [7],\\ninter-to-integer transforms [8] or both [9]. In the prediction\\nbased methods, the compression is typically performed in a\\nraster scan order of pixels or blocks. A pixel or a block of pix\\x02els is predicted using previously coded left and upper neighbor\\npixels or neighboring left and upper pixels of the block, and the\\nprediction error pixel or block is then entropy coded. Then the\\ncompression of the next pixel or block starts. The prediction\\nerror pixels are typically more decorrelated than the original\\npixels, allowing good compression performance with simple\\nentropy codes. For example, JPEG-LS [1] is based on a pixel\\nThe author is with the Department of Electrical and Electronics Engi\\x02neering at the Middle East Technical University, Ankara, Turkey. (email:\\nkamisli@metu.edu.tr)\\nCodes are available at https://github.com/metu-kamisli/LLICTI\\nbased prediction algorithm and lossless compression in HEVC\\n[7], [10] is based on a block based prediction algorithm.\\nIn the integer-to-integer transform based lossless compres\\x02sion methods, the integer-valued image pixels are transformed\\ninto a transform domain with integer-valued coefficients [11],\\n[8], [9]. The transformation is invertible and thus no loss\\nof information occurs. For example, lossless compression in\\nJPEG2000 is based on the integer 5/3 wavelet transform\\n[11], [12], [13]. The obtained transform coefficients are again\\nmore decorrelated than the image pixels and allow for good\\ncompression performance with simple entropy codes. It can\\nalso be useful to use the prediction and integer-to-integer\\ntransform methods together in some contexts [9].\\nWith the high expressive power of artificial neural networks\\nand the recent advances and interest in deep learning, lossy\\n[14], [15], [16], [17] and lossless [18], [19], [20] image com\\x02pression methods with neural networks have been proposed.\\nThe state of the art in learned lossy [17], [21] and lossless\\n[19], [20] image compression is on par with or exceeds\\nthe compression performance of state of the art classical\\ncompression methods [22], [4].\\nRecent learned lossless image compression methods can be\\nroughly categorized to three groups: methods based on pixel\\x02based auto-regressive models, methods based on integer dis\\x02crete flows and methods based on scale-based auto-regressive\\nmodels. Methods based on pixel-based auto-regressive models\\nare similar to the methods based on classical pixel based\\nprediction methods in the sense that they encode/decode the\\nimage in a pixel-by-pixel manner yet use neural networks to\\nobtain the probabilities of the pixel to be coded/decoded from\\npreviously encoded/decoded neighbor pixels. While achieving\\nquite good compression performance, their major drawback is\\nthe long decoding times due to the sequential runs of the neural\\nnetwork for every pixel in the image [23], [24]. Methods based\\non integer discrete flows are similar to classical systems based\\non inter-to-integer transforms as discrete flows are neural\\nnetwork based systems mapping integer pixels to integer\\nlatent/transform variables. Their major drawback is that they\\nrequire quite big neural networks for good compression per\\x02formance [25]. Methods based on scale-based auto-regressive\\nmodels decompose the original image into a multiple scale\\nrepresentation, encode/decode first the lowest resolution scale\\nand then encode/decode the remaining scales sequentially\\nconditioned on previously encoded/decoded scales. These sys\\x02tems typically fare better than the previous two in terms of\\nencoding/decoding speed and/or neural networks size [18],\\n[19], [20].\\nThis paper presents a learned lossless image compression\\nalgorithm that falls in the scale-based auto-regressive model\\narXiv:2212.13243v1 [eess.IV] 26 Dec 2022\\n2\\ncategory. The system first encodes/decodes a sub-sampled grid\\nof the original image grid (e.g. every 32nd pixel horizon\\x02tally and vertically) using simple fixed-length codes. Then it\\nprogressively interpolates (i.e. produces probability distribu\\x02tions of) finer sub-sampled grids of the original image grid\\nconditioned on all previously encoded/decoded grids so far,\\nto achieve the encoding/decoding of the entire image. The\\ninterpolators are all neural network based and learned from a\\ndataset.\\nThe contributions of this paper are as follows. The system\\npresented achieves better than or on par compression per\\x02formance with the recent literature, yet requires more than\\n10x less neural network parameters and encoding/decoding\\ncomputational complexity [18], [19], [20]. These achievements\\nare due to the contributions/findings in the overall system and\\nneural network architecture design, such as\\n‚Ä¢ using the same interpolator neural networks in different\\nscales,\\n‚Ä¢ using separate neural networks for different parameters\\nof the probability distribution and\\n‚Ä¢ performing the processing in the YCoCg-R color space\\ninstead of the RGB color space,\\nwhich all help to improve compression performance and/or\\nreduce complexity, as detailed in Section III and IV.\\nThe remainder of the paper is organized as follows. Section\\nII reviews recent work on learned lossless image compression\\nwith scale-based auto-regressive models. Section III presents\\nthe proposed method of learned lossless image compression\\nthrough interpolation. Section IV presents experimental results\\nand comparisons and Section V concludes the paper.\\nII. SCALE-BASED AUTO-REGRESSIVE MODELS\\nIn L3C [18], which is the first to propose a scale-based\\nauto-regressive model, a three scale model is used. First, all\\nscale representations denoted z\\n(i)\\n, i = 1, 2, 3 are obtained at\\nthe encoder by processing the previous scale representation\\nwith encoder neural networks E(i)(.) and then quantizing the\\nscale representation variables to the nearest integer:\\nx\\n(i+1) ‚Üê E(i)\\n(x\\n(i)\\n),\\nz\\n(i+1) ‚Üê Q(x(i+1)) i = 0, 1, 2 (1)\\nThe original image is the initial scale representation x\\n(0) and\\nQ(.) is the quantization operator. Each scale representation\\nhas half the horizontal and vertical resolution of the previous\\nscale. The last scale‚Äôs probability distribution is assumed\\nuniform and each preceding scale‚Äôs probability distribution is\\nconditioned on the next scale:\\np(z\\n(3)) ‚àº uniform (2)\\np(z\\n(i‚àí1)|z(i)\\n) i = 3, 2 (3)\\np(x\\n(0)|z(1)) (4)\\nBased on the probability models, first, the last scale z\\n(3)\\nis transmitted by the encoder to the decoder with simple\\nfixed length codes. Then, starting with z\\n(3), both the encoder\\nand decoder process the scale representation z\\n(i) with neural\\nnetworks to obtain the conditional probability distribution of\\nthe previous scale p(z\\n(i‚àí1)|z(i)\\n) and encode/decode it. This is\\nrepeated until the original image is encoded/decoded.\\nThe lossless image compression through super resolution\\n(SReC) [19] paper has a framework similar to L3C but the\\nmajor differences are how the multiscale representations are\\nobtained and the progressive modeling of the probability distri\\x02bution of each scale. Unlike in L3C, the scale representations\\nx\\n(i)\\nare obtained simply by average pooling every 2x2 pixel\\ngroup of the previous scale and rounding to the nearest integer:\\nx\\n(i+1) ‚Üê AvgP ool2x2(x(i)\\n) i = 0, 1, 2 (5)\\nEach scale representation‚Äôs pixels are split to four groups based\\non even and odd rows and columns:\\n{x\\n(i)\\n00 , x\\n(i)\\n01 , x\\n(i)\\n10 , x\\n(i)\\n11 } ‚Üê x\\n(i)\\n(6)\\nStarting with the second last scale (i = 3), each scale‚Äôs\\nprobability distribution is progressively obtained by modeling\\neach group‚Äôs probability distribution conditioned on the next\\nscale and the previous groups in the same scale:\\np(x\\n(i‚àí1)\\n00 |x\\n(i)\\n), (7)\\np(x\\n(i‚àí1)\\n01 |x\\n(i)\\n, x\\n(i‚àí1)\\n00 ), (8)\\np(x\\n(i‚àí1)\\n10 |x\\n(i)\\n, x\\n(i‚àí1)\\n00 , x\\n(i‚àí1)\\n01 ), (9)\\nThe last group‚Äôs pixels (x\\n(i‚àí1)\\n11 ) are obtained from the first\\nthree groups and the next scale since the next scale was\\nobtained by average pooling:\\nx\\n(i‚àí1)\\n11 ‚Üê 4x\\n(i) ‚àí x\\n(i‚àí1)\\n00 ‚àí x\\n(i‚àí1)\\n01 ‚àí x\\n(i‚àí1)\\n10 + {‚àí1/0/1/2},\\n(10)\\nNote that some additional bits (represented by {‚àí1/0/1/2}\\nin the above equation) are transmitted by the encoder to accu\\x02rately find the x\\n(i‚àí1)\\n11 pixels due to the rounding operation in\\nthe average pooling. Finally, all groups can then be combined\\nto form the scale‚Äôs representation:\\nx\\n(i‚àí1) ‚Üê {x\\n(i‚àí1)\\n00 , x\\n(i‚àí1)\\n01 , x\\n(i‚àí1)\\n10 , x\\n(i‚àí1)\\n11 } (11)\\nEquations (7)-(11) are repeated for the next scales (i = 2, 1)\\nto complete the probability model of the entire image.\\nBased on the probability models, first, the last scale x\\n(3) is\\ntransmitted by the encoder to the decoder with simple fixed\\nlength codes. Then, both the encoder and decoder process the\\nscale representation x\\n(3) with neural networks to obtain the\\nconditional probability distribution of the initial group in the\\nprevious scale p(x\\n(i‚àí1)\\n00 |x\\n(i)\\n) and encode/decode it. Similar\\nprocessing with neural networks is performed to obtain the\\nconditional probability distribution of the other groups based\\non Equation (8) and (9) and they are encoded/decoded. Then\\nEquation (10) and (11) are applied to obtain the previous\\nscale representation x\\n(i‚àí1). Then the processing starts again\\nwith Equation (7) for the new scale. This is repeated until the\\noriginal image x\\n(0) is encoded/decoded.\\nIn MSPSM [20], the overall framework is similar to that\\nof SReC but the scale representations x\\n(i)\\nare obtained by\\nnot average pooling but simply splitting the original image\\ninto 4 groups based on even and rows and columns or 6\\ngroups for improved performance. Then the initial group is\\n3\\nsplit again to form the next scale. Starting with the initial\\ngroup in the last scale, each group‚Äôs probability distribution is\\nmodeled conditioned on the previous groups in a progressive\\nmanner. Note that since groups are obtained by splitting, once\\nall groups in a scale are decoded, the previous scale‚Äôs initial\\ngroup is readily available.\\nThe compression performance of L3C is better than all\\nclassical lossless compression systems except FLIF [4]. SReC\\nimproves the compression performance over L3C significantly\\nand outperforms also FLIF. MSPSM presents three models\\nwith increasing complexity and the compression performance\\nof the first one is similar to that of SReC while the latter\\ntwo have better performance. While L3C, SReC and MSPSM\\nprovide state of the art lossless compression performance and\\nreasonable encoding/decoding speeds on GPUs they require\\nneural networks with millions of parameters (L3C: 5.0M,\\nSReC: 4.2M, MSPSM:1.9M/9.9M) and high computational\\ncomplexity. This paper presents a scale-based auto-regressive\\nsystem that has similarities to SReC and MSPSM but sev\\x02eral modifications in the overall system and neural network\\ndesign are proposed, which all help to improve compression\\nperformance and/or reduce complexity, to achieve the same\\ncompression performance as SReC and the first model in\\nMSPSM with 10x (or more) less parameters and computational\\ncomplexity.\\nIII. LEARNED LOSSLESS IMAGE COMPRESSION TROUGH\\nINTERPOLATION (LLICTI)\\nThis section presents the proposed method of learned loss\\x02less image compression through interpolation (LLICTI) in four\\nsub-sections.\\nA. Overall System Architecture and Decoding Procedure\\nA multi-scale representation of the original input image is\\nobtained at the encoder by first splitting it into 4 subbands\\nbased on even and odd indices of the rows and columns.\\nThis splitting is then repeated on the first subband of each\\nscale to obtain a multi-scale representation of the original\\nimage. The obtained multi-scale representation with sub\\x02bands x\\n(i)\\nmn, where i denotes the scale index and (m, n) ‚àà\\n{(0, 0),(0, 1),(1, 0),(1, 1)} denote the subband index, can be\\nrepresented as follows if the original image is denoted x\\n(0)\\n00\\nand there are S scales (see also Figure 1):\\n{x\\n(i+1)\\n00 , x\\n(i+1)\\n01 , x\\n(i+1)\\n10 , x\\n(i+1)\\n11 } ‚Üê x\\n(i)\\n00 i = 0, ..., S ‚àí 1\\n(12)\\nFig. 1. Multi-scale representation of original image x\\n(0)\\n00 in LLICTI.\\nThe decoding procedure starts with the last scale by de\\x02coding the x\\n(S)\\n00 subband with fixed length codes of their\\npixel values in RGB color space. The pixels decoded with\\nfixed length codes is 1\\n4S of all original image pixels and is a\\nnegligible fraction for large S, such as S = 5 which we use\\nin our experiments.\\nThe processing steps to decode the remaining subbands in\\nthis scale are summarized in Figure 2. First, the x\\n(S)\\n00 subband\\nis processed at the decoder with an interpolator convolutional\\nneural network (iCNN(S)\\n11 ) to obtain the probability distri\\x02bution parameters of the next subband x\\n(S)\\n11 , which is then\\ndecoded by an entropy decoder from the bitstream. Next,\\nx\\n(S)\\n00 and x\\n(S)\\n11 subbands are the inputs to another interpolator\\nconvolutional neural network (iCNN(S)\\n01 ) to obtain the prob\\x02ability distribution parameters of the x\\n(S)\\n01 subband, which is\\nthen decoded. Then, in a similar manner, the x\\n(S)\\n10 subband is\\ndecoded. Finally, all the decoded subbands are combined to\\nobtain the initial subband of the next scale to decode:\\nx\\n(i‚àí1)\\n00 ‚Üê {x\\n(i)\\n00 , x\\n(i)\\n01 , x\\n(i)\\n10 , x\\n(i)\\n11 } i = S (13)\\nTo decode the next scales, the above described processing steps\\nare repeated for each scale (i = S ‚àí 1, ..., 1), yielding the\\ndecoding of the original image x\\n(0)\\n00 . The encoder performs\\nsimilar and appropriate processing to encode the image.\\nFig. 2. Decoding procedure for one scale. Three interpolator convolutional\\nneural networks (iCNN) are used in each scale.\\nNote that the described decoding procedure corresponds to\\nan interpolation based scheme which starts with an image grid\\nof every (2S)\\nth pixel (i.e. x\\n(S)\\n00 ) and interpolates progressively\\nmissing pixels of the grid populating the entire image, thus\\nthe name of the method.\\nB. Probability Model\\nThe probability model of an original image, upon which the\\nsystem architecture and decoding procedure in Section III-A\\nis based on, can be described as follows. The pixels in the\\ninitial subband of the last scale x\\n(S)\\n00 are assumed to follow a\\nuniform distribution:\\np(x\\n(S)\\n00 ) ‚àº uniform (14)\\nThe probability distribution of the initial subband of the\\npreceding scale p(x\\n(S‚àí1)\\n00 ) is then factorized as the product\\n4\\nof conditional distributions of each subband of the scale\\nconditioned progressively on preceding subbands:\\np(x\\n(i‚àí1)\\n00 ) = p(x\\n(i)\\n00 )√ó\\np(x\\n(i)\\n11 | x\\n(i)\\n00 )√ó\\np(x\\n(i)\\n10 | x\\n(i)\\n00 , x\\n(i)\\n11 )√ó\\np(x\\n(i)\\n01 | x\\n(i)\\n00 , x\\n(i)\\n11 , x\\n(i)\\n10 ), i = S (15)\\nNote that Equation (15) can be applied recursively for each\\nscale (i = S ‚àí 1, ..., 1) to factorizate the distribution of\\nthe original image x\\n(0)\\n00 into the conditional distributions of\\nsubbands, conditioned on previous subbands, with an initial\\nmarginal distribution given in Equation (14).\\nTo describe each conditional distribution in Equation (15),\\nlet Y\\n(i)\\nmn denote the collection of subbands that x\\n(i)\\nmn conditions\\non. Further, let x\\n(i)\\nmn[u, v] denote the pixel at spatial location\\n(u, v) and C\\n(i)\\nmn[u, v] denote the collection of pixels in Y\\n(i)\\nmn\\nthat the pixel x\\n(i)\\nmn[u, v] conditions on through the receptive\\nfield of the interpolator CNN (iCNN(i)\\nmn). Then each condi\\x02tional distribution in Equation (15) is factorized as follows:\\np(x\\n(i)\\nmn | Y\\n(i)\\nmn) = Y\\nu,v\\np(x\\n(i)\\nmn[u, v] | C\\n(i)\\nmn[u, v]),\\n(m, n) ‚àà {(1, 1),(1, 0),(0, 1)} (16)\\nTo describe the conditional distributions of each pixel in\\nEquation (16), let us simplify the notation by dropping the\\nspatial location, subband and scale indices and simply denote\\na pixel by x and the collection of pixels it is conditioned on by\\nC. Then, the conditional distribution of each pixel in Equation\\n(16) can be further factorized over distributions of its RGB\\ncolor components (x = {x\\n(r)\\n, x(g), x(b)}), i.e. sub-pixels1, as\\nfollows:\\np(x\\n(r)\\n, x(g), x(b)|C) = p(x\\n(r)\\n|C)√ó\\np(x\\n(g)\\n|C, x(r)) √ó\\np(x\\n(b)\\n|C, x(g), x(b)) (17)\\nEach conditional distribution in Equation (17) is modeled\\nwith a discretized Gaussian mixture model (GMM) given\\nbelow in Equation (18), where F(.) denotes the cumulative\\ndistribution function (CDF) of the standard Gaussian distribu\\x02tion and ¬µ\\n(c)\\ni\\n, œÉ\\n(c)\\ni\\nand œÄ\\n(c)\\ni\\nare the mean, standard deviation\\nand the weight of the i\\nth mixture of the GMM, respectively,\\nof sub-pixel x\\n(c) ‚àà {x(r)\\n, x(g), x(b)}:\\np(x\\n(c)\\n; œÄ\\n(c)\\n, ¬µ(c), œÉ(c)) = X\\nK\\ni=1\\nœÄ\\n(c)\\ni\\n[F(\\nx\\n(c) + 0.5 ‚àí ¬µ\\n(c)\\ni\\nœÉ\\n(c)\\ni\\n)\\n‚àíF(\\nx\\n(c) ‚àí 0.5 ‚àí ¬µ\\n(c)\\ni\\nœÉ\\n(c)\\ni\\n)]\\n(18)\\nThe GMM parameters ¬µ\\n(c)\\ni\\n, œÉ\\n(c)\\ni\\nand œÄ\\n(c)\\ni\\nare produced by\\nthe respective interpolator CNN processing the conditioning\\n1Here, we follow the convention in the related previous research and use\\nsub-pixel to denote each color component and pixel to denote all color\\ncomponents together\\npixels C. The conditioning of each sub-pixel on also the\\npreceding sub-pixels in Equation (17) is accomplished with\\nthe simple but efficient method of Salimans in PixelCNN++\\n[26], which simply updates the means by multiplication of\\npreceding sub-pixels with scaling coefficients ({a, b, c} = Œ±)\\nthat are produced also by the interpolator convolutional neural\\nnetwork:\\n¬µ\\n(r) =¬µ(r)\\n(C)\\n¬µ\\n(g) =¬µ(g)\\n(C) + a(C)x\\n(r)\\n¬µ\\n(b) =¬µ(b)(C) + b(C)x(r) + c(C)x(g)\\n(19)\\nC. General Neural Network Architecture of One Interpolator\\nThe neural network architecture used for all interpolators\\niCNN(i)\\nmn, i = 1, ..., S and (m, n) ‚àà {(1, 1),(1, 0),(0, 1)}\\nin the proposed LLICTI system is shown in Figure 3. It has\\nL layers, where layer 2 to L comprise convolution layers\\nwith 1x1 kernel size, stride of 1 and (except the last layer)\\nReLU activation functions. The first layer contains one, two or\\nthree convolutional layers, depending on how many previous\\nsubbands x\\n(i)\\nmn are inputs to this interpolator (see Figure 2).\\nThe outputs are added and processed with ReLU activation.\\nFig. 3. Architecture of interpolator Convolutional Neural Network at scale i\\nfor sub-band x10 (iCNN(i)\\n10 ). The architectures for the other two interpolators\\nof a scale, iCNN(i)\\n01 and iCNN(i)11 , are similar and lack the bottom and\\nbottom two convolutional layers in Layer 1, respectively.\\nSince all layers, except the first, have filters with kernel\\nsize 1x1, the receptive field of the iCNN or the pixels of\\nthe previous subbands used for interpolation are determined\\nby the kernel sizes of the first layer convolutions. Due to the\\nrelative positions of different subbands on the sampling grid,\\nthe kernel sizes were determined based on the input subband\\nand the subband to be interpolated, as shown in Figure 4.\\nTo train the parameters of all interpolator convolutional\\nneural networks, the cross entropy loss function\\nEq(x\\n(0)\\n00 )\\n[‚àí log p(x\\n(0)\\n00 )] (20)\\nis used, where q(.) is the true distribution of the image and\\np(.) is the distribution of the image obtained by the presented\\nprobability model in Equations (14)-(19). In practice, the loss\\nsimply becomes the sum of all ‚àí log p(.) quantities of all\\ninterpolated pixels in the training batch where p(.) are the\\nprobability distributions provided by the interpolators for each\\ninterpolated pixel.\\n5\\nFig. 4. Interpolator layer 1 kernel sizes. x\\n(i)\\n00 (4x4) ‚Üí x\\n(i)\\n11\\nx\\n(i)\\n00 (3x4), x\\n(i)\\n11 (4x3) ‚Üí x\\n(i)\\n01 x\\n(i)\\n00 (4x3), x\\n(i)\\n11 (3x4), x\\n(i)\\n01 (4x4) ‚Üí x\\n(i)\\n10\\nD. System and NN Architecture Parameters to Analyze\\nThe proposed LLICTI system described so far has 3 in\\x02terpolators (iCNN) per scale, for a total of 3S interpolators\\noverall, where S is the total number of scales. There are many\\nparameters of the overall system and the iCNN architecture\\nthat we will analyze. In Section IV-B, we examine\\n‚Ä¢ whether compressing pixels in the RGB or YCoCg-R\\ncolor space,\\n‚Ä¢ whether obtaining probability distribution parameters\\nœÄ, ¬µ, œÉ, Œ± using a single or using multiple independent\\ninterpolator CNNs\\n‚Ä¢ whether using the same interpolator CNN (i.e. with same\\nweights) in several scales\\nperforms better in terms of compression-complexity trade\\x02off. We also examine standard neural network architecture\\nparameters such as, the number of layers and the number\\nof channels of each layer for better compression-complexity\\ntrade-off.\\nIV. EXPERIMENTS AND RESULTS\\nThis section first presents experimental settings and then\\nprovides experimental analyses and results of the proposed\\nlearned lossless image compression through interpolation\\n(LLICTI) systems. Comparisons with the literature in terms of\\ncompression performance, number of used parameters, encod\\x02ing/decoding times and computational complexity estimates\\nare also provided.\\nA. Experimental Settings\\nAll LLICTI systems to be discussed in Section IV-B are\\ntrained with the Open Images training dataset prepared by\\nthe authors of L3C [18]. The Adam optimizer [27] is used\\nfor optimization with a batch size of 64. The learning rate is\\ninitialized to 10‚àí4and is reduced with a decay factor of 0.5\\n(up to a minimum of 10‚àí5) when the validation cost plateaus.\\nThe training takes typically around 500K weight updates.\\nThe LLICTI systems are tested with the Open Images test\\ndataset prepared again by the authors of L3C [18], which\\nincludes 500 images. The arithmetic coder provided by [18],\\nwhich runs on the CPU [28], is used for encoding to and\\ndecoding from a bitstream during the tests. Note that all\\nneural network operations are performed on the GPU and the\\nobtained probability tables are copied to the CPU memory and\\nthe arithmetic encoding/decoding operations are performed on\\nthe CPU in our tests of LLICTI. PyTorch [29] is used to\\nimplement both training and testing. Our codes are available\\non GitHub [30].\\nB. Experimental Analysis and Ablation Study\\nThe following experiments summarized in Table I are\\nconducted to analyze several parameters of the overall com\\x02pression system and the interpolator CNN architecture, as dis\\x02cussed in Section III-D. Note that all interpolators (iCNN(i)\\nmn\\ni = 1, ..., S and (m, n) ‚àà {(1, 1),(1, 0),(0, 1)}) can be\\ntrained independently from each other since their inputs can\\nbe obtained directly from the original image and their outputs\\ncontribute to the cost (i.e. total bitrate) independently. Consid\\x02ering also the fact that compression of scale 1 subbands x\\n(1)\\nmn,\\n(m, n) ‚àà {(1, 1),(1, 0),(0, 1)} is more important (as they\\ncomprise 4 times as many pixels as scale 2, which comprises\\n4 times as many pixels as scale 3 etc) than that of other\\nscale subbands, the initial experiments discussed below are\\nconducted by compressing only scale 1 subbands (i.e. x\\n(1)\\n00 is\\nassumed available and x\\n(1)\\n11 , x\\n(1)\\n01 , x\\n(1)\\n10 are compressed).\\nNote that a primary goal of the LLICTI method is to obtain\\na lossless image compression system that has significantly\\nless complexity than other similar systems in the literature\\nwhile achieving the best possible compression performance.\\nIn particular, the goal is to have at least 10x less parameters\\nin the LLICTI system than the smallest neural network in\\nthe related literature, which is the normal model in MSPSM\\nwith 1.8M parameters. The parameters, in particular number\\nof layers and channels, of the LLICTI systems in the following\\nexperiments were chosen to chase that goal.\\nThe first conducted experiment is experiment 1, which can\\nbe considered the default system and compresses pixels in the\\nRGB color space, uses a single (i.e. joint) interpolator CNN\\nto obtain all probability distribution parameters (œÄ, ¬µ, œÉ, Œ±),\\nhas L = 4 layers in the interpolator CNNs and has C = 96\\nchannels in each convolutional layer. This system achieves\\na compression performance of 5.781 bits per pixel (bpp)\\nand requires 93K parameters. Note that the 5.781 bpp is\\nobtained by dividing the total number of bits used for encoding\\nx\\n(1)\\n11 , x\\n(1)\\n01 and x\\n(1)\\n10 by the total number of pixels in the original\\nimage, and the bits required for compressing x\\n(1)\\n00 are not\\nconsidered in this and following similar experiments.\\nNext, it is investigated whether/how the color space of\\ninput pixels can improve compression-complexity trade-off\\nand experiment 2 is conducted, where the original image\\nis converted to the YCoCg-R color space [31], which is\\ninteger-to-integer invertible, and then compressed. While the\\ninterpolator CNNs can learn the dependencies in the RGB\\ncolor space and account for them, it may be beneficial in\\nterms of compression-complexity trade-off to provide the input\\nto the neural networks in a decorrelated color space and\\nallocate neural network capacity to learn other dependencies.\\nThe system in experiment 2 achieves an improved compres\\x02sion performance of 5.736 bpp and requires the same 93K\\nparameters. Hence, YCoCg-R color space is used in the next\\nexperiments.\\n6\\nTABLE I\\nANALYSIS OF SYSTEM AND NEURAL NETWORK ARCHITECTURE PARAMETERS\\nExperiment # 1 2 3 4 5 6 7 8 9 10 11 12 13 14\\n(Reference Experiment) (1) (2) (3) (4) (5) (4) (7) (8) (9) (10) (11) (12) (13)\\nColor space: RGB/YCoCg-R RGB YCC YCC YCC YCC YCC YCC YCC YCC YCC YCC YCC YCC YCC\\n(œÄ, ¬µ, œÉ, Œ±): Joint/Separate JNT JNT SEP SEP SEP SEP SEP SEP SEP SEP SEP SEP SEP SEP\\n# Layers in iCNN 4 4 4 3 2 2 3 3 3 3 3 3 3 3\\n# Channels\\nScale 1 96 96 44 44 44 64 60 - - - - - 60 88\\nScale 2 - - - - - - - 44 32 - - - ‚Üë ‚Üë\\nScale 3 - - - - - - - - - 36 24 24 - ‚Üë\\nScale 4 - - - - - - - - - 30 20 ‚Üë - ‚Üë\\nScale 5 - - - - - - - - - 30 20 ‚Üë - ‚Üë\\nShare iCNN across scales - - - - - - - - - F F T T T\\nCompression (bpp)\\nScale 1 5.781 5.736 5.607 5.605 5.658 5.624 5.524 - - - - - 5.525 5.465\\nScale 2 - - - - - - - 1.852 1.862 - - - 1.868 1.841\\nScale 3 - - - - - - - - - 0.551 0.556 0.556 - 0.553\\nScale 4 - - - - - - - - - 0.158 0.159 0.158 - 0.160\\nScale 5 - - - - - - - - - 0.044 0.044 0.044 - 0.045\\nTotal # Parameters (√ó103) 93 93 94 72 49 72 109 72 48 144 88 34 109 188\\nNext, it is investigated whether each interpolator CNN\\nshould comprise a single CNN that produces all parameters\\n(œÄ, ¬µ, œÉ, Œ±) required for the probability distribution model or\\nwhether each interpolator should comprise 4 separate CNNs,\\neach producing one of the parameters. Since each of these\\nparameters affect the probability distribution (and thus the\\ncost function) in different ways, learning them trough separate\\nneural networks could be better in terms of compression\\x02complexity trade-off. Hence, experiment 3 is conducted, which\\ndiffers from its reference (experiment 2), by the use of 4\\nseparate CNNs for each interpolator and the reduced number\\nof channels (from 96 to 44) in each CNN to keep the total\\nnumber of parameters similar. The system in experiment 3\\nachieves an improved compression performance of 5.607 bpp\\nand requires a similar 94K parameters. Hence, in the next\\nexperiments, for each interpolator, 4 separate CNNs are used,\\neach of which produces one of the parameters (œÄ, ¬µ, œÉ, Œ±).\\nNext, the number of layers and channels in the interpolator\\nCNNs are examined in experiments 4-7. In experiment 4,\\nthe number of layers in the interpolator CNNs are reduced\\nfrom 4 to 3 and almost the same compression performance\\nof 5.605 bpp as reference experiment 3 is achieved, while\\nthe required number of parameters drops from 94K to 72K.\\nBased on this result, the number of layers in the interpolator\\nCNNs are further reduced to 2 in experiment 5, however, the\\ncompression performance deteriorated to 5.658 bpp while 49K\\nparameters were required. Thus experiment 6 is conducted,\\nwhere the number of layers is kept at 2 but the number of\\nchannels is increased to 64 to achieve the same number of\\nparameters (72K) as experiment 4. The achieved compression\\nperformance is 5.624 bpp, which is inferior to the result of\\nexperiment 4. Hence, experiment 7 is conducted, which keeps\\nthe number of layers at 3 and increases the number of channels\\nto 60 to achieve a compression performance of 5.524 bpp with\\n109K required parameters.\\nIt is now assumed that several parameters of the overall\\nsystem and the interpolator CNN architecture have been identi\\x02fied for better compression-complexity trade-off. In particular,\\ncompressing pixels in the YCoCg-R space, using 4 separate\\nCNNs for each interpolator to obtain parameters (œÄ, ¬µ, œÉ, Œ±)\\nand using 3 layers in the interpolator CNNs have been iden\\x02tified to provide better compression-complexity trade-off and\\nwill be used in the next experiments.\\nNext, in experiments 8 and 9, compressing scale 2 sub\\x02bands (i.e. x\\n(2)\\n00 is assumed available and x\\n(2)\\n11 , x\\n(2)\\n01 , x\\n(2)\\n10 are\\ncompressed) is considered and only the number of channels\\nin the interpolator CNNs are examined. In experiment 8, 44\\nchannels are used to achieve a compression performance of\\n1.852 bpp with 72K required parameters. In experiment 9,\\nthe number of channels is reduced to 32 and a slightly worse\\ncompression performance of 1.862 bpp is obtained with 48K\\nrequired parameters.\\nIn experiments 10 and 11, scale 3, 4, and 5 subbands are\\ncompressed. In experiment 10, 36, 30 and 30 channels are used\\nto achieve a compression performance of 0.551 bpp, 0.158\\nbpp and 0.044 bpp for scales 3, 4 and 5, respectively, with a\\ntotal 144K required parameters. In experiment 11, the number\\nof channels is reduced to 24, 20 and 20 for scales 3, 4 and\\n5, respectively, and the obtained compression performance is\\nslightly inferior but the required total number of parameters\\ndrops by a significant amount to 88K.\\nNext, it is investigated whether using the same interpolator\\nCNN (i.e. with same weights) in several scales is beneficial for\\ncompression-complexity trade-off. Although the dependency\\nof pixels in different scales may be different, it is likely\\nthat there are significant amount of common features and\\nsharing interpolator CNNs across scales may improve the\\ncompression-complexity trade-off. Hence, in experiment 12,\\nthe interpolator CNNs for scale 3 with 24 channels are\\nshared across scales 3, 4 and 5 (Note that they are trained\\nfrom scratch using the total cost of the three scales.) The\\ncompression performance is almost identical to that of the\\nreference experiment 11 for all scales but the total number of\\nrequired parameters drops significantly from 88K to 34K. This\\nis a surprising and significant result. Inspired by this result,\\nexperiment 13 is conducted, where the interpolator CNNs\\n(with the same 60 channels as in experiment 7) are shared\\nacross scales 1 and 2. The obtained compression performance\\nof 5.525 bpp for scale 1 is identical to that of experiment 7\\nand scale 2 is compressed at a competitive 1.868 bpp (see\\n7\\nexperiments 8, 9). Hence, sharing interpolator CNNs across\\nscale 1 and 2 proved to be also very beneficial in terms of\\ncompression-complexity trade-off.\\nFinally, experiment 14 is conducted, where the same in\\x02terpolator CNNs are shared across all scales 1-5 and the\\nnumber of channels in the convolution layers is set to 88\\nto achieve a total 188K parameters, which is 10x less (our\\ninitial goal) than the smallest neural network in the literature\\n[20]. The compression performance is the best so far in the\\nexperiments for scales 1 and 2, achieving 5.465 bpp and\\n1.841 bpp, respectively. The compression performance for the\\nremaining scales 3-5 is also very competitive as can be seen\\nin Table I. The total bitrate for all scales becomes 8.064 bpp\\n(which dos not include the 0.023 bpsp bitrate of x\\n(5)\\n00 that is\\nto be coded with fixed-length codes of 8 bits per symbol.)\\nHence, the system and the neural network architecture for the\\ninterpolators in experiment 14 is chosen to be the best in terms\\nof compression-complexity trade-off and is compared to other\\nsystems in the literature in the next section.\\nC. Comparisons with Related Literature\\nThis section compares the proposed LLICTI to other loss\\x02less image compression systems in the literature in terms\\nof compression performance, number of required parameters,\\nencoding/decoding times and computational complexity. The\\ncomparison results are summarized in Table II. (Note that in\\nthese results of the LLICTI system, all necessary informa\\x02tion is written into the bitstream including x\\n(5)\\n00 and control\\ninformation, such as the number of scales, height and width\\nof every scale and the image. The LLICTI system can also\\ncompress arbitrary sized images.)\\nTABLE II\\nCOMPARISONS WITH RELATED LITERATURE\\nMethod bpsp(bpp) Param- Enc/Dec Enc/Dec\\neters Computat. Time\\n(KMAC/pix) (sec)\\nTraditional\\nMethods\\nPNG 4.01(12.03) - - -\\nJPEG2000 3.06(9.18) - - -\\nWebP 3.05(9.15) - - -\\nFLIF [4] 2.87(8.61) - - -\\nLearning\\nBased\\nMethods\\nL3C [18] 2.99(8.97) 5.01 M 686/428 0.67/0.61\\nSReC [19] 2.70(8.10) 4.20 M 878/878 0.56/0.59\\nMSPSM(norm)[20] 2.71(8.14) 1.87 M - -\\nMSPSM(big) [20] 2.63(7.88) 1.87 M - -\\nLLICTI (Ours) 2.70(8.10) 0.19 M 66/66 0.53/0.70\\n1) Compression performance and number of parameters:\\nThe compression performance results for the Open Images\\ntest dataset [18] are given in terms of bits per sub-pixel\\n(bpsp) and bits per pixel (bpp). The best performing traditional\\nmethod is FLIF [4] and it achieves an average compression\\nperformance of 2.87 bpsp. Among the learning based methods,\\nthe first proposed scale-based auto-regressive method L3C\\n[18] achieves a compression performance of 2.99 bpsp. Next,\\nSReC [19], MSPSM(normal) [20] and the proposed LLICTI\\nachieve a compression performance of 2.70 bpsp. A common\\nfeature of these three scale-based auto-regressive methods\\nis that they perform the compression/probability-modeling\\nof each scale in 3 progressive steps. MSPSM(big) achieves\\nan improved compression performance of 2.63 bpsp as it\\nperforms the compression/probability-modeling of each scale\\nin 6 progressive steps [20].\\nIf the learned systems are compared in terms of number\\nof parameters of the neural networks that they use, it can be\\nseen that the proposed LLICTI method uses 0.19 M (188K)\\nparameters, which is about 10x less than MSPSM, 22x less\\nthan SReC and 26x less than L3C. Despite the significantly\\nless number of parameters of LLICTI, compression perfor\\x02mance similar to these methods was achieved. The improved\\ncompression-complexity trade-off of LLICTI is due to several\\nfactors, such as simpler neural network design, sharing the\\nsame interpolator neural networks across different scales,\\nusing separate neural networks for different parameters of the\\nprobability distribution and performing the compression in the\\nYCoCg-R color space instead of the RGB color space.\\n2) Computational complexity and encoding/decoding times:\\nThe computational complexity of the neural network cal\\x02culations for encoding and decoding in the learning based\\nsystems are compared using the multiply-accumulate (MAC)\\noperations counter tool in [32]. As shown in Table II, the\\nproposed LLICTI system requires more than 10x less MACs\\nthan L3C and SReC for both encoding and decoding, which is\\nmainly due to the less number of parameters of the used neural\\nnetworks. (Note that MSPSM is not included in computation\\nor encoding/decoding times comparisons since their codes are\\nnot shared.)\\nThe average encoding and decoding time of an image\\n(averaged over all 500 images on the Open Images test dataset\\nwhere the average image resolution is ‚àº768x576) measured\\non our computing system with an NVIDIA GeForce RTX\\n2080 GPU and Intel i7-9700 CPU are also given in Table\\nII. L3C, SReC and the proposed LLICTI achieve similar\\naverage encoding/decoding times under one second and close\\nto half a second. However, the implementations of L3C and\\nSReC enjoy an advantage over that of LLICTI. In particular,\\nimplementations of L3C and SReC don‚Äôt copy the calcu\\x02lated CDFs (necessary for arithmetic coding on the CPU)\\nfrom GPU to CPU but use a custom CUDA kernel to store\\nthe CDFs in managed/unified memory accessible from both\\nCPU and GPU (see L3C appendix [18] and GitHub page\\n[33].) On the other hand, LLICTI implementation moves the\\nCDF tensors from GPU to CPU for arithmetic coding. If\\nthe LLICTI implementation would also store CDF tensors\\nin unified/managed memory, its encoding/decoding times are\\nlikely to decrease. Lastly, while encoding/decoding times of\\nMSPSM could not be obtained on our computing system (since\\ntheir codes are not shared), it is reported in their paper [20]\\nthat their encoding/decoding times are about 20% and 100%\\nlonger than those of SReC, for their normal and big systems,\\nrespectively.\\nV. CONCLUSIONS\\nThis paper presented a learned lossless image compression\\nmethod based on a progressive interpolation scheme. First, a\\nsubset of pixels of the original image grid (e.g. every 32nd\\npixel horizontally and vertically) is encoded/decoded using\\n8\\nsimple fixed-length codes. Then, finer sub-sampled grids of\\nthe original image grid are progressively interpolated (i.e.\\ntheir probability distributions are obtained) conditioned on\\npreviously encoded/decoded grids so far, to encode/decode all\\npixels of the image grid. The interpolators are neural network\\nbased and learned from a dataset.\\nThe presented method was shown to achieve better than or\\non par compression performance with the recent scale-based\\nauto-regressive models literature, yet required 10x or more\\nless neural network parameters and encoding/decoding com\\x02putation complexity. The improved compression-complexity\\ntrade-off was attributed to several contributions/findings in the\\noverall system and neural network architecture design, such as\\nusing the same interpolator neural networks in different scales,\\nusing separate neural networks for different parameters of the\\nprobability distribution and performing the processing in the\\nYCoCg-R color space instead of the RGB color space.\\nACKNOWLEDGMENT\\nWe would like to thank Sinem Gumus for obtaining the\\nencoding/decoding times of several compression methods on\\nthe same computing system.\\nREFERENCES\\n[1] W. B. Pennebaker and J. L. Mitchell, JPEG: Still image data compres\\x02sion standard. Springer Science & Business Media, 1992.\\n[2] T. Boutell and T. Lane, ‚ÄúPng (portable network graphics) specification\\nversion 1.0,‚Äù Network Working Group, pp. 1‚Äì102, 1997.\\n[3] M. J. Weinberger, G. Seroussi, and G. Sapiro, ‚ÄúThe loco-i lossless\\nimage compression algorithm: Principles and standardization into jpeg\\x02ls,‚Äù IEEE Transactions on Image processing, vol. 9, no. 8, pp. 1309‚Äì\\n1324, 2000.\\n[4] J. Sneyers and P. Wuille, ‚ÄúFlif: Free lossless image format based on\\nmaniac compression,‚Äù in 2016 IEEE international conference on image\\nprocessing (ICIP). IEEE, 2016, pp. 66‚Äì70.\\n[5] Google, ‚ÄúAn image format for the web,‚Äù https://developers.google.com/\\nspeed/webp, 2022, last accessed 12 October 2022.\\n[6] Y.-L. Lee, K.-H. Han, and G. J. Sullivan, ‚ÄúImproved lossless intra\\ncoding for h. 264/mpeg-4 avc,‚Äù IEEE Transactions on Image Processing,\\nvol. 15, no. 9, pp. 2610‚Äì2615, 2006.\\n[7] M. Zhou, W. Gao, M. Jiang, and H. Yu, ‚ÄúHevc lossless coding and\\nimprovements,‚Äù IEEE Transactions on Circuits and Systems for Video\\nTechnology, vol. 22, no. 12, pp. 1839‚Äì1843, 2012.\\n[8] A. R. Calderbank, I. Daubechies, W. Sweldens, and B.-L. Yeo, ‚ÄúLossless\\nimage compression using integer to integer wavelet transforms,‚Äù in\\nProceedings of International Conference on Image Processing, vol. 1.\\nIEEE, 1997, pp. 596‚Äì599.\\n[9] F. Kamisli, ‚ÄúLossless image and intra-frame compression with integer\\x02to-integer dst,‚Äù IEEE Transactions on Circuits and Systems for Video\\nTechnology, vol. 29, no. 2, pp. 502‚Äì516, 2017.\\n[10] S. R. Alvar and F. Kamisli, ‚ÄúOn lossless intra coding in hevc with 3-tap\\nfilters,‚Äù Signal Processing: Image Communication, vol. 47, pp. 252‚Äì262,\\n2016.\\n[11] D. Le Gall and A. Tabatabai, ‚ÄúSub-band coding of digital images using\\nsymmetric short kernel filters and arithmetic coding techniques,‚Äù in\\nICASSP-88., International Conference on Acoustics, Speech, and Signal\\nProcessing. IEEE, 1988, pp. 761‚Äì764.\\n[12] M. Rabbani and R. Joshi, ‚ÄúAn overview of the jpeg 2000 still image com\\x02pression standard,‚Äù Signal processing: Image communication, vol. 17,\\nno. 1, pp. 3‚Äì48, 2002.\\n[13] C. Christopoulos, A. Skodras, and T. Ebrahimi, ‚ÄúThe jpeg2000 still\\nimage coding system: an overview,‚Äù IEEE transactions on consumer\\nelectronics, vol. 46, no. 4, pp. 1103‚Äì1127, 2000.\\n[14] J. Balle, V. Laparra, and E. P. Simoncelli, ‚ÄúEnd-to-end optimized image ¬¥\\ncompression,‚Äù arXiv preprint arXiv:1611.01704, 2016.\\n[15] J. Balle, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, ‚ÄúVari- ¬¥\\national image compression with a scale hyperprior,‚Äù arXiv preprint\\narXiv:1802.01436, 2018.\\n[16] D. Minnen, J. Balle, and G. D. Toderici, ‚ÄúJoint autoregressive and ¬¥\\nhierarchical priors for learned image compression,‚Äù Advances in neural\\ninformation processing systems, vol. 31, 2018.\\n[17] D. Minnen and S. Singh, ‚ÄúChannel-wise autoregressive entropy models\\nfor learned image compression,‚Äù in 2020 IEEE International Conference\\non Image Processing (ICIP). IEEE, 2020, pp. 3339‚Äì3343.\\n[18] F. Mentzer, E. Agustsson, M. Tschannen, R. Timofte, and L. Van Gool,\\n‚ÄúPractical full resolution learned lossless image compression,‚Äù in 2019\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR). IEEE Computer Society, 2019, pp. 10 621‚Äì10 630.\\n[19] S. Cao, C.-Y. Wu, and P. Krahenb ¬® uhl, ‚ÄúLossless image compression ¬®\\nthrough super-resolution,‚Äù arXiv preprint arXiv:2004.02872, 2020.\\n[20] H. Zhang, F. Cricri, H. R. Tavakoli, N. Zou, E. Aksu, and M. M. Han\\x02nuksela, ‚ÄúLossless image compression using a multi-scale progressive\\nstatistical model,‚Äù in Proceedings of the Asian Conference on Computer\\nVision, 2020.\\n[21] A. B. Koyuncu, H. Gao, A. Boev, G. Gaikov, E. Alshina, and E. Stein\\x02bach, ‚ÄúContextformer: A transformer with spatio-channel attention for\\ncontext modeling in learned image compression,‚Äù in European Confer\\x02ence on Computer Vision. Springer, 2022, pp. 447‚Äì463.\\n[22] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-\\nR. Ohm, ‚ÄúOverview of the versatile video coding (vvc) standard and\\nits applications,‚Äù IEEE Transactions on Circuits and Systems for Video\\nTechnology, vol. 31, no. 10, pp. 3736‚Äì3764, 2021.\\n[23] I. Schiopu and A. Munteanu, ‚ÄúDeep-learning-based lossless image cod\\x02ing,‚Äù IEEE Transactions on Circuits and Systems for Video Technology,\\nvol. 30, no. 7, pp. 1829‚Äì1842, 2020.\\n[24] S. Gumus and F. Kamisli, ‚ÄúA learned pixel-by-pixel lossless image\\ncompression method with 59k parameters and parallel decoding,‚Äù arXiv\\npreprint arXiv:2212.01185, 2022.\\n[25] E. Hoogeboom, J. W. Peters, R. van den Berg, and M. Welling, ‚ÄúInteger\\ndiscrete flows and lossless compression,‚Äù in Proceedings of the 33rd\\nInternational Conference on Neural Information Processing Systems,\\n2019, pp. 12 145‚Äì12 155.\\n[26] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, ‚ÄúPixelCNN++:\\nImproving the pixelCNN with discretized logistic mixture likelihood\\nand other modifications,‚Äù in International Conference on Learning\\nRepresentations, 2017. [Online]. Available: https://openreview.net/\\nforum?id=BJrFC6ceg\\n[27] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù\\narXiv preprint arXiv:1412.6980, 2014.\\n[28] F. Mentzer, ‚Äútorchac: Fast arithmetic coding for pytorch,‚Äù https://github.\\ncom/fab-jul/torchac, Dec. 2022.\\n[29] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., ‚ÄúPytorch: An\\nimperative style, high-performance deep learning library,‚Äù Advances in\\nneural information processing systems, vol. 32, 2019.\\n[30] F. Kamisli, ‚ÄúLearned lossless image compression through interpolation,‚Äù\\nhttps://github.com/metu-kamisli/LLICTI, Dec. 2022.\\n[31] H. Malvar and G. Sullivan, ‚ÄúYcocg-r: A color space with rgb reversibility\\nand low dynamic range,‚Äù ISO/IEC JTC1/SC29/WG11 and ITU-T SG16\\nQ, vol. 6, 2003.\\n[32] V. Sovrasov, ‚ÄúFlops counter for convolutional networks in pytorch frame\\x02work,‚Äù https://github.com/sovrasov/flops-counter.pytorch, Nov. 2022,\\nlast accessed November 2022.\\n[33] F. Mentzer, ‚ÄúPractical full resolution learned lossless image compres\\x02sion,‚Äù https://github.com/fab-jul/L3C-PyTorch, Dec. 2022.', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://arxiv.org/abs/2404.19755', id='https://arxiv.org/abs/2404.19755', title='Analysis and Enhancement of Lossless Image Compression in JPEG-XL', score=0.39512085914611816, published_date='2024-04-30T17:58:06.000Z', author='Mamedov; Rustam', image=None, favicon=None, subpages=None, extras=None, text=\"\\n \\n \\n Computer Science &gt; Information Theory \\n \\n arXiv:2404.19755 (cs)\\n \\n \\n \\n [Submitted on 30 Apr 2024] \\n View PDF \\nAs the demand for digital information grows in fields like medicine, remote sensing, and archival, efficient image compression becomes crucial. This paper focuses on lossless image compression, vital for managing the increasing volume of image data without quality loss. Current research emphasizes techniques such as predictive coding, transform coding, and context modeling to improve compression ratios. This study evaluates lossless compression in JPEG XL, the latest standard in the JPEG family, and aims to enhance its compression ratio by modifying the codebase. Results show that while overall compression levels are below the original codec, one prediction method improves compression for specific image types. This study offers insights into enhancing lossless compression performance and suggests possibilities for future advancements in this area.\\n \\n \\n \\n Submission history From: Rustam Mamedov [ view email] [v1] \\n Tue, 30 Apr 2024 17:58:06 UTC (803 KB) \\n \\n \\n \\n \\n Current browse context: cs.IT \\n \\n \\n \\n \\n Bookmark \\n \\n \\n \\n \\n \\n \\n Bibliographic and Citation Tools \\n \\n \\n \\n \\n \\n \\n \\n Code, Data and Media Associated with this Article \\n \\n \\n \\n Demos \\n \\n \\n \\n Recommenders and Search Tools \\n \\n \\n \\n arXivLabs: experimental projects with community collaborators \\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. \\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. \\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. \\n \\n \\n ||||I|||| Skip to main content\\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\\n > cs > arXiv:2404.19755\\n\\n Help | Advanced Search\\n\\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\\n Search\\n GO\\n\\n quick links\\n\\n * Login\\n * Help Pages\\n * About\\n\\n Computer Science > Information Theory\\n\\n arXiv:2404.19755 (cs)\\n [Submitted on 30 Apr 2024]\\n\\n Title: Analysis and Enhancement of Lossless Image Compression in JPEG-XL\\n\\n Authors: Rustam Mamedov\\n View a PDF of the paper titled Analysis and Enhancement of Lossless Image Compression in JPEG-XL, by Rustam Mamedov\\n View PDF\\n Abstract: As the demand for digital information grows in fields like medicine, remote sensing, and archival, efficient image compression becomes crucial. This paper focuses on lossless image compression, vital for managing the increasing volume of image data without quality loss. Current research emphasizes techniques such as predictive coding, transform coding, and context modeling to improve compression ratios. This study evaluates lossless compression in JPEG XL, the latest standard in the JPEG family, and aims to enhance its compression ratio by modifying the codebase. Results show that while overall compression levels are below the original codec, one prediction method improves compression for specific image types. This study offers insights into enhancing lossless compression performance and suggests possibilities for future advancements in this area.\\n Subjects: Information Theory (cs.IT) \\n Cite as: arXiv:2404.19755 [cs.IT] \\n (or arXiv:2404.19755v1 [cs.IT] for this version)\\n \\n\\n Submission history\\n\\n From: Rustam Mamedov [view email]\\n [v1] Tue, 30 Apr 2024 17:58:06 UTC (803 KB)\\n Full-text links:\\n\\n Access Paper:\\n\\n View a PDF of the paper titled Analysis and Enhancement of Lossless Image Compression in JPEG-XL, by Rustam Mamedov\\n * View PDF\\n * TeX Source\\n * Other Formats\\n view license\\n Current browse context:\\n cs.IT\\n < prev | next >\\n new | recent | 2404\\n Change to browse by:\\n cs\\n math\\n math.IT\\n\\n References & Citations\\n\\n * NASA ADS\\n * Google Scholar\\n * Semantic Scholar\\n a export BibTeX citation Loading...\\n\\n BibTeX formatted citation\\n\\n √ó\\n loading...\\n Data provided by:\\n\\n Bookmark\\n\\n Bibliographic Tools\\n\\n Bibliographic and Citation Tools\\n\\n Bibliographic Explorer Toggle\\n Bibliographic Explorer (What is the Explorer?)\\n Litmaps Toggle\\n Litmaps (What is Litmaps?)\\n scite.ai Toggle\\n scite Smart Citations (What are Smart Citations?)\\n Code, Data, Media\\n\\n Code, Data and Media Associated with this Article\\n\\n Links to Code Toggle\\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\\n DagsHub Toggle\\n DagsHub (What is DagsHub?)\\n GotitPub Toggle\\n Gotit.pub (What is GotitPub?)\\n Links to Code Toggle\\n Papers with Code (What is Papers with Code?)\\n ScienceCast Toggle\\n ScienceCast (What is ScienceCast?)\\n Demos\\n\\n Demos\\n\\n Replicate Toggle\\n Replicate (What is Replicate?)\\n Spaces Toggle\\n Hugging Face Spaces (What is Spaces?)\\n Spaces Toggle\\n TXYZ.AI (What is TXYZ.AI?)\\n Related Papers\\n\\n Recommenders and Search Tools\\n\\n Link to Influence Flower\\n Influence Flower (What are Influence Flowers?)\\n Connected Papers Toggle\\n Connected Papers (What is Connected Papers?)\\n Core recommender toggle\\n CORE Recommender (What is CORE?)\\n About arXivLabs\\n\\n arXivLabs: experimental projects with community collaborators\\n\\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\\n * About\\n * Help\\n * Click here to contact arXiv Contact\\n * Click here to subscribe Subscribe\\n * Copyright\\n * Privacy Policy\\n * Web Accessibility Assistance\\n\\n * arXiv Operational Status\\n Get status notifications via email or slack\", highlights=None, highlight_scores=None, summary=None)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Exa Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEATOR_PROMPT = \"\"\"\n",
    "You are a highly esteemed expert researcher and visionary innovator, specializing in information theory, machine learning, and data compression. A groundbreaking research paper, \"Understanding is Compression,\" has just been published, introducing the LMCompress framework. Your task is to deeply analyze this breakthrough and propose 3-5 specific, actionable, and high-impact novel ideas that could exploit its core principles.\n",
    "Breakthrough Summary (Paper: \"Understanding is Compression\"):\n",
    "The paper presents LMCompress, a novel lossless compression framework. Unlike traditional methods, LMCompress uses large language models (LLMs) as data understanding engines rather than direct encoders. Inspired by Solomonoff induction (a theoretically optimal but uncomputable prediction method based on understanding), the core argument is that superior understanding leads to superior compression.\n",
    "LMCompress Mechanism:\n",
    "Tokenization: Input data (text, image, video, audio) is tokenized.\n",
    "LLM Probability Estimation: A generative LLM estimates the probability distribution for each token.\n",
    "Arithmetic Coding: Probabilities from the LLM are used for arithmetic coding.\n",
    "Domain Specialization: Models fine-tuned on specific domains (e.g., iGPT for images, LLaMA for text/audio) enhance \"understanding\" and compression for that data type.\n",
    "Key Breakthrough Aspects:\n",
    "Exceeding Shannon Limits: LMCompress surpasses traditional Shannon-based compression limits by leveraging LLMs to approximate uncomputable data patterns.\n",
    "Superior Performance: It consistently outperforms state-of-the-art methods (doubling/quadrupling compression ratios for images vs. JPEG-XL, audio vs. FLAC, video vs. H.264, text vs. bz2).\n",
    "Kolmogorov Paradigm Shift: This marks a shift from frequency-based, computable compression to model-based, semantic compression. The essence is understanding and modeling the source, not just statistical redundancy removal.\n",
    "Implications: Significant potential for future communication systems (e.g., 6G) by mitigating bandwidth limits through deeper data understanding at both ends.\n",
    "\n",
    "Your Task: Generate Novel Ideas\n",
    "\n",
    "Based on this breakthrough, please suggest 3-5 queries to a search engine to pull \n",
    "Idea Title: A concise, descriptive title.\n",
    "Core Concept: A clear explanation of the idea (1-2 paragraphs).\n",
    "Exploitation of Breakthrough: Specifically detail how this idea leverages one or more core principles of LMCompress (e.g., semantic understanding, domain-specific LLMs, exceeding Shannon limits, the Kolmogorov paradigm).\n",
    "Potential Impact & Importance: Describe the potential benefits, why this idea is important, and what significant problems it could solve or new opportunities it could create.\n",
    "Key Challenges/Research Questions: Briefly outline potential hurdles, open research questions, or necessary next steps for realizing this idea.\n",
    "Focus on ideas that are:\n",
    "Truly Novel: Go beyond obvious next steps. Think about cross-disciplinary applications or entirely new use-cases.\n",
    "High-Impact: Could lead to significant advancements, new products/services, or solve major existing problems.\n",
    "Actionable (Conceptually): While potentially ambitious, the core concept should be clear enough that one could envision a research or development path.\n",
    "Consider the implications of \"understanding as compression\" in diverse fields. Think about how LLM-driven semantic compression could revolutionize areas beyond simple data storage and transmission.\n",
    "\n",
    "For example, one idea could be to create \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P = \"\"\"You are a highly esteemed expert researcher, your task is to find ideas \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = \"\"\"You are a highly esteemed expert researcher, your task is to find research or scientific ideas that could be unlocked by the breakthrough described in the paper \"Understanding is Compression.\"\n",
    "\n",
    "Here is a summary of the paper:\n",
    "\n",
    "Breakthrough Summary (Paper: \"Understanding is Compression\"):\n",
    "The paper presents LMCompress, a novel lossless compression framework. Unlike traditional methods, LMCompress uses large language models (LLMs) as data understanding engines rather than direct encoders. Inspired by Solomonoff induction (a theoretically optimal but uncomputable prediction method based on understanding), the core argument is that superior understanding leads to superior compression.\n",
    "LMCompress Mechanism:\n",
    "Tokenization: Input data (text, image, video, audio) is tokenized.\n",
    "LLM Probability Estimation: A generative LLM estimates the probability distribution for each token.\n",
    "Arithmetic Coding: Probabilities from the LLM are used for arithmetic coding.\n",
    "Domain Specialization: Models fine-tuned on specific domains (e.g., iGPT for images, LLaMA for text/audio) enhance \"understanding\" and compression for that data type.\n",
    "Key Breakthrough Aspects:\n",
    "Exceeding Shannon Limits: LMCompress surpasses traditional Shannon-based compression limits by leveraging LLMs to approximate uncomputable data patterns.\n",
    "Superior Performance: It consistently outperforms state-of-the-art methods (doubling/quadrupling compression ratios for images vs. JPEG-XL, audio vs. FLAC, video vs. H.264, text vs. bz2).\n",
    "Kolmogorov Paradigm Shift: This marks a shift from frequency-based, computable compression to model-based, semantic compression. The essence is understanding and modeling the source, not just statistical redundancy removal.\n",
    "Implications: Significant potential for future communication systems (e.g., 6G) by mitigating bandwidth limits through deeper data understanding at both ends.\n",
    "\n",
    "\n",
    "Now, please generate 3-5 queries to a search engine to pull research or scientific ideas that could be unlocked by the breakthrough described in the paper \"Understanding is Compression.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
