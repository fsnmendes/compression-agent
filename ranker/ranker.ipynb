{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKER_PROMPT = \"\"\"You are a highly critical and discerning research program evaluator with deep expertise in compression and the sciences.\n",
    "\n",
    "There has been a recent BREAKTHROUGH in the field of compression. Your task is to explore novel and impactful discoveries that this BREAKTHROUGH will enable.\n",
    "\n",
    "You will be given two inputs:\n",
    "1) a summary of the research breakthrough.\n",
    "2) another research paper that explores a specific problem.\n",
    "\n",
    "Your task is to EVALUATE whether this breakthrough could be leveraged to improve the given research paper significantly.\n",
    "\n",
    "Think step by step, and plan a very specific strategy you could work on to improve this research paper in different ways:\n",
    "- Change the methodology.\n",
    "- Optimize the results.\n",
    "- Change the results.\n",
    "- Change the conclusion.\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "\n",
    "1.  **NOVELTY (Score 1-5, 5 being highly novel):**\n",
    "    *   How original is this idea?\n",
    "    *   Does it propose a genuinely new application, methodology, or theoretical extension of the LMCompress breakthrough?\n",
    "    *   Or, is it an obvious next step, a minor tweak to existing work, or a rehash of old concepts merely relabeled with \"LLM compression\"?\n",
    "    *   **Justification required.**\n",
    "\n",
    "2.  **IMPORTANCE (Score 1-5, 5 being critically important):**\n",
    "    *   What is the potential impact of this idea if successfully realized?\n",
    "    *   Would it solve a significant problem, open up major new capabilities, or substantially advance our understanding or application of semantic compression?\n",
    "    *   Consider its practical, theoretical, or societal relevance.\n",
    "    *   **Justification required.**\n",
    "\n",
    "3.  **FEASIBILITY & VERIFIABILITY (Combined Score 1-5, 5 being highly feasible & verifiable):**\n",
    "    *   **Feasibility:** Can a proof-of-concept (PoC) or a minimal viable demonstration for this idea be reasonably implemented with current or near-future resources and technology (assuming the LMCompress capability exists)?\n",
    "    *   **Verifiability:** Can the success or failure of this idea be clearly demonstrated and validated *quickly*?\n",
    "        *   This means an output that can be checked by code (e.g., a compression ratio achieved, a task completed, a specific measurable outcome).\n",
    "        *   OR, by a human inspecting a tangible result (e.g., a decoded image's quality, a summarized text's accuracy, a correctly derived mathematical step, a functioning compressed communication channel) within minutes.\n",
    "        *   Is the proposed verification method robust and unambiguous?\n",
    "    *   **Justification required.**\n",
    "\n",
    "4.  **RELIANCE ON SEMANTIC COMPRESSION (Score 1-5, 5 meaning critically reliant):**\n",
    "    *   Is the *efficient compression of semantic data* (as enabled by LMCompress) the *core* enabler or the *primary bottleneck* that this idea addresses?\n",
    "    *   In other words, was this idea largely impractical, significantly less effective, or even impossible *before* a breakthrough like LMCompress, specifically due to the inability to efficiently compress or leverage the semantic understanding of data like text, images, audio, or complex symbolic structures?\n",
    "    *   If the idea could have been pursued almost as effectively with older compression techniques or non-compression-focused AI, it scores lower here.\n",
    "    *   **Justification required.**\n",
    "\n",
    "**Overall Assessment:**\n",
    "Sum up all the scores yielded by the individual metrics to get a final score.\n",
    "\n",
    "**Input Format:**\n",
    "Here is the breaktrhough paper: [breakthrough_paper]\n",
    "Here is the research paper: [research_paper]\n",
    "\n",
    "**Output Format:**\n",
    "-   Research paper summary: [...]\n",
    "-   Strategy to improve the research paper: [...]\n",
    "-   Novelty: [Score]/5. Justification: [...]\n",
    "-   Importance: [Score]/5. Justification: [...]\n",
    "-   Feasibility & Verifiability: [Score]/5. Justification: [...]\n",
    "-   Reliance on Semantic Compression: [Score]/5. Justification: [...]\n",
    "-   Overall Assessment: [Score]/20\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "\n",
    "def rank(breakthrough_paper, research_paper):\n",
    "    breakthrough_paper = extract_text_from_pdf(breakthrough_paper)\n",
    "    research_paper = extract_text_from_pdf(research_paper)\n",
    "\n",
    "    res = client.messages.create(\n",
    "        model=\"claude-opus-4-0\",\n",
    "        system=RANKER_PROMPT,\n",
    "        max_tokens=2000,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the breakthrough paper: {breakthrough_paper}\\n\\nHere is the research paper: {research_paper}\"}\n",
    "        ]\n",
    "    )\n",
    "    with open(f'ranker_outputs/{breakthrough_paper}-{research_paper}.txt', 'wb') as f:\n",
    "        f.write(res.content[0].text.encode('utf-8'))\n",
    "    return res.content[0].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submitted ranking of https:arxiv.org:pdf:2407.07723.pdf-https:arxiv.org:pdf:2407.07723.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished ranking of ## Research Paper Summary\n",
      "\n",
      "The research paper \"Understanding is Compression\" by Li et al. presents LMCompress, a revolutionary data compression method that leverages large language models (LLMs) to achieve unprecedented compression ratios. The core insight is that better understanding of data leads to better compression. LMCompress uses different models for different data types (iGPT for images/videos, fine-tuned LLaMA for audio/text) combined with arithmetic coding. The method achieves 2x better compression than JPEG-XL for images, FLAC for audio, H.264 for video, and 4x better than bz2 for text.\n",
      "\n",
      "## Strategy to Improve the Research Paper\n",
      "\n",
      "### 1. Change the Methodology\n",
      "- **Multi-modal unified compression**: Instead of using separate models for different data types, develop a unified multi-modal LLM that can understand and compress all data types through a single architecture\n",
      "- **Hierarchical compression**: Implement a two-stage approach where the first stage uses LMCompress for semantic compression, and the second stage applies traditional entropy coding on the residuals\n",
      "- **Context-aware adaptive compression**: Develop dynamic context windows that adjust based on data complexity rather than fixed 1024/2048 byte chunks\n",
      "\n",
      "### 2. Optimize the Results\n",
      "- **Model distillation**: Create smaller, specialized models distilled from the large models that maintain compression performance while reducing computational overhead\n",
      "- **Parallel processing**: Implement GPU-accelerated parallel compression for independent data chunks to dramatically reduce compression time\n",
      "- **Progressive compression**: Develop a layered compression scheme where basic compression can be decoded quickly, with additional layers providing better quality\n",
      "\n",
      "### 3. Change the Results\n",
      "- **Real-time compression**: Focus on achieving real-time compression speeds for streaming applications by trading off some compression ratio for speed\n",
      "- **Error-resilient compression**: Add redundancy mechanisms that allow partial recovery from corrupted compressed data\n",
      "- **Compression with privacy**: Implement differential privacy mechanisms during the compression process to protect sensitive information\n",
      "\n",
      "### 4. Change the Conclusion\n",
      "- **Theoretical limits**: Provide mathematical proof of the theoretical compression limits achievable through semantic understanding\n",
      "- **Energy efficiency analysis**: Include comprehensive analysis of energy consumption vs compression ratio trade-offs\n",
      "- **Standardization pathway**: Propose a concrete roadmap for standardizing LMCompress for industry adoption\n",
      "\n",
      "## Evaluation\n",
      "\n",
      "**Novelty: 5/5**\n",
      "Justification: This is genuinely groundbreaking work that fundamentally shifts the compression paradigm from Shannon's information theory to Kolmogorov complexity via Solomonoff induction. Using LLMs for compression by leveraging their semantic understanding is a paradigm shift, not an incremental improvement. The approach of using different specialized models for different data types while maintaining a unified compression framework is highly original.\n",
      "\n",
      "**Importance: 5/5**\n",
      "Justification: The impact is transformative - achieving 2-4x improvements over state-of-the-art compression methods addresses critical challenges in 6G communications, data storage, and transmission. This breakthrough could revolutionize how we store and transmit data globally, with massive implications for bandwidth-limited applications like satellite communications, archival storage, and edge computing. The potential economic and environmental benefits from reduced storage and transmission requirements are enormous.\n",
      "\n",
      "**Feasibility & Verifiability: 4/5**\n",
      "Justification: The paper provides concrete implementation details and reproducible results across multiple data types. The compression ratios are easily verifiable through standard benchmarks. The main feasibility challenge is the computational cost of running large models for compression/decompression, though this is offset by the dramatic compression gains. The code availability promise and detailed experimental setup make verification straightforward. Deducting one point for the high computational requirements that may limit immediate practical deployment.\n",
      "\n",
      "**Reliance on Semantic Compression: 5/5**\n",
      "Justification: This work is fundamentally enabled by semantic compression - the entire approach depends on LLMs' ability to understand data semantically rather than just statistically. Traditional compression methods hit their limits because they lack semantic understanding. The breakthrough explicitly requires the semantic modeling capabilities of LLMs to predict next tokens accurately based on deep contextual understanding. Without LLMs' semantic compression abilities, achieving these compression ratios would be impossible.\n",
      "\n",
      "**Overall Assessment: 19/20**\n",
      "\n",
      "This represents a genuine paradigm shift in data compression with transformative potential across multiple industries and applications. The work successfully demonstrates that semantic understanding through LLMs can shatter traditional compression limits, opening entirely new possibilities for data storage and transmission in the era of 6G and beyond.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tqdm\n",
    "\n",
    "BREAKTHROUGH_DIR = '../optimizations/understanding_is_compression'\n",
    "IDEA_DIR = '../optimizations/understanding_is_compression'\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    breakthroughs = os.listdir(BREAKTHROUGH_DIR)\n",
    "    ideas = os.listdir(IDEA_DIR)\n",
    "    \n",
    "    for breakthrough in breakthroughs:\n",
    "        for idea in ideas:\n",
    "            futures.append(executor.submit(rank, f'{BREAKTHROUGH_DIR}/{breakthrough}', f'{IDEA_DIR}/{idea}'))\n",
    "            print(f'submitted ranking of {breakthrough}-{idea}')\n",
    "    \n",
    "    for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "        print(f\"finished a ranking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
