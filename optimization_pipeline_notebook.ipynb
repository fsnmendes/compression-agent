{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fsnmendes/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Faster $(Δ+ 1)$-Edge Coloring: Breaking the $m \\sqrt{n}$ Time Barrier\n",
      "URL: https://arxiv.org/abs/2405.15449\n",
      "ID: https://arxiv.org/abs/2405.15449\n",
      "Score: 0.4087863266468048\n",
      "Published Date: 2024-05-24T00:00:00.000Z\n",
      "Author: Bhattacharya; Sayan; Carmon; Din; Costa; Martín; Solomon; Shay; Zhang; Tianyi\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "Vizing's theorem states that any $n$-vertex $m$-edge graph of maximum degree $\\Delta$ can be {\\em edge colored} using at most $\\Delta + 1$ different colors [Diskret.~Analiz, '64]. Vizing's original proof is algorithmic and shows that such an edge coloring can be found in $\\tilde{O}(mn)$ time. This was subsequently improved to $\\tilde O(m\\sqrt{n})$, independently by Arjomandi [1982] and by Gabow et al.~[1985].\n",
      " In this paper we present an algorithm that computes such an edge coloring in $\\tilde O(mn^{1/3})$ time, giving the first polynomial improvement for this fundamental problem in over 40 years.\n",
      " \n",
      " \n",
      " Submission history From: Martin Costa [ view email] [v1] \n",
      " Fri, 24 May 2024 11:25:45 UTC (858 KB) \n",
      " ||||I|||| Skip to main content\n",
      " We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n",
      " > cs > arXiv:2405.15449\n",
      "\n",
      " Help | Advanced Search\n",
      "\n",
      " All fields Title Author Abstract Comments Journal reference\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: TSP Escapes the $O(2^n n^2)$ Curse\n",
      "URL: https://arxiv.org/abs/2405.03018\n",
      "ID: https://arxiv.org/abs/2405.03018\n",
      "Score: 0.40259602665901184\n",
      "Published Date: 2024-05-05T00:00:00.000Z\n",
      "Author: Stoian; Mihail\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " Computer Science &gt; Data Structures and Algorithms \n",
      " \n",
      " arXiv:2405.03018 (cs)\n",
      " \n",
      " \n",
      " \n",
      " [Submitted on 5 May 2024] \n",
      " View PDF \n",
      "The dynamic programming solution to the traveling salesman problem due to Bellman, and independently Held and Karp, runs in time $O(2^n n^2)$, with no improvement in the last sixty years. We break this barrier for the first time by designing an algorithm that runs in deterministic time $2^n n^2 / 2^{\\Omega(\\sqrt{\\log n})}$. We achieve this by strategically remodeling the dynamic programming recursion as a min-plus matrix product, for which faster-than-naïve algorithms exist.\n",
      " \n",
      " \n",
      " \n",
      " Submission history From: Mihail Stoian [ view email] [v1] \n",
      " Sun, 5 May 2024 18:00:42 UTC (169 KB) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Current browse context: cs.DS \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Bookmark \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Bibliographic and Citation Tools \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Code, Data and Media Associated with this Article \n",
      " \n",
      " \n",
      " \n",
      " Demos \n",
      " \n",
      " \n",
      " \n",
      " Recommenders and Search Tools \n",
      " \n",
      " \n",
      " \n",
      " arXivLabs: experimental projects with community colla\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: How Fast Do Algorithms Improve?\n",
      "URL: https://ide.mit.edu/wp-content/uploads/2021/09/How_Fast_Do_Algorithms_Improve.pdf\n",
      "ID: https://ide.mit.edu/wp-content/uploads/2021/09/How_Fast_Do_Algorithms_Improve.pdf\n",
      "Score: None\n",
      "Published Date: 2021-09-21T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n",
      "How Fast Do Algorithms\n",
      "Improve?\n",
      "By YASH SHERRY\n",
      "MIT Computer Science & Artificial Intelligence Laboratory, Cambridge, MA 02139 USA\n",
      "NEIL C. THOMPSON\n",
      "MIT Computer Science & Artificial Intelligence Laboratory, Cambridge, MA 02139 USA\n",
      "MIT Initiative on the Digital Economy, Cambridge, MA 02142 USA\n",
      "Algorithms determine which calculations computers use to solve\n",
      "problems and are one of the central pillars of computer science.\n",
      "As algorithms improve, they enable scientists to tackle larger\n",
      "problems and explore new domains and new scientific tech\u0002niques [1], [2]. Bold claims have been made about the pace of algorithmic\n",
      "progress. For example, the President’s Council of Advisors on Science and\n",
      "Technology (PCAST), a body of senior scientists that advise the U.S. President,\n",
      "wrote in 2010 that “in many areas, performance gains due to improvements in\n",
      "algorithms\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization\n",
      "URL: https://arxiv.org/pdf/2205.09647v1.pdf\n",
      "ID: https://arxiv.org/pdf/2205.09647v1.pdf\n",
      "Score: 0.3993629515171051\n",
      "Published Date: 2022-05-19T00:00:00.000Z\n",
      "Author: D. Yu. Kovalev,Alexander Gasnikov\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Dmitry Kovalev dakovalev1@gmail.com\n",
      "IITP RAS †\n",
      "Kaust\n",
      "IITP RAS †\n",
      "Alexander Gasnikov gasnikov@yandex.ru\n",
      "IITP RAS †\n",
      "THE FIRST OPTIMAL ACCELERATION OF HIGH-ORDER METHODS IN SMOOTH CONVEX OPTIMIZATION\n",
      "May 20, 2022\n",
      "In this paper, we study the fundamental open question of finding the optimal high-order algorithm for solving smooth convex minimization problems.Arjevani et al. (2019)established the lower bound Ω −2/(3p+1) on the number of the p-th order oracle calls required by an algorithm to find an -accurate solution to the problem, where the p-th order oracle stands for the computation of the objective function value and the derivatives up to the order p. However, the existing state-of-the-art high-order methods of Gasnikov et al. (2019b); Bubeck et al. (2019); Jiang et al. (2019) achieve the oracle complexity O −2/(3p+1) log(1/ ), which does not match the lower bound. The reason for this is that these algorithms require performing a complex binary search procedure, which makes them neither\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: A faster algorithm for Vertex Cover parameterized by solution size\n",
      "URL: https://export.arxiv.org/pdf/2205.08022v5.pdf\n",
      "ID: https://export.arxiv.org/pdf/2205.08022v5.pdf\n",
      "Score: 0.3991052210330963\n",
      "Published Date: 2022-05-16T00:00:00.000Z\n",
      "Author: David G. Harris,N. S. Narayanaswamy\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: David G Harris davidgharris29@gmail.com\n",
      "Dept. of Computer Science\n",
      "University of Maryland\n",
      "College Park\n",
      "N S Narayanaswamy\n",
      "Department of Computer Science and Engineering\n",
      "Indian Institute of Technology Madras\n",
      "India\n",
      "A faster algorithm for Vertex Cover parameterized by solution size\n",
      "We describe a new algorithm for vertex cover with runtime O * (1.25284 k ), where k is the size of the desired solution and O * hides polynomial factors in the input size. This improves over the previous runtime of O * (1.2738 k ) due to Chen, Kanj, & Xia (2010) standing for more than a decade. The key to our algorithm is to use a measure which simultaneously tracks k as well as the optimal value λ of the vertex cover LP relaxation. This allows us to make use of prior algorithms for Maximum Independent Set in bounded-degree graphs and Above-Guarantee Vertex Cover.The main step in the algorithm is to branch on high-degree vertices, while ensuring that both k and µ = k − λ are decreased at each step. There can be l\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Faster 0-1-Knapsack via Near-Convex Min-Plus-Convolution\n",
      "URL: https://arxiv.org/pdf/2305.01593.pdf\n",
      "ID: https://arxiv.org/pdf/2305.01593.pdf\n",
      "Score: 0.39748603105545044\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: K. Bringmann,Alejandro Cassis\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Faster 0-1-Knapsack via Near-Convex Min-Plus-Convolution\n",
      "May 3, 2023 2 May 2023\n",
      "\n",
      "Karl Bringmann bringmann@cs.uni-saarland.de. \n",
      "Max Planck Institute for Informatics\n",
      "Saarland Informatics Campus\n",
      "Saarland University\n",
      "\n",
      "\n",
      "Alejandro Cassis acassis@cs.uni-saarland.de. \n",
      "Saarland University and Max Planck Institute for Informatics, Saarland Informatics Campus\n",
      "\n",
      "\n",
      "Faster 0-1-Knapsack via Near-Convex Min-Plus-Convolution\n",
      "May 3, 2023 2 May 2023\n",
      "We revisit the classic 0-1-Knapsack problem, in which we are given n items with their weights and profits as well as a weight budget W , and the goal is to find a subset of items of total weight at most W that maximizes the total profit. We study pseudopolynomial-time algorithms parameterized by the largest profit of any item p max , and the largest weight of any item w max . Our main result are algorithms for 0-1-Knapsack running in time O(n w max p 2/3 max ) and O(n p max w 2/3 max ), improving upon an algorithm in time O(n p max w max ) by Pisinger [J. Algori\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Faster Knapsack Algorithms via Bounded Monotone Min-Plus-Convolution\n",
      "URL: https://arxiv.org/pdf/2205.08493v1.pdf\n",
      "ID: https://arxiv.org/pdf/2205.08493v1.pdf\n",
      "Score: 0.39723947644233704\n",
      "Published Date: 2022-05-17T00:00:00.000Z\n",
      "Author: Karl Bringmann,Alejandro Cassis\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Karl Bringmann \n",
      "Max Planck Institute for Informatics\n",
      "Saarland Informatics Campus\n",
      "Saarland University\n",
      "SaarbrückenGermany\n",
      "\n",
      "Alejandro Cassis \n",
      "Max Planck Institute for Informatics\n",
      "Saarland Informatics Campus\n",
      "Saarland University\n",
      "SaarbrückenGermany\n",
      "\n",
      "Faster Knapsack Algorithms via Bounded Monotone Min-Plus-Convolution *\n",
      "\n",
      "We present new exact and approximation algorithms for 0-1-Knapsack and Unbounded Knapsack:Exact Algorithm for 0-1-Knapsack: 0-1-Knapsack has known algorithms running in time O(n + min{n · OPT, n · W, OPT 2 , W 2 }) [Bellman '57], where n is the number of items, W is the weight budget, and OPT is the optimal profit. We present an algorithm running in time O(n + (W + OPT) 1.5 ). This improves the running time in case n, W, OPT are roughly equal. Tzamos '19, Jansen, Rohwedder '19, Chan, He '22], where n is the number of items, w max is the largest weight of any item, and p max is the largest profit of any item. We present an algorithm running in time O(n + (p max + w max ) 1.5 )\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Negative-Weight Single-Source Shortest Paths in Near-Linear Time: Now Faster!\n",
      "URL: https://export.arxiv.org/pdf/2304.05279v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.05279v1.pdf\n",
      "Score: 0.3956212103366852\n",
      "Published Date: 2023-04-11T00:00:00.000Z\n",
      "Author: Karl Bringmann,Alejandro Cassis,Nick Fischer\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Karl Bringmann \n",
      "Max Planck Institute for Informatics\n",
      "Saarland Informatics Campus\n",
      "Saarland University\n",
      "SaarbrückenGermany\n",
      "\n",
      "Alejandro Cassis \n",
      "Max Planck Institute for Informatics\n",
      "Saarland University\n",
      "Saarland Informatics CampusSaarbrückenGermany\n",
      "\n",
      "Nick Fischer \n",
      "Weizmann Institute of Science\n",
      "RehovotIsrael\n",
      "\n",
      "Negative-Weight Single-Source Shortest Paths in Near-Linear Time: Now Faster! Acknowledgements We thank Danupon Nanongkai for many helpful discussions on the topic\n",
      "and phrases Shortest pathsLow Diameter DecompositionDrift analysis\n",
      "In this work we revisit the fundamental Single-Source Shortest Paths (SSSP) problem with possibly negative edge weights. A recent breakthrough result by Bernstein, Nanongkai and Wulff-Nilsen established a near-linear O(m log 8 (n) log(W ))-time algorithm for negative-weight SSSP, where W is an upper bound on the magnitude of the smallest negative-weight edge. In this work we improve the running time to O(m log 2 (n) log(nW ) log log n), which is an improvement by\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Negative-Weight Single-Source Shortest Paths in Near-linear Time\n",
      "URL: https://export.arxiv.org/pdf/2203.03456v4.pdf\n",
      "ID: https://export.arxiv.org/pdf/2203.03456v4.pdf\n",
      "Score: 0.38896840810775757\n",
      "Published Date: 2022-03-07T00:00:00.000Z\n",
      "Author: Aaron Bernstein,Danupon Nanongkai,Christian Wulff-Nilsen\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Aaron Bernstein \n",
      "Rutgers University † University of Copenhagen\n",
      "KTH Stockholm. ‡ BARC\n",
      "University of Copenhagen\n",
      "\n",
      "\n",
      "Danupon Nanongkai \n",
      "Rutgers University † University of Copenhagen\n",
      "KTH Stockholm. ‡ BARC\n",
      "University of Copenhagen\n",
      "\n",
      "\n",
      "Christian Wulff-Nilsen \n",
      "Rutgers University † University of Copenhagen\n",
      "KTH Stockholm. ‡ BARC\n",
      "University of Copenhagen\n",
      "\n",
      "\n",
      "Negative-Weight Single-Source Shortest Paths in Near-linear Time\n",
      "30 Oct 2022\n",
      "We present a randomized algorithm that computes single-source shortest paths (SSSP) in O(m log 8 (n) log W ) time when edge weights are integral and can be negative. 1 This essentially resolves the classic negative-weight SSSP problem. The previous bounds areÕ((m+n 1.5 ) log W ) [BLNPSSSW FOCS'20] and m 4/3+o(1) log W [AMV FOCS'20]. Near-linear time algorithms were known previously only for the special case of planar directed graphs [Fakcharoenphol and Rao FOCS'01].In contrast to all recent developments that rely on sophisticated continuous optimization methods and dynami\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Faster sorting algorithms discovered using deep reinforcement learning\n",
      "URL: https://www.nature.com/articles/s41586-023-06004-9.pdf?error=cookies_not_supported&code=746c6d4b-bc15-478a-8620-f5e7db377aa3\n",
      "ID: https://www.nature.com/articles/s41586-023-06004-9.pdf?error=cookies_not_supported&code=746c6d4b-bc15-478a-8620-f5e7db377aa3\n",
      "Score: 0.38783302903175354\n",
      "Published Date: 2023-05-29T00:00:00.000Z\n",
      "Author: Daniel J. Mankowitz\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Nature | Vol 618 | 8 June 2023 | 257\n",
      "Article\n",
      "Faster sorting algorithms discovered using \n",
      "deep reinforcement learning\n",
      "Daniel J. Mankowitz1,3 ✉, Andrea Michi1,3, Anton Zhernov1,3, Marco Gelmi1,3, Marco Selvi1,3, \n",
      "Cosmin Paduraru1,3, Edouard Leurent1,3, Shariq Iqbal1\n",
      ", Jean-Baptiste Lespiau1, Alex Ahern1, \n",
      "Thomas Köppe1\n",
      ", Kevin Millikin1, Stephen Gaffney1, Sophie Elster1, Jackson Broshear1, \n",
      "Chris Gamble1\n",
      ", Kieran Milan1, Robert Tung1, Minjae Hwang2, Taylan Cemgil1, \n",
      "Mohammadamin Barekatain1\n",
      ", Yujia Li1, Amol Mandhane1, Thomas Hubert1, \n",
      "Julian Schrittwieser1\n",
      ", Demis Hassabis1, Pushmeet Kohli1, Martin Riedmiller1, Oriol Vinyals1 & \n",
      "David Silver1\n",
      "Fundamental algorithms such as sorting or hashing are used trillions of times on any \n",
      "given day1\n",
      ". As demand for computation grows, it has become critical for these \n",
      "algorithms to be as performant as possible. Whereas remarkable progress has been \n",
      "achieved in the past2\n",
      ", making further improvements on the efciency of these \n",
      "routines has proved chal\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: Find me pdfs of recent major algorithmic breakthroughs that optimize an existing algorithm\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from exa_py import Exa\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"EXA_API_KEY\"] = os.getenv('EXA_API_KEY')\n",
    "exa = Exa(api_key = os.getenv('EXA_API_KEY'))\n",
    "\n",
    "result = exa.search_and_contents(\n",
    "    \"Find me pdfs of recent major algorithmic breakthroughs that optimize an existing algorithm\",\n",
    "    text = { \"max_characters\": 1000 },\n",
    "    category = \"research paper\",\n",
    "    num_results = 10,\n",
    "    start_published_date = \"2015-01-01T08:00:00.000Z\",\n",
    "    end_published_date = \"2025-05-25T06:59:59.999Z\"\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Result(url='https://arxiv.org/pdf/1911.03572v1.pdf', id='https://arxiv.org/pdf/1911.03572v1.pdf', title='DZip: improved general-purpose loss less compression based on novel neural network modeling', score=0.3941058814525604, published_date='2020-09-18T17:58:10.000Z', author='Mohit Goyal,Kedar Tatwawadi,Shubham Chandak,Idoia Ochoa', image=None, favicon=None, subpages=None, extras=None, text='Mohit Goyal \\nElectrical and Computer Engineering\\nUniversity of Illinois\\nUrbanaILUSA\\n\\nKedar Tatwawadi kedart@stanford.edu \\nDepartment of Electrical Engineering\\nStanford University\\nCAUSA\\n\\nShubham Chandak \\nDepartment of Electrical Engineering\\nStanford University\\nCAUSA\\n\\nIdoia Ochoa \\nElectrical and Computer Engineering\\nUniversity of Illinois\\nUrbanaILUSA\\n\\nDZip: improved general-purpose lossless compression based on novel neural network modeling\\n\\nWe consider lossless compression based on statistical data modeling followed by predictionbased encoding, where an accurate statistical model for the input data leads to substantial improvements in compression. We propose DZip, a general-purpose compressor for sequential data that exploits the well-known modeling capabilities of neural networks (NNs) for prediction, followed by arithmetic coding. Dzip uses a novel hybrid architecture based on adaptive and semi-adaptive training. Unlike most NN based compressors, DZip does not require additional training data and is not restricted to specific data types, only needing the alphabet size of the input data. The proposed compressor outperforms general-purpose compressors such as Gzip (on average 26% reduction) on a variety of real datasets, achieves near-optimal compression on synthetic datasets, and performs close to specialized compressors for large sequence lengths, without any human input. The main limitation of DZip in its current implementation is the encoding/decoding time, which limits its practicality. Nevertheless, the results showcase the potential of developing improved general-purpose compressors based on neural networks and hybrid modeling.\\n\\nIntroduction\\n\\nThere has been a tremendous surge in the amount of data generated in the past years. Along with image and textual data, new types of data such as genomic, 3D VR, and point cloud data are being generated at a rapid pace [1], [2]. Thus, data compression is critical for reducing the storage and transmission costs associated with these data, and has been studied extensively from both theoretical and practical standpoints. In particular, a wide class of (lossless) compressors utilize the \"prediction + entropy coding\" approach, wherein a statistical model generates predictions for the upcoming symbols given the past and an entropy coder (e.g., arithmetic coder [3]) uses the predicted probabilities to perform compression. In this general framework, a better prediction model directly induces a better compressor.\\n\\nGiven the close link between prediction and compression, there has been interest in using neural networks for compression due to their exceptional performance on several modeling and prediction tasks (e.g., language modeling [4,5], generative modeling [6]). Neural network based models can typically learn complex patterns in the data better than traditional finite context and Markov models, leading to significantly lower prediction error (measured as log-loss or perplexity [4]). This has led to the development of several compressors using neural networks as predictors, including the recently proposed LSTM-Compress [7] and DecMac [8]. Most of the previous works, however, have been tailored for compression of certain data types (e.g., text), where the prediction model is trained on separate training data or the model architecture is tuned for the specific data type. This approach is applicable only in presence of existing training data and requires significant domain knowledge, and thus cannot be used for general-purpose compression.\\n\\n\\nOur Contributions\\n\\nIn this work, we propose a general-purpose lossless compressor for sequential data, DZip, that relies on neural network based modeling. DZip is a standalone compressor (does not require any additional training datasets) and is applicable to a wide variety of sequential datasets, needing only the alphabet size of the input sequence. We use a novel hybrid training approach which is ideally suited for such a setting and combines elements of adaptive and semi-adaptive modeling. We evaluate DZip on datasets from several domains including text and genomics, and show that it achieves on average 26% improvement over GZip, reducing the gap between general-purpose and specialized compressors. DZip also outperforms the state-of-the-art lossless compressors BSC [9] on most datasets, showing the advantages of improved modeling. In addition, we perform evaluations on certain synthetic datasets of known entropy that highlight the ability of DZip to learn long-term patterns better than the other general-purpose compressors. DZip serves as an example to showcase the potential of neural networks to boost compression performance. Its current implementation suffers however from large encoding/decoding times partly due to the underlying NN platform, limiting its practicality. DZip is available as an open source tool at https://github.com/mohit1997/DZip, providing a framework to experiment with several neural network models and training methodologies.\\n\\n\\nRelated Works\\n\\nEver since Shannon introduced information theory [10] and showed that the entropy rate is the fundamental limit on the compression rate for any stationary process, several attempts have been made to achieve this optimum. Several classes of generalpurpose lossless compressors have been developed since then, including dictionarybased compressors (e.g., Gzip, 7-zip/LZMA) and sorting transform based compressors (e.g., BZip2, BSC). In addition, several specialized compressors have been developed, often using a statistical approach combining prediction models with arithmetic coding.\\n\\nInspired by the performance of neural networks (NNs) in modeling and prediction tasks, several lossless compressors based on NNs have been proposed. The work in [11] proposed the application of a character-level recurrent neural network (RNN) model and showed competitive compression performance as compared to the existing compressors on text data. However, as vanilla RNNs were used, the performance was relatively subpar for complex sources with longer memory. More recently, LSTM-Compress [7] was proposed, which uses an LSTM model to adaptively learn the source distribution while encoding with an arithmetic coder. There has also been work on designing specialized text compressors that exploit the generalization ability of NNs, using similar datasets for training the model to be used for compression (e.g., DecMac [8]). Most of these compressors are heavily specialized for a specific data type or require a model pretrained on similar data, thus limiting their applicability as general-purpose compression tools.\\n\\nIn parallel to the work on compression, there has been significant progress in language modeling (e.g., BERT [4], GPT-2 [5]) and generative prediction models for images (e.g., PixelCNN++ [6]). In principle, these can be used for compression leading to significant improvements over the state-of-the-art, e.g., bringing the text compression rate below 1 bit per character. However, in practice, the model itself is typically quite large and needs vast amounts of data for training, which limits their direct applicability to general-purpose compression.\\n\\n\\nBackground\\n\\nConsider a data stream S N = {S 1 , S 2 , . . . , S N } over an alphabet S which we want to compress losslessly. We consider the statistical coding approach consisting of a prediction model followed by an arithmetic coder. For a sequence S N , the aim of the model is to estimate the conditional probability distribution of the r th symbol S r based on the previously observed K symbols, denoted as P (S r |S r−1 , . . . , S r−K ), where K is a hyperparameter. An estimate of this probability and S r are then fed into the arithmetic encoding block which recursively updates its state. This state serves for the compressed representation at the end of this process. The compressed size using this approach is equivalent to the cross entropy loss shown in Eq. 1, where |S| is the alphabet size, y r ,ŷ r (vectors of size |S|) are the one-hot encoded ground truth and the predicted probabilities, respectively, and N is the sequence length.\\nL = N r=1 |S| k=1 y rk log 2 1 y rk .(1)\\nThe model that estimates the probability P (S r |S r−1 , . . . , S r−K ) should be trained so as to minimize the cross entropy loss on the data to be compressed. This training can be performed in several ways [12] as discussed below:\\n\\nStatic: Here the model is first trained on some external training data and it is made available to both the compressor and the decompressor. The performance in this case is highly dependent on the generalization abilities of the model. This approach is restricted to cases where similar training data is available and is not directly applicable to general-purpose compression tasks.\\n\\nAdaptive: Here both the compressor and the decompressor are initialized with the same random model which is updated adaptively based on the sequence seen up to some point. This approach does not require the availability of training data and works quite well for small models. For large models, however, this approach may suffer due to the difficulties in training the model in a single pass and adapting quickly to changing statistics (e.g., for non-stationary data).\\n\\nSemi-adaptive: Here the model is trained based only on the input sequence and the training procedure can involve multiple passes through the input data. The trained model parameters are included as part of the compressed file, along with the arithmetic coding output. This additional cost is expected to be compensated by the fact that the sequence-specific training will lead to a better predictive model and as a result a smaller arithmetic coding output. Note that there is a tradeoff involved between having an accurate model and the bits required to store that model\\'s parameters, as described by the minimum description length (MDL) principle [13]. Essentially, a larger model can lead to better compression, but the', highlights=None, highlight_scores=None, summary=None), Result(url='https://pmc.ncbi.nlm.nih.gov/articles/PMC11799261/', id='https://pmc.ncbi.nlm.nih.gov/articles/PMC11799261/', title='A novel lossless encoding algorithm for data compression–genomics data as an exemplar', score=None, published_date='2025-01-23T00:00:00.000Z', author='', image=None, favicon='https://pmc.ncbi.nlm.nih.gov/static/img/favicons/favicon-32x32.png', subpages=None, extras=None, text='\\n \\n \\n \\n Abstract \\n Data compression is a challenging and increasingly important problem. As the amount of data generated daily continues to increase, efficient transmission and storage have never been more critical. In this study, a novel encoding algorithm is proposed, motivated by the compression of DNA data and associated characteristics. The proposed algorithm follows a divide-and-conquer approach by scanning the whole genome, classifying subsequences based on similarities in their content, and binning similar subsequences together. The data is then compressed into each bin independently. This approach is different than the currently known approaches: entropy, dictionary, predictive, or transform-based methods. Proof-of-concept performance was evaluated using a benchmark dataset with seventeen genomes ranging in size from kilobytes to gigabytes. The results showed a considerable improvement in the compression of each genome, preserving several megabytes compared to state-of-the-art tools. Moreover, the algorithm can be applied to the compression of other data types include mainly text, numbers, images, audio, and video which are being generated daily and unprecedentedly in massive volumes. \\n Keywords: compression, Huffman encoding, LZ, genomics, BWT 1 Introduction \\n The importance of data compression, a fundamental problem in computer science, information theory, and coding theory, continues to increase as global data quantities expand rapidly. The primary goal of compression is to reduce the size of data for subsequent storage or transmission. There are two common types of compression algorithms: lossless and lossy. Lossless algorithms guarantee exact restoration of the original data, whereas lossy algorithms do not. Such losses are caused, for instance, by the exclusion of unnecessary information, such as metadata in video or audio that will not be observed by users. \\n Data exist in different formats including text, numbers, images, audio, and video. Several coding algorithms and the corresponding variants have been developed for textual data, the main focus of this paper. This includes the Huffman Huffman (1952), Shannon Shannon (2001), Shannon-Fano Fano (1949), Shannon-Fano-Elias Cover (1999), Lempel-Ziv (LZ77) Ziv and Lempel (1977), Burrows-Wheeler transform Burrows and Wheeler (1994) and Tunstall (1968) algorithms. The Huffman algorithm includes several variants: minimum-variance Huffman, canonical Huffman, length-limited Huffman, nonbinary Huffman, adaptive Huffman, Faller-Gallager-Knuth (an adaptive Huffman) Knuth (1985), and Vitter (an adaptive Huffman) Vitter (1987). The LZ algorithm also includes several variants, such as LZ78 Ziv and Lempel (1978), Lempel-Ziv-Welch (LZW) Welch (1984), Lempel-Ziv-Stac (LZS) Friend (2004), Lempel-Ziv-Oberhumer (LZO) Oberhumer (2008), Lempel-Ziv-Storer-Szymanski (LZSS) Storer and Szymanski (1982), Lempel–Ziv-Ross-Williams (LZRW) Williams (1991), and the Lempel–Ziv–Markov chain algorithm (LZMA) Ranganathan and Henriques (1993). Additional techniques involve arithmetic encoding Langdon (1984), range encoding Martín (1979), move-to-front encoding (also referred to as symbol ranking encoding) Ryabko (1980); Bentley et al. (1986), run-length encoding Capon (1959), delta encoding, unary encoding, context tree weighting encoding Willems et al. (1995), prediction by partial matching Cleary and Witten (1984), context mixing Mahoney (2005), asymmetric numeral systems (also called asymmetric binary encoding) Duda (2013), length index preserving transform Awan and Mukherjee (2001), and dynamic Markov encoding Cormack and Horspool (1987). \\n Compression algorithms can be classified based on the methodology used in the algorithm, such as entropy, dictionary, predictive, and transform-based methods. These methods have been described extensively in several recent studies Gopinath and Ravisankar (2020); Kavitha (2016); Uthayakumar et al. (2018); Kodituwakku and Amarasinghe (2010), however, a brief description for each method is provided in the Supplementary Material. \\n Genomics (DNA/RNA) data is a type of textual information with several unique characteristics. First, the alphabet consists only of A, C, G, and T characters representing the four nucleotides: adenine, cytosine, guanine, and thymine, respectively. Second, DNA data contain repeat sequences and palindromes. Third, the size of genomics data can be very large, relative to most media files. The human genome, for instance, consists of more than three billion nucleotides (specifically 3,272,116,950 bases ( https://www.ncbi.nlm.nih.gov/grc/human/data?asm=GRCh38.p13) requiring more than 3 gigabytes of storage), and the sequencing is typically conducted with high depth (30–100x) to sequence the same region several times for more accurate reading. As such, sequencing genomic data (especially for humans) is currently being performed for research and diagnostic purposes in daily basis (the number of bases sequenced from December 1982 through August 2024 was 29,643,594,176,326 ( https://www.ncbi.nlm.nih.gov/genbank/statistics/)). Several algorithms have been developed to compress these data, which can be divided into vertical and horizontal techniques Grumbach and Tahi (1994). Vertical mode algorithms utilize a reference genome/source, while horizontal mode algorithms are reference-free. \\n Genomic data are stored in different formats, including FASTA Lipman and Pearson (1985), FASTQ Cock et al. (2010), and SAM Li et al. (2009), with FASTA being the most common and also the primary focus of this paper. Several comparative studies for compressing FASTA files have been published in recent years Kryukov et al. (2020); Hosseini et al. (2016); Mansouri et al. (2020); Bakr and Sharawi, 2013; Jahaan et al. (2017). Genomic sequences typically consist of millions or billions of sequenced reads, with lengths in the hundreds, stored with the quality of each base in a primarily FASTQ format. A common DNA data processing strategy involves aligning the sequenced reads with a reference genome. The output is the reads themselves, with their base qualities and alignment results for each read, stored in a SAM format. Surveys of compression tools for SAM and FASTQ data are available in the literature Bonfield and Mahoney (2013); Hosseini et al. (2016). \\n The small alphabet found in DNA data simplifies the compression process. However, the problem remains challenging due to the discrete, uniform distribution (frequencies) of bases in DNA data. Efficient compression relies mainly on repetitiveness in the data and encoding as few characters/words as possible, since encoding more characters costs more bits-per-character. If the characters are uniformly distributed in the text, their repetitions will also be distributed uniformly and encoding only a fraction of them (to decrease the bits-per-character) will lead to low compression outcomes. The application of Huffman encoding, for instance, produces 2-bit assignments for each base. The algorithm will then produce an inefficient/suboptimal compression result that does not utilize repetitions found in the DNA data. Motivated by this issue, we introduce in this work a lossless and reference-free encoding algorithm. 2 Methods \\n The following observations can be inferred from a careful analysis of DNA. First, many regional (local) sub-sequences (assume a length of 100 bases) contain non-uniform or skewed distributions. Second, similar sub-sequences (which provide better compression results if encoded together) are often distant from each other. This distance is typically longer than the length of sliding windows (usually in kilobases/kilobytes) commonly used in algorithms such as LZ or far apart from previous sequence/symbol used in prediction models such as context weighting tree, predictive partial matching, or dynamic Markov compression. Even if these sequences are located within the same sliding window or previous sequences/symbols, they are often sufficiently distant from each other, which leads to inefficient compression and encoding. These two observations were the motivation behind the design of the following encoding algorithm. \\n 2.1 Compression algorithm \\n Given a string T of length t, containing characters from a fixed alphabet of length \\n, and a window-length \\n, the description of the proposed algorithm is stated in algorithm 1 and is illustrated in Figure 1. \\n FIGURE 1. \\n \\n A flowchart illustrating the compression algorithm \\n Algorithm 1 \\n Compression algorithm. \\n \\n \\n \\n \\n Algorithm 2 \\n Decompression algorithm. \\n \\n \\n 2.2 Decompression algorithm \\n Decompression algorithm is the inverse of compression algorithm and is described in algorithm 2. As the total length of the sequences in all bin is \\n and the total length of \\n is also \\n (as described in algorithm 1), the decompression of all bins will cost no more than \\n. Hence, the time and memory costs of decompression all bins and \\n is linear. \\n This algorithm can be applied not only to DNA or textual data, but to archiving processes and other data types namely numbers, images (binning for instance similar pixels instead of similar subsequences as in text), audio (binning for instance similar subbands/frequency-ranges), and video (binning for instance similar images)). Sub-binning or nested-binning processes can also be applied. \\n This design facilitates organizing and sorting the input data using a divide-and-conquer method by creating bins for similar data and encodes/compresses data in the same bin that are better if compressed together, to achieve better compression results with a minor increase in time costs. In the case of more random/divergent input data, which is the common case, this algorithm avoid relying on a single encoding or compression technique (as in entropy methods), being dependent on the previous sequences and their randomness (as in prediction methods), requiring construction of dictionaries dependent also on the previous sequences and their randomness (as in dictionary methods), or potentially introducing inefficient transformation due to the randomness of the data (as in transform methods). In contrast, the proposed algorithm divides the data into groups of similar segments, regardless of their position in the original sequence, which decreases the randomness in the data and contributes in organizing the input data by binning the similar data together to ultimately handling the compression process more efficiently. \\n Note that the continued application of sub-binning processes will eventually reduce the randomness/divergence of the data and improve the compression results, by obtaining data that are optimal or suboptimal for compression. These processes will require additional time costs, but these costs will still be practical at low sub-binning depth and feasible at high sub-binning depths, especially for small data or the compression of large data for archiving. Therefore, sub-binning will eventually provide more control, organization, and possibly a deterministic solution to encoding and compression problems. Further analysis and investigations are also provided in the Supplementary Material. \\n The encoding algorithm is named after both authors and Pramod K. Srivastava (Professor in the Department of Immunology, University of Connecticut School of Medicine) for honoring him as he was an advisor to the first author. As such, it is named the Okaily-Srivastava-Tbakhi ( OST) algorithm. 2.3 OST-DNA \\n This is the first implementation of the OST algorithm which accepts DNA data as input which can be denoted as OST-DNA. Bin labels are computed using a Huffman tree encoding. The reason for selecting Huffman algorithm since the label of larger bin must be more frequent in \\n, hence encode this label with shorter codes (while the label of smaller bins with longer codes). \\n For example, if the Huffman tree for a subsequence produces the following encoding schema: G:0, A:10, T:110, and C:111, then the label will be GATC_1233 label (1 indicates G is encoded using 1 bit, A using 2 bits, and so on). The number of bits used to encode a character gives a general indication of its frequency compared to the other characters. The number of bins can be reduced by decreasing the label length as follows. To produce a label length of 1, we used the first base of the Huffman tree and its bit length. As such, the above Huffman encoding schema will be represented by G_1. If the bin label length is 2, then the label will be GA_12. This clearly decreases the number of labels, but at the cost of decreasing the similarity among sequences in the same bin therefore their compression. Note that this classification method (bin labeling) is suitable for DNA data as its alphabet is small. For data with larger alphabets, same or other classification methods might be sought such as binning sequences that contain some letter/s most frequently. \\n As the windows do not overlap, each base in T will be read in \\n time. The cost of constructing a Huffman tree for a subsequence is then \\n, requiring the construction of \\n Huffman trees for all non-overlap subsequences in \\n. The total cost hence is \\n. In order to allow for the acquisition of non-uniform distributions for the characters in \\n (the pigeonhole principal), the value of w must be larger than that of \\n, noting that \\n is a constant. As such, the total cost of the compression process of OST-DNA can be \\n. \\n Since the value of w is fixed in this version of OST-DNA, Huffman trees are constructed once for each window sequence. In the case of a variable w where the window will be extended until the sequence label matches a bin label, it is not efficient to calculate Huffman trees for the entire sequence at every extension, hence adaptive Huffman trees can be applied instead. \\n Generally, the compressed bin files and L can be collected into a single file using an archiver which could perform further compression. However, this process was omitted in this study to demonstrate the efficiency of the OST algorithm without any further compression that may be produced by the archiving process. 3 Results \\n We implemented OST-DNA using the python language. We used the same dataset applied to another benchmark Kryukov et al. (2020) in order to test and compare the compression results using OST-DNA with the tools listed in Supplementary Table S1 in Supplementary Material. The dataset consists of seventeen genomes, as shown in Supplementary Tables S2, S3 in Supplemental Methods, ranging in size from 50 KB to 13 GB with a total size of 16,773.88 MB, orgin from different species (virus, bacteria, protist, fungus, algae, animal, plant), and were sequenced using Illumina, 454, SOLID, PacBio, Sanger dideoxy, or mixed technolgies. \\n The following preprocessing steps were applied to each tested genome. All new lines, header lines, lowercase bases, and bases not identified as A, C, G, T, or N, were removed. This produced a one-line sequence for each genome, containing only the four capitalized DNA bases and the letter “N”. Character “N″ represents uncalled/undetermined base during sequencing or assembling process. Assembled genomes may contain also bases with lowercase that represent soft-masked sequences. In compression process, these sequences are converted to capital case with recording their start/end coordinates so that during decompressing process their original case is restored. As the tested genomes contain low number of soft-masked sequences and as this study is using genomics data for testing purposes, these sequences were just removed from the genomes. The python script used to perform these preprocessing steps and the size of each genome, before and after applying the script, are provided in Supplementary Table S4 in Supplementary Material. \\n Compression ratio was the primary metric used for evaluating the performance of the proposed algorithm. It is equal to the size of the compressed file divided by the size of the uncompressed (original) file. The original files in this study are the one-line genome files. Other metrics include compression time (seconds), decompression time (seconds), compression speed (the size of the uncompressed file in MB divided by the compression time in seconds-MB/s), and the decompression speed (the size of the uncompressed file in MB divided by the decompression time in seconds-MB/s). \\n The following tools were selected as they are common tools for compressing textual data and implementing one or more compression algorithms available in the literature. This is meant to test compressing the resultant bins using all available encoding algorithms. The tools namely are: bcm, blzpack, brotli, bsc, bzip2, cmix, compress, freeze, gzip, hook, Huffman-codec, lizard, lrzip, lz4, lzb, lzfse, lzip, lzop, lzturbo, Nakamichi, ppm, qzip, rans static, rzip, snzip, srank, xz, zlib, zip, zpipe, and zstd. Description of each tool is provided in Supplementary Table S1 in Supplementary Material. The cumulative compression results (for all one-line genomes) are provided in Table 1, while the compression results for each one-line genome are listed in Supplementary Table S6. The most efficient tools in terms of compression ratio were lrzip (saved 14,317.40 MB out of 16,773.88 MB), brotli (13,958.42 MB), lzip (13,916.39 MB), xz (13,915.92 MB), bsc (13,391.09 MB), and bcm (13,314.83 MB). In addition, comparing the results of the commonly used tools (bzip2 and gzip) indicated bzip2 was better, saving 12,601.12 MB. \\n TABLE 1. \\n Compression performance for each common tool cumulatively over all tested genomes. \\n \\n \\n Tool \\n Compression ratio (%) \\n Compression time (s) \\n Decompression time (s) \\n Compression speed (MB/s) \\n Decompression speed (MB/s) \\n \\n \\n \\n bcm \\n 20.6217 \\n 4,071 \\n 3,728 \\n 4.1203 \\n 0.9279 \\n \\n \\n blzpack \\n 37.2279 \\n 227 \\n 149 \\n 73.8937 \\n 41.9098 \\n \\n \\n brotli \\n 16.7848 \\n 62,659 \\n 103 \\n 0.2677 \\n 27.3346 \\n \\n \\n bsc \\n 20.1670 \\n 2,369 \\n 68 \\n 7.0806 \\n 49.7469 \\n \\n \\n bzip2 \\n 24.8765 \\n 2,474 \\n 1,169 \\n 6.7801 \\n 3.5695 \\n \\n \\n compress \\n 25.3977 \\n 443 \\n 168 \\n 37.8643 \\n 25.3582 \\n \\n \\n freeze \\n 27.4616 \\n 6,078 \\n 233 \\n 2.7598 \\n 19.7698 \\n \\n \\n gzip \\n 26.9031 \\n 4,211 \\n 171 \\n 3.9833 \\n 26.3900 \\n \\n \\n hook \\n 21.3395 \\n 7,803 \\n 8,074 \\n 2.1497 \\n 0.4433 \\n \\n \\n Huffman-codec \\n 27.4015 \\n 1,152 \\n 401 \\n 14.5607 \\n 11.4621 \\n \\n \\n lizard \\n 34.8186 \\n 11,449 \\n 41 \\n 1.4651 \\n 142.4494 \\n \\n \\n lrzip \\n 14.6446 \\n 21,589 \\n 378 \\n 0.7770 \\n 6.4986 \\n \\n \\n lz4 \\n 52.7757 \\n 161 \\n 73 \\n 104.1856 \\n 121.2677 \\n \\n \\n lzfse \\n 29.3101 \\n 1,118 \\n 130 \\n 15.0035 \\n 37.8187 \\n \\n \\n lzip \\n 17.0353 \\n 20,079 \\n 304 \\n 0.8354 \\n 9.3996 \\n \\n \\n lzop \\n 47.6212 \\n 152 \\n 106 \\n 110.3545 \\n 75.3578 \\n \\n \\n LzTurbo \\n 28.4807 \\n 157 \\n 46 \\n 106.8400 \\n 103.8548 \\n \\n \\n ppm \\n 23.8049 \\n 5,020 \\n 6,314 \\n 3.3414 \\n 0.6324 \\n \\n \\n qzip \\n 41.9873 \\n 1,556 \\n 126 \\n 10.7801 \\n 55.8960 \\n \\n \\n rans \\n 24.0431 \\n 144 \\n 91 \\n 116.4853 \\n 44.3182 \\n \\n \\n rzip \\n 24.9315 \\n 2,515 \\n 1,279 \\n 6.6695 \\n 3.2697 \\n \\n \\n snzip \\n 45.5241 \\n 159 \\n 73 \\n 105.4961 \\n 104.6048 \\n \\n \\n srank \\n 41.2318 \\n 794 \\n 778 \\n 21.1258 \\n 8.8897 \\n \\n \\n xz \\n 17.0381 \\n 18,666 \\n 266 \\n 0.8986 \\n 10.7442 \\n \\n \\n zip \\n 26.9031 \\n 4,165 \\n 173 \\n 4.0273 \\n 26.0849 \\n \\n \\n zlib \\n 26.9187 \\n 4,278 \\n 117 \\n 3.9210 \\n 38.5924 \\n \\n \\n zpipe \\n 26.9187 \\n 4,283 \\n 106 \\n 3.9164 \\n 42.5973 \\n \\n \\n zstd \\n 26.7482 \\n 251 \\n 75 \\n 66.8282 \\n 59.8227 \\n \\n \\n \\n Next, seven versions of OST-DNA were implemented. In each version, one of the seven most efficient tools (bcm, brotli, bsc, lrzip, lzip, xz, and bzip2) is used to compress the bins generated by the OST-DNA algorithm. The command used by each tool to compress the one-line genomes is the same used to compress the bins. Each of these seven versions was applied to each one-line genome. The compression and decompression commands used to run each tool are provided in Supplementary Table S5 in Supplementary Material. The default options for each tool were used to compress the one-line genomes. The same commands (default options) were used to compress the bins. No parallel processing was applied. If a tool apply parallel processing by default, the options were modified to be single/sequential processing. In addition, each of the seven versions were run over window lengths of 25, 50, 100, 125, 150, 250, 500, 750, 1,000, 2,500, 5,000, and 10,000 to investigate the compression results over each of these lengths. Lastly, each of the seven versions were run across label lengths of 1, 2, 3, 4, and 5 to investigate also the results over each of these lengths. The best result in terms of compression ratio over all pairs of window lengths and label lengths and cumulatively (over all 17 one-line genomes) achieved by each OST-DNA version is shown in Table 2. A comparison of the results produced by each OST-DNA version indicated OST-DNA-bcm saved an additional 77.38 MB compared to bcm, OST-DNA-brotli: 140.41 MB, OST-DNA-bsc: 66.79 MB, OST-DNA-bzip2: 34.83 MB, OST-DNA-lrzip: 12.05 MB, OST-DNA-lzip: 38.34 MB, and OST-DNA-xz: 41.65 MB. This demonstrates that the proposed algorithm can improve compression results compared to the corresponding standalone tools. Moreover, the tools that are based on LZ algorithm (OST-DNA-brotli, OST-DNA-lrzip, OST-DNA-lzip, and OST-DNA-xz which are dictionary based algorithms and perform better when the input data is more redundant and the redundancies are closer to each other as the case of the input data in the bins) performed better than the other tools (OST-DNA-bcm, OST-DNA-bsc, and OST-DNA-bzip2 which are based on block sorting and BWT). \\n TABLE 2. \\n Compression performance for best window-length and label-length of each of the seven OST-DNA versions cumulatively over all tested genome s. \\n \\n \\n Tool \\n Window length \\n Label length \\n Compression ratio (%) \\n Compression time (s) \\n Decompression time (s) \\n Compression speed (MB/s) \\n Decompression speed (MB/s) \\n \\n \\n \\n bcm OST-DNA-bcm \\n - 250 \\n - 4 \\n 20.6217 20.1603 \\n 4,071 12,026 \\n 3,728 4,388 \\n 4.1203 1.3948 \\n 0.9279 0.7707 \\n \\n \\n brotli OST-DNA-brotli \\n - 750 \\n - 4 \\n 16.7848 15.9477 \\n 62,659 72,315 \\n 103 318 \\n 0.2677 0.2320 \\n 27.3346 8.4121 \\n \\n \\n bsc OST-DNA-bsc \\n - 250 \\n - 5 \\n 20.1670 19.7688 \\n 2,369 10,355 \\n 68 2,160 \\n 7.0806 1.6199 \\n 49.7469 1.5352 \\n \\n \\n bzip2 OST-DNA-bzip2 \\n - 250 \\n - 2 \\n 24.8765 24.6689 \\n 2,474 10,132 \\n 1,169 1,230 \\n 6.7801 1.6555 \\n 3.5695 3.3642 \\n \\n \\n lrzip OST-DNA-lrzip \\n - 1,000 \\n - 1 \\n 14.6446 14.5728 \\n 21,589 31,911 \\n 378 432 \\n 0.7770 0.5256 \\n 6.4986 5.6584 \\n \\n \\n lzip OST-DNA-lzip \\n - 750 \\n - 2 \\n 17.0353 16.8067 \\n 20,079 30,691 \\n 304 472 \\n 0.8354 0.5465 \\n 9.3996 5.9728 \\n \\n \\n xz OST-DNA-xz \\n - 750 \\n - 2 \\n 17.0381 16.7898 \\n 18,666 29,098 \\n 266 393 \\n 0.8986 0.5765 \\n 10.7442 7.1662 \\n \\n \\n \\n The best tool in terms of compression ratio was lrzip, yet OST-DNA-lrzip saved an additional 12.05 MB more than lrzip. In terms of compression time, bsc was the fastest tool. OST-DNA-bsc could save an additional 66.79 MB more than bsc with a practical increase in the compression/decompression times (hence corresponding decrease in compression and decompression speeds). These increases are a result of the time needed for classifying and binning sequences during compression, as well as the need to collect and restore the original genome during decompression. However, they can be decreased significantly as follows. First, the OST-DNA script was not optimized for implementation as it is intended in this study to provide proof of concept. Additional improvements to the script can reduce both the compression and decompression times by increasing the corresponding speeds. In addition, parallel processing, which could further reduce run-time, was not applied during the binning, compression, or decompression steps of OST-DNA. Finally, fewer bins would lead to faster sequence labeling and longer window lengths could speed up both compression and decompression, with a trade-off in the compression ratio. \\n The compression results for OST-DNA using each of the seven tools for each one-line genome were also better than the results using the corresponding standalone tool. This can be found by comparing the compression results using each OST-DNA version with each window length and each label length for the one-line genomes, as shown in Supplementary Table S7 (the compression results for the standalone tools are provided in Table 1). This is justified due to the fact that if the subsequences that are similar/redundant are long distant from each other (for instance at the beginning and end of the input data), then they will be compressed together using OST algorithm as they will be binned together but this is not the case with other compression methodologies especially if the input data is larger and larger. So, the longer distant similar/redundant subsequences and larger input data, the better advantage for OST algorithm compared to other compression methodologies. \\n By analyzing the compression results for all OST-DNA versions, using different sequence label lengths and the adopted classification methods in this work (i.e., Huffman tree encoding schema), we found the most efficient results correlated with a window length of 250 to 1,000 bases. This is reasonable, as lengths shorter or longer than this will yield a uniform distribution of bases in the sequence. However, dynamic window lengths can be more practical and feasible given the additional costs for encoding the lengths. We found efficient label lengths to be 2 and 4. This is reasonable as increased label lengths produce more bins and more similarities among sequences in the same bin. Compression is more efficient when sequences in a bin are more similar. Supplementary Table S8 shows compression results for each version of OST-DNA for each window and each label length cumulatively applied to all one-line genomes. Further analysis at the bin level, rather than the genome level, is provided in Supplementary Table S9. \\n Compression results produced by applying each OST-DNA version to each bin, using the same window and label lengths but with different genomes, were considerably different (see Supplementary Table S10). This was not the case for bins produced using the same label length and same genome, but with different window lengths (see Supplementary Table S11). This means that sequences with the same label but from different genomes differed significantly (even though their labels were the same). This observation suggests the need to find a set of labels or labeling steps that could compress sequences from any source (genome) with similar efficiency, to improve the compression results further. In other words, sequences that share a label from this set would be compressed at a similar rate, regardless of the source (genome) from which they were derived. This set of labels could also be used better archival of multiple genomes. \\n The current version of OST-DNA compresses each bin using a specific tool. However, this is not optimal. Finding a tool that optimally compresses each bin, or a novel algorithm that is customized for efficient compression based on the bin content or label, could further improve the overall performance. 4 Discussion \\n Note that the aim of this implementation of the proposed algorithm is to proof-the-concept. Academic and commercial versions and after careful sophistication and customized methods will be sought in the near future. \\n The binning/bucketing approach was suggested to compress NGS sequencing reads as these reads must be overlapped due to the similarity in the sequenced genome or the amplification step in the sequencing process. OST algorithm on the other hand, propose an approach to compress a single genome/text. There are algorithms proposed to compress a single genome but they rely on compression-by-referencing approach which compress the genome based on the similarities with another public genome. While OST algorithm does not relay on any external resources and take advantage of the similarities inside the genome itself using classification-then-binning approach. Moreover, for general texts (general alphabets such english language) there is no reliable reference that can be used by the compression-by-referencing approach, while OST algorithm is still applicable for any general text. Funding Statement \\n The author(s) declare that no financial support was received for the research, authorship, and/or publication of this article. Data availability statement \\n Source code of the seven OST-DNA tools are available at https://github.com/aalokaily/OST. Author contributions \\n AA: Writing–original draft. AT: Writing–review and editing. Conflict of interest \\n The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Publisher’s note \\n All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Supplementary material \\n The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fbinf.2024.1489704/full#supplementary-material \\n \\n References \\n \\n \\n \\nAwan F. S., Mukherjee A. (2001). “Lipt: a lossless text transform to improve compression,” in \\n Proceedings international Conference on information Technology: Coding and computing (IEEE), 452–460. [ Google Scholar] \\n \\n \\nBakr N. S., Sharawi A. A. (2013). Dna lossless compression algorithms. Am. J. Bioinforma. Res.\\n3, 72–81. 10.5923/j.bioinformatics.20130303.04\\n [ DOI] [ Google Scholar] \\n \\n \\nBentley J. L., Sleator D. D., Tarjan R. E., Wei V. K. (1986). A locally adaptive data compression scheme. Commun. ACM\\n29, 320–330. 10.1145/5684.5688\\n [ DOI] [ Google Scholar] \\n \\n \\nBonfield J. K., Mahoney M. V. (2013). Compression of fastq and sam format sequencing data. PloS one\\n8, e59190. 10.1371/journal.pone.0059190\\n [ DOI] [ PMC free article] [ PubMed] [ Google Scholar] \\n \\n \\nBurrows M., Wheeler D. J. (1994). A block-sorting lossless data compression algorithm. Citeseer. [ Google Scholar] \\n \\n \\nCapon J. (1959). A probabilistic model for run-length coding of pictures. IRE Trans. Inf. Theory\\n5, 157–163. 10.1109/tit.1959.1057512\\n [ DOI] [ Google Scholar] \\n \\n \\nCleary J., Witten I. (1984). Data compression using adaptive coding and partial string matching. IEEE Trans. Commun.\\n32, 396–402. 10.1109/tcom.1984.1096090\\n [ DOI] [ Google Scholar] \\n \\n \\nCock P. J., Fields C. J., Goto N., Heuer M. L., Rice P. M. (2010). The sanger fastq file format for sequences with quality scores, and the solexa/illumina fastq variants. Nucleic acids Res.\\n38, 1767–1771. 10.1093/nar/gkp1137\\n [ DOI] [ PMC free article] [ PubMed] [ Google Scholar] \\n \\n \\nCormack G. V., Horspool R. N. S. (1987). Data compression using dynamic markov modelling. Comput. J.\\n30, 541–550. 10.1093/comjnl/30.6.541\\n [ DOI] [ Google Scholar] \\n \\n \\nCover T. M. (1999). Elements of information theory. John Wiley and Sons. [ Google Scholar] \\n \\n \\nDuda J. (2013). Asymmetric numeral systems: entropy coding combining speed of huffman coding with compression rate of arithmetic coding. arXiv preprint arXiv:1311.2540\\n [ Google Scholar] \\n \\n \\nElias P. (1975). Universal codeword sets and representations of the integers. IEEE Trans. Inf. theory\\n21, 194–203. 10.1109/tit.1975.1055349\\n [ DOI] [ Google Scholar] \\n \\n \\nFano R. M. (1949). The transmission of information. Research Laboratory of Electronics: Massachusetts Institute of Technology. [ Google Scholar] \\n \\n \\nFraenkel A. S., Kleinb S. T. (1996). Robust universal complete codes for transmission and compression. Discrete Appl. Math.\\n64, 31–55. 10.1016/0166-218x(93)00116-h\\n [ DOI] [ Google Scholar] \\n \\n \\nFriend R. C. (2004). Transport layer security (TLS) protocol compression using lempel-ziv-stac (LZS). RFC\\n3943. 10.17487/RFC3943\\n [ DOI] [ Google Scholar] \\n \\n \\nGopinath A., Ravisankar M. (2020). “Comparison of lossless data compression techniques,” in \\n 2020 international Conference on inventive computation technologies (ICICT) (IEEE), 628–633. [ Google Scholar] \\n \\n \\nGrumbach S., Tahi F. (1994). A new challenge for compression algorithms: genetic sequences. Inf. Process. and Manag.\\n30, 875–886. 10.1016/0306-4573(94)90014-0\\n [ DOI] [ Google Scholar] \\n \\n \\nHosseini M., Pratas D., Pinho A. J. (2016). A survey on data compression methods for biological sequences. Information\\n7, 56. 10.3390/info7040056\\n [ DOI] [ Google Scholar] \\n \\n \\nHuffman D. A. (1952). A method for the construction of minimum-redundancy codes. Proc. IRE\\n40, 1098–1101. 10.1109/jrproc.1952.273898\\n [ DOI] [ Google Scholar] \\n \\n \\nJahaan A., Ravi T., Panneer Arokiaraj S. (2017). A comparative study and survey on existing dna compression techniques. Int. J. Adv. Res. Comput. Sci.\\n8. 10.26483/ijarcs.v8i3.3086\\n [ DOI] [ Google Scholar] \\n \\n \\nKavitha P. (2016). A survey on lossless and lossy data compression methods. Int. J. Comput. Sci. and Eng. Technol. (IJCSET)\\n7. [ Google Scholar] \\n \\n \\nKnuth D. E. (1985). Dynamic huffman coding. J. algorithms\\n6, 163–180. 10.1016/0196-6774(85)90036-7\\n [ DOI] [ Google Scholar] \\n \\n \\nKodituwakku S., Amarasinghe U. (2010). Comparison of lossless data compression algorithms for text data. Indian J. Comput. Sci. Eng.\\n1, 416–425. [ Google Scholar] \\n \\n \\nKryukov K., Ueda M. T., Nakagawa S., Imanishi T. (2020). Sequence compression benchmark (scb) database—a comprehensive evaluation of reference-free compressors for fasta-formatted sequences. GigaScience\\n9, giaa072. 10.1093/gigascience/giaa072\\n [ DOI] [ PMC free article] [ PubMed] [ Google Scholar] \\n \\n \\nLangdon G. G. (1984). An introduction to arithmetic coding. IBM J. Res. Dev.\\n28, 135–149. 10.1147/rd.282.0135\\n [ DOI] [ Google Scholar] \\n \\n \\nLi H., Handsaker B., Wysoker A., Fennell T., Ruan J., Homer N., et al. (2009). The sequence alignment/map format and samtools. Bioinformatics\\n25, 2078–2079. 10.1093/bioinformatics/btp352\\n [ DOI] [ PMC free article] [ PubMed] [ Google Scholar] \\n \\n \\nLipman D. J., Pearson W. R. (1985). Rapid and sensitive protein similarity searches. Science\\n227, 1435–1441. 10.1126/science.2983426\\n [ DOI] [ PubMed] [ Google Scholar] \\n \\n \\nMahoney M. V. (2005). Adaptive weighing of context models for lossless data compression. Tech. Rep.\\nFlorida Tech. [ Google Scholar] \\n \\n \\nMansouri D., Yuan X., Saidani A. (2020). A new lossless dna compression algorithm based on a single-block encoding scheme. Algorithms\\n13, 99. 10.3390/a13040099\\n [ DOI] [ Google Scholar] \\n \\n \\nMartín G. (1979). Range encoding: an algorithm for removing redundancy from a digitised message. Video Data Rec. Conf., 24–27. [ Google Scholar] \\n \\n \\nOberhumer M. (2008). Lzo-a real-time data compression library. [ Google Scholar] \\n \\n \\nRanganathan N., Henriques S. (1993). High-speed vlsi designs for lempel-ziv-based data compression. IEEE Trans. Circuits Syst. II Analog Digital Signal Process.\\n40, 96–106. 10.1109/82.219839\\n [ DOI] [ Google Scholar] \\n \\n \\nRyabko B. Y. (1980). Data compression by means of a “book stack”. Probl. Peredachi Inf.\\n16, 16–21. [ Google Scholar] \\n \\n \\nSalomon D. (2004). Data compression: the complete reference. Springer Science and Business Media. [ Google Scholar] \\n \\n \\nShannon C. E. (2001). A mathematical theory of communication. ACM Sigmob. Mob. Comput. Commun. Rev.\\n5, 3–55. 10.1145/584091.584093\\n [ DOI] [ Google Scholar] \\n \\n \\nStorer J. A., Szymanski T. G. (1982). Data compression via textual substitution. J. ACM (JACM)\\n29, 928–951. 10.1145/322344.322346\\n [ DOI] [ Google Scholar] \\n \\n \\nStout Q. (1980). Improved prefix encodings of the natural numbers (corresp.). IEEE Trans. Inf. Theory\\n26, 607–609. 10.1109/tit.1980.1056237\\n [ DOI] [ Google Scholar] \\n \\n \\nTunstall B. P. (1968). Synthesis of noiseless compression codes. Atlanta, FL: Georgia Institute of Technology. Ph.D. thesis. [ Google Scholar] \\n \\n \\nUthayakumar J., Vengattaraman T., Dhavachelvan P. (2018). Swarm intelligence based classification rule induction (CRI) framework for qualitative and quantitative approach: an application of bankruptcy prediction and credit risk analysis. J. King Saud University-Computer Inf. Sci.\\n32, 647–657. 10.1016/j.jksuci.2017.10.007\\n [ DOI] [ Google Scholar] \\n \\n \\nVitter J. S. (1987). Design and analysis of dynamic huffman codes. J. ACM (JACM)\\n34, 825–845. 10.1145/31846.42227\\n [ DOI] [ Google Scholar] \\n \\n \\nWelch T. A. (1984). A technique for high-performance data compression. Computer\\n17, 8–19. 10.1109/mc.1984.1659158\\n [ DOI] [ Google Scholar] \\n \\n \\nWillems F. M., Shtarkov Y. M., Tjalkens T. J. (1995). The context-tree weighting method: basic properties. IEEE Trans. Inf. theory\\n41, 653–664. 10.1109/18.382012\\n [ DOI] [ Google Scholar] \\n \\n \\nWilliams R. N. (1991). “An extremely fast ziv-lempel data compression algorithm,” in \\n [1991] proceedings. Data compression conference (IEEE), 362–371. [ Google Scholar] \\n \\n \\nZiv J., Lempel A. (1977). A universal algorithm for sequential data compression. IEEE Trans. Inf. theory\\n23, 337–343. 10.1109/tit.1977.1055714\\n [ DOI] [ Google Scholar] \\n \\n \\nZiv J., Lempel A. (1978). Compression of individual sequences via variable-rate coding. IEEE Trans. Inf. Theory\\n24, 530–536. 10.1109/tit.1978.1055934\\n [ DOI] [ Google Scholar] \\n Associated Data \\n This section collects any data citations, data availability statements, or supplementary materials included in this article. \\n Supplementary Materials \\n Data Availability Statement \\n Source code of the seven OST-DNA tools are available at https://github.com/aalokaily/OST. \\n \\n \\n', highlights=None, highlight_scores=None, summary=None), Result(url='https://www.mdpi.com/1996-1073/15/24/9345', id='https://www.mdpi.com/1996-1073/15/24/9345', title='A Novel Data Compression Methodology Focused on Power Quality Signals Using Compressive Sampling Matching Pursuit', score=None, published_date=None, author='Silvana Varela', image='https://pub.mdpi-res.com/energies/energies-15-09345/article_deploy/html/images/energies-15-09345-g001-550.jpg?1670595684', favicon='https://pub.mdpi-res.com/img/mask-icon-128.svg', subpages=None, extras=None, text='\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Article Menu \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Font Type:\\n \\n Arial \\n Georgia \\n Verdana \\n \\n \\n \\n Open Access Article \\n \\n \\n by\\n \\n Milton Ruiz *, \\n Manuel Jaramillo, \\n Alexander Aguila, \\n Leony Ortiz and \\n Silvana Varela \\n \\n \\n \\n \\n Carrera de Electricidad, Universidad Politécnica Salesiana, Quito 170146, Ecuador \\n \\n \\n \\n \\n * \\n Author to whom correspondence should be addressed. \\n \\n \\n \\n \\n \\n \\n Submission received: 6 October 2022 \\n /\\n Revised: 21 November 2022 \\n /\\n Accepted: 28 November 2022 \\n /\\n Published: 9 December 2022 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Abstract: \\n In this research a new data compression technique for electrical signals was proposed. The methodology combined wavelets and compressed sensing techniques. Two algorithms were proposed; the first one was designed to find specific characteristics of any type of energy quality signal such as the number of samples per cycle, zero-crossing indices, and signal amplitude. With the data obtained, the second algorithm was designed to apply a biorthogonal wavelet transform resulting in a shifted signal, and its amplitude was modified with respect to the original. The errors were rectified with the attributes found in the early stage, and the application of filters was conducted to reduce the ripple attached. Then, the third algorithm was designed to apply Compressive Sampling Matching Pursuit, which is a greedy algorithm that creates a dictionary with orthogonal bases representing the original signal in a sparse vector. The results exhibited excellent features of quality and were accomplished by the suggested compression and reconstruction technique. These results were a compression ratio of 1020:1, that is, the signal was compressed by 99.90% with respect to the original one. The quality indicators achieved were RTE = 0.9938, NMSE = 0.0098, and COR = 0.99, exceeding the results of the most relevant research papers published in Q1 high-impact journals that were further discussed in the introduction section. \\n \\n \\n \\n \\n 1. Introduction An electrical power system (EPS) encompasses subsystems such as generation, transmission, distribution, market, and users. An EPS is the most complex system built by mankind due to the fact that the costs of communication systems and sensors have reduced rapidly, resulting in the growth of the fitting of millions of sensors in all areas. The electricity consumption of EPSs is managed by measurement and control devices with suitable levels of communication, resistance, reliability, safety, and the capacity to adjust to loads that can vary frequently. A smart grid manages all interactions between physical and computational components and systems that operate in parallel. For the system’s management, not only must electrical variables be considered but also commercial models, economic opportunities, technologies, and regulatory policies, resulting in a new smart electrical network based on distributed systems, such as energy management systems (EMS), demand response management systems (DRMS), advanced distribution management systems (ADMS), advanced metering infrastructures (AMI), distributed energy resource management systems (DERMS), etc. [ 1]. These are high-penetration systems that monitor a network that depend on of the number of measurement points, and the sampling rate can generate zettabytes (ZB) of data that must be processed, transmitted, and stored. By the end of 2022, the amount of data generated by China by the IoT is estimated to be 10 ZB [ 2]. 2. Problem Formulation As was shown in the above-mentioned summary of the state-of-the-art in big data, a trend is evident in the installation of measurement equipment at all levels of electrical systems, from generation to consumer, generating enormous amounts of information. The electrical signals are sampled at high frequencies in Hz and stored in vectors where the domain is in terms of time, frequency, and power, and the range is in terms of the values of voltage, current, etc. The resolution of the signal depends on the quantization bits of the analogue-to-digital converters (adc), forming vectors The inclusion of several metering devices in the residential, commercial, and industrial sectors, generating reliable electrical networks with a low environmental impact and with financial benefits for making informed decisions based on an electricity market in real time. 2.1. Lossless Compression The answer to the problem of the use of lossy compression was proposed based on the representation of signals in their orthogonal bases, such that for 0 &lt; p &lt; ∞. In numerical analysis, wavelets are used as a tool for solving partial differential equations and applications of linear operators are used for arbitrary functions, Such as applications in sound engineering, image, and signal processing. An orthonormal wavelet basis for is a family of functions: \\n \\n \\n \\n By translation and dilation of the mother wavelet , any function can be expressed as a function of the wavelet : \\n \\n \\n \\n Maintaining the equality, the wavelet coefficients are calculated by the scalar products: \\n \\n \\n \\n The biorthogonal wavelet for can be represented as: \\n \\n \\n \\n Let be a dictionary of vectors having the unit of -norm, i.e., for all This dictionary is supposed to be complete, which means that it includes K linearly independent vectors that define a basis of the signal space 2.2. Compressive Sampling Matching Pursuit (CoSaMP) For vectors that represent sparse and compressible signals in , the “quasinorm” is defined as: \\n \\n \\n \\n Signal is sparse when . Real signals can be transformed to sparse signals, which means that their inputs decay fast when categorized by magnitude. As a result, compressible signals closely approximate sparse signals, for example, orthonormal bases such as a Fourier or wavelet basis. CoSaMP is a greedy pursuit algorithm that integrates combinatorial algorithms to ensure speed and to provide accurate error bounds. The most important information of the signals is stored in the discrete vectors , where the values are the representations of atoms. The reconstruction of the signal is carried out with an alternative technique to Nyquist, since the vector\\xa0is a representation of the original signal with sparse data. To reconstruct the original signal\\xa0, a vector is required that is the sparse representation of the original signal,\\xa0, multiplied by a dictionary matrix, , which are the orthogonal bases of the original signal. The rows of the dictionary are much smaller than the columns fulfilling \\xa0plus an error approximation . Finally, x \\n , where\\xa0\\n . For a given precision parameter, , the algorithm CoSaMP produces a sparse approximation that satisfies: \\n \\n \\n \\n \\n where is a best -sparse approximation to . The running time is , where bounds the cost of a matrix vector multiplied by Φ. The working storage is O(N). 2.3. Reconstruction Quality Metrics The compression process has metrics that allow the analyzing of the quality of processed signals, taking into account the reconstructed signal and comparing it with the original signal . The two vectors must have the same dimension, that is, , that is, they have the same number of samples that symbolize the values of the signal in discrete time. Next, the equations that allow the determining of the quality of a signal are presented: Normalized mean-squared error ().—the result between the original signal and the reconstructed signal must be the closest to 0. The equation is presented below: \\n \\n \\n \\n Correlation ().—the result between the original signal and the reconstructed signal must be the closest to 1, where the operator “·” is the inner product of the vectors. The equation is shown as follows: \\n \\n \\n \\n Percentage of retained energy ().—the result between the original and the reconstructed signal should be as close to 100%. The equation is shown as follows: \\n \\n \\n \\n 2.4. Proposed Algorithms This research proposed three algorithms. Algorithm 1 was developed to extract signals characteristic such as the zero-crossing index, the maximum amplitude, and the number of samples per period, thus allowing any type of signal with different sampling rates or amplitudes to be compressed. The first step is the acquisition of data from any measurement device. The data can contain different signals such as the voltage and current measurements of three phases at the beginning and end of a transmission line along with time, with a total of 13 signals that are arranged in a matrix called Electrical Signal “ES”. For Algorithm 1 to be executed dynamically regardless of the number of signals, the number of rows and columns of the matrix is calculated, and the values in the Row Electrical Signal “RES” and Column Electrical Signal “CES” are stored. The second step is the extraction of the characteristics of the signals such as the zero-crossing index that stores the values in Start Zero-Crossing “SZC” and Final Zero-Crossing “FZC”. Identifying the zero-crossing index allows the algorithm to quantify the number of samples per cycle “NS” and to determine the maximum amplitudes of each signal “ML” and the location index “IL”. The third step is to determine the orthogonal bases of each signal using the six level biorthogonal wavelet transform. “C” contains the wavelet decomposition, and “L” contains the number of coefficients per level. Finally, the approximation coefficients at level N are calculated using the wavelet decomposition structure [C,L]. Finally, the result is the compressed signal “CS”. It must be taken into consideration that the execution time of Matching Pursuit depends on the number of samples of the original signal. The time complexity in big O notation is . It is for this reason that the signal is first compressed by applying a wavelet in order to later apply compressed sensing, obtaining low compression and reconstruction times. The fourth step separates the time vector from the signals by taking the first and last time values along with the size of the vector and storing them in the Compression Time “CT”. Next, the size of the new Compressed Data CD matrix that contains only the signal samples is calculated, storing the values in the Row Compressed Data “RCD” and Column Compressed Data “CCD”. The fifth step is the same as that performed in the second step. The sixth step creates an identity matrix of size NS, which is the number of samples per cycle, and stores it in the Identity Matrix “ID”, and then creates the transposed Discrete Cosine Transform matrix of size NS and stores it in the variable Psi. With these two matrices, the Phi sensing matrix is formed. Next, values are assigned to certain parameters that are required by Matching Pursuit.\\n Algorithm 1 Feature extraction and orthogonal bases 1: Step 1: Acquire data from a database file 2:\\xa0\\xa0\\xa0\\u2003ES = 𝑙𝑜𝑎𝑑 (′𝑓𝑖𝑙𝑒𝑛𝑎𝑚𝑒′) 3:\\xa0\\xa0\\xa0\\u2003[RES,CES] = size(ES) 4: Step 2: Feature Extraction 5:\\xa0\\xa0\\xa0\\u2003𝑓𝑜𝑟 𝑖 = 1: 𝐶ES 6:\\u2003\\u2003\\u2003\\u2003\\u2003SZC (𝑖) = ES (:, 𝑖) &lt; 0 𝑎𝑛𝑑 ES (:+1, 𝑖) &gt; 0 7:\\u2003\\u2003\\u2003\\u2003\\u2003FZC (𝑖) = ES (:, 𝑖) &gt; 0 𝑎𝑛𝑑 ES (:+1, 𝑖) &lt; 0 8:\\u2003\\u2003\\u2003\\u2003\\u2003NS(𝑖) = SZC (𝑖)- FZC (𝑖) 9:\\xa0\\xa0\\xa0\\u2003𝑒𝑛𝑑𝑓𝑜𝑟 10:\\xa0\\xa0\\xa0\\xa0\\xa0𝑓𝑜𝑟 𝑖 = 1: 𝐶ES 11:\\u2009\\u2009\\xa0\\xa0\\u2003\\u2003\\u2003[ML (𝑖), IL (𝑖)] = 𝑚𝑎𝑥 (ES(SZC (𝑖):FZC(𝑖), 𝑖)) 12:\\xa0\\xa0\\xa0\\xa0\\xa0𝑒𝑛𝑑𝑓𝑜𝑟 13: Step 3: Orthogonal bases 14:\\xa0\\xa0\\xa0\\xa0\\xa0for i = 1: CES 15:\\u2009\\u2009\\xa0\\xa0\\u2003\\u2003\\u2003[C, L] = wavedec(ES(:,i),level,’bior1.1’); 16:\\u2009\\u2009\\xa0\\xa0\\u2003\\u2003\\u2003CS(:i) = appcoef(C,L,’bior1.1’); 17:\\xa0\\xa0\\xa0\\xa0\\xa0end 18: Step 4: Split the time vector from the data 19:\\xa0\\xa0\\xa0\\xa0\\xa0CT = [ES (1,1) ES (end,1) RES] 20: \\u2003\\u2003\\u2003\\u2003\\xa0\\xa0CD = (CS(:,2:end)) 21:\\xa0\\xa0\\xa0\\xa0\\xa0[RCD,CCD] = size(CD) 22: Step 5: Feature Extraction 23:\\xa0\\xa0\\xa0\\xa0\\xa0𝑓𝑜𝑟 𝑖 = 1: CCD 24:\\u2009\\u2009\\xa0\\xa0\\xa0\\u2003\\u2003\\u2003SZC (𝑖) = CD (:, 𝑖) &lt; 0 𝑎𝑛𝑑 CD (:+1, 𝑖) &gt; 0 25:\\u2009\\u2009\\xa0\\xa0\\u2003\\u2003\\u2003\\xa0FZC (𝑖) = CD (:, 𝑖) &gt; 0 𝑎𝑛𝑑 CD (:+1, 𝑖) &lt; 0 26:\\u2009\\u2009\\xa0\\xa0\\xa0\\u2003\\u2003\\u2003NS(𝑖) = SZC (𝑖)- FZC (𝑖) 27: \\xa0\\xa0\\xa0\\xa0\\xa0𝑒𝑛𝑑𝑓𝑜𝑟 28: \\xa0\\xa0\\xa0\\xa0\\xa0𝑓𝑜𝑟 𝑖 = 1: CCD 29: \\u2003\\u2003\\u2003\\u2003[ML (𝑖), IL (𝑖)] = 𝑚𝑎𝑥 (CD(SZC (𝑖) :FZC(𝑖), 𝑖)) 30: \\xa0\\xa0\\xa0\\xa0\\xa0𝑒𝑛𝑑𝑓𝑜𝑟 31: Step 6: Compressed sensing 32:\\xa0\\xa0\\xa0\\xa0\\xa0ID = eye(NS) 33:\\xa0\\xa0\\xa0\\xa0\\xa0Psi = dctmtx(NS)’ 34:\\xa0\\xa0\\xa0\\xa0\\xa0Phi = [ID Psi] 35:\\xa0\\xa0\\xa0\\xa0\\xa0k = 4 36:\\xa0\\xa0\\xa0\\xa0\\xa0SV = Compressive Sampling Matching Pursuit (Phi, k) 37:\\xa0\\xa0\\xa0\\xa0\\xa0CSMP = SV(CD) 38:\\xa0\\xa0\\xa0\\xa0\\xa0Data.CT = CT 39:\\xa0\\xa0\\xa0\\xa0\\xa0Data.CSMP = CSMP For example, the minimum number of atoms (variable k) that are necessary to carry out an acceptable reconstruction is three when compared to the fifty-two samples per cycle of the signal. Next, the Compressive Sampling Matching Pursuit algorithm is implemented with the Phi matrix and k values and is stored in the solver variable SV. The signal compression is performed by entering each cycle of the signal in the SV solver, and the result is a sparse vector of size 1 × 52, in which only one value is presented and the remaining fifty-one are zeros, which are stored in the Compressed Signal Matching Pursuit CSMP variable Finally, in order not to store an array of zeros, the values obtained with the indices of the positions and the time are stored in an array of arrays, which is a structure called Data. Algorithm 2 presents the steps that must be performed to reconstruct the signal. The first step is the acquisition of the data from the Data structure. The second step is conducted to form the signal, where it must be taken into account that the only values stored in CSMP are the four data values and the indices that represent the locations in the matrix of zeros in each period of the signal. The second step is the signal reconstruction. The reconstruction of each period is conducted by multiplying CSMP by the Phi matrix, thus obtaining the CS signal. If the application requires the same amount of data as the original signal, ES, interpolation must be performed, thus obtaining a reconstructed signal with the same size as the original signal that is stored in Signal Recovery SR. The CT values are the start time, end time, and the number of samples of the original signal, which allow the creation of a time vector equal to the original one. Finally, it is necessary to calculate the metrics of the quality of the signals that are NMSE, COR, and RTE between the original signal and the reconstructed one.\\n Algorithm 2 Reconstruction 1: Step 1: Data acquisition 2:\\u2003\\xa0\\xa0\\u2009load Data 3:\\u2003\\xa0\\xa0\\u2009\\u2003\\u2003\\u2003CT = Data.CT 4:\\u2003\\xa0\\xa0\\u2009CSMP = Data.CSMP 5: Step 2: Reconstruction 6:\\u2003\\xa0\\xa0\\u2009CS = Phi * CSMP 7:\\u2003\\xa0\\xa0\\u2009SR = interpft(CS,CT(3)); 8:\\u2003\\xa0\\xa0\\u2009TR = linspace(CT(1), CT(2), CT(3)) 9:\\u2003\\xa0\\xa0\\u2009NMSE 10:\\xa0\\xa0\\xa0\\xa0\\u2009COR 11:\\xa0\\xa0\\xa0\\xa0\\u2009RTE 3. Results The power quality variables analyzed were swell, sag, flicker, triphasic fault, and stable state. For the measurement of very fast transients such as flickers or atmospheric discharges that are of a short duration, impulses between 50 ns and 1 ms are required. The established sampling frequency was 200 kHz for 0.33 s, generating 66,000 samples per phase. The matrix that was formed by each electrical distortion was of a size of 66,000 × 4 and contained the time along with the measurements of the three phases R, S, and T. The disk size was 1,889,931 bytes. The equipment in which the entire signal compression process was carried out was a laptop with an Intel(R) Xeon(R) E-2176M Processor CPU @ 2.70 GHz and with 64 GB of RAM; the GPUs of the graphics card were used for parallel processing. Figure 1 shows, in the upper-left part, different types of electrical faults, where the signals are represented in the time domain and are displayed by cycles. The number of samples for each signal was 52 per cycle. Next, different results are shown by modifying the index k, which is the number of atoms in the signal. In the upper right part, the graphical results of the algorithms proposed with k = 1 are presented. It can be seen that a minimum of one atom was required to carry out the reconstruction of the original signal with a certain degree of error. In the lower-left part, the result is presented with k = 5. It can be seen that a minimum of five atoms were required to carry out the reconstruction of the original signal with a certain degree of error. In the bottom-right part, the result is presented with k = 10. It can be seen that a minimum of ten atoms were required to carry out the reconstruction of the original signal with a certain degree of error. The results obtained are presented below. Figure 2, Figure 3 and Figure 4 shows the compression results for signals in a steady state, plus the swell, flicker, triphasic fault, and sag, respectively. The index k was varied from 1 to 10. When k = 1, the optimization process calculated the single most representative atom; when the index k increased, the number of atoms increased in steps of one. It can be seen that while the index k increased, the RTE, NMSE, and COR metrics improved, the TC compression times and restoration times were reduced due to the shorter optimization process, and the final weight increased while the compression ratio decreases. Figure 2 shows the statistical results of the 50 simulations performed using different power quality signals and by varying the number of atoms. The retained energy percentage reached optimal levels from k = 4, as shown in the results, with an RTE between 98.2% and 99.7%. Figure 3 shows the similarity degree of the statistical results between the original and reconstructed signals. The correlation of the 50 simulations carried out is presented, and the optimal levels were achieved from k = 5, as shown in the results, with a COR between 99.6% and 99.8%. Figure 4 shows the statistical results of the degree of the normalized mean-square error between the original signal and the reconstructed signal. The NMSE of the 50 simulations carried out is presented, and the optimal levels were achieved from k = 4, as shown in the results, with an NMSE between 0.0028% and 0.017%. Table 1 shows the average compression and reconstruction times, the final size of the signal, the compression percentage, and the average compression ratio of the different energy quality signals with the number of atoms ranging from one to ten. 4. Analysis of Results In recent years, research on the compression of electrical energy signals has increased due to the impact of intelligent electrical networks. It is for this reason that the results of Q1 high-impact journal articles from the last 5 years until 2022 served as a reference to be achieved and surpassed by this research. Below is a summary ( Table 2) of the best results achieved from the most relevant articles. The Compression Ratio obtained exceeded 1020:1 with a 99.90% compression of the original signal, surpassing all the high-impact research until 2022. 5. Conclusions This research presented a methodology that used compressed sensing techniques and contributed to the processes of measurement, transmission, processing, and storage of information, thanks to the high levels of compression achieved. The results achieved with the algorithms proposed in this research compressed the electrical signals of power quality with ratios of 2216:1, but the quality indicators were not good. To improve the indicators, we recommend using a minimum value of k = 3; in this way, all the results achieved by other researchers until the year 2022 were surpassed. To reconstruct a signal that contained fifty-two samples per cycle, a minimum of one atom was required. The atom was the most representative value of the signal and was obtained by applying the Compressive Sampling Matching Pursuit technique. Each signal contained different values and positions of the atoms. Atom positions depend on changes that might happen in the signal under analysis. Finally, the compression level presents inverse proportion when compared to the quality indicators of the RTE, NMSE, and COR signals. As can be seen in the results, the higher the compression, for example with k = 1and considering a compression ratio of of 2216:1, the correlation was 70.00. On the other hand with k = 3 and a lower level of compression of 1024:1, the correlation was 99.07 It is proposed as future work to use the algorithms proposed in the present investigation with windowing, combining compression with and without loss of information to further improve the compression indexes RTE, NMSE, and COR. \\n \\n \\n Author Contributions Conceptualization, methodology, software, resources, validation, and formal analysis, M.R.; investigation, writing—original draft preparation, M.R., M.J., A.A., L.O. and S.V. All authors have read and agreed to the published version of the manuscript. Funding This work was supported by Universidad Politécnica Salesiana. Data Availability Statement Not applicable. Conflicts of Interest The authors declare no conflict of interest. References Gopstein, A.; Nguyen, C.; O’Fallon, C.; Hastings, N.; Wollman, D. NIST Framework and Roadmap for Smart Grid Interoperability Standards, Release 4.0; Department of Commerce, National Institute of Standards and Technology: Gaithersburg, MD, USA, 2021. [ Google Scholar] [ CrossRef] Liu, H.; Huang, F.; Li, H.; Liu, W.; Wang, T. A big data framework for electric power data quality assessment. In Proceedings of the 2017 14th Web Information Systems and Applications Conference, WISA 2017, Liuzhou, China, 11–12 November 2017; pp. 289–292. [ Google Scholar] [ CrossRef] Pinto, L.S.; Assunção, M.V.; Ribeiro, D.A.; Ferreira, D.D.; Huallpa, B.N.; Silva, L.R.; Duque, C.A. Compression Method of Power Quality Disturbances Based on Independent Component Analysis and Fast Fourier Transform. Electr. Power Syst. Res. 2020, 187, 106428. [ Google Scholar] [ CrossRef] Mogahed, H.; Yakunin, A. Development of a Lossless Data Compression Algorithm for Multichannel Environmental Monitoring Systems. In Proceedings of the 2018 14th International Scientific-Technical Conference APEIE–44894, Novosibirsk, Russia, 2–6 October 2018. [ Google Scholar] [ CrossRef] Ruiz, M.; Simani, S.; Inga, E.; Jaramillo, M. A novel algorithm for high compression rates focalized on electrical power quality signals. Heliyon 2021, 7, e06475. [ Google Scholar] [ CrossRef] [ PubMed] Dong, S.; Xu, M.; Zhou, A.; Zhu, L.; Qiao, J.; Bo, S. Research on Architecture of Power Big Data High-Speed Storage System for Energy Interconnection. In Proceedings of the 4th IEEE International Conference on Automation, Electronics and Electrical Engineering, AUTEEE 2021, Shenyang, China, 19–21 November 2021; pp. 588–592. [ Google Scholar] [ CrossRef] Bo, W.; Fei, G.; Mei, L.; Faxian, Q. Research on Electric Power Emergency Warning Mechanism Based on Meteorological Big Data. In Proceedings of the 2021 6th International Symposium on Computer and Information Processing Technology, ISCIPT 2021, Changsha, China, 11–13 June 2021; pp. 362–365. [ Google Scholar] [ CrossRef] Liu, B.; Zhu, C.; Wang, D.; Dong, P. Design of verification and verification system for electric metering pipeline meters based on big data analysis. In Proceedings of the 2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering, ICBAIE 2021, Nanchang, China, 27 March 2021; pp. 248–251. [ Google Scholar] [ CrossRef] Zhang, L.; Li, Y.; Qiu, B.; Zhang, J.; Liang, W. Design of communication power centralized remote monitoring system based on big data technology. In Proceedings of the 2021 International Conference on Electronics, Circuits and Information Engineering, ECIE 2021, Zhengzhou, China, 22–24 January 2021; pp. 46–49. [ Google Scholar] [ CrossRef] Yuefu, F.; Yongchao, W.; Dongdong, C.; Yugui, N. Research on Reliability Evaluation of Electric Power Communication Network Based on Big Data. In Proceedings of the 2019 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS), Shenyang, China, 29–31 July 2019. [ Google Scholar] Silva, L.R.M.; de Andrade Filho, L.M.; Duque, C.A. Sparse representation algorithm applied to power systems signal compression. Int. Trans. Electr. Energy Syst. 2019, 29, e2693. [ Google Scholar] [ CrossRef] Basavaraj, S. Approach for Power Quality Monitoring and Data Compression. In Proceedings of the International Conference on Power and Energy Systems: Towards Sustainable Energy (PESTSE), Bengaluru, India, 21–23 January 2016; Volume 1, pp. 4–8. [ Google Scholar] De Andrade, L.C.M.; Nanjundaswamy, T.; Oleskovicz, M.; Fernandes, R.A.S.; Rose, K. Advances in Classification and Compression of Power Quality Signals. J. Control. Autom. Electr. Syst. 2019, 30, 402–412. [ Google Scholar] [ CrossRef] Ruiz, M.; Montalvo, I. Electrical faults signals restoring based on compressed sensing techniques. Energies 2020, 13, 2121. [ Google Scholar] [ CrossRef] [ Green Version] Wang, X.; Tian, L.; Gao, Y.; Hou, Y. Analysis of power quality disturbance signal based on improved compressed sensing reconstruction algorithm. In Proceedings of the 2017 IEEE Transportation Electrification Conference and Expo, Asia-Pacific, ITEC Asia-Pacific 2017, Harbin, China, 7–10 August 2017; pp. 1–5. [ Google Scholar] [ CrossRef] Chen, C.; Xi, W.; Cui, Y.; Dong, T.; Shang, W. Compression algorithm for relay protection equipment data. In Proceedings of the 2019 IEEE 3rd Conference on Energy Internet and Energy System Integration (EI2), Beijing, China, 26–28 November 2019. [ Google Scholar] [ CrossRef] Khan, J. Weighted entropy and modified MDL for compression and denoising data in smart grid. Int. J. Electr. Power Energy Syst. 2021, 133, 107089. [ Google Scholar] [ CrossRef] Noronha Barros, F.G.; Fonseca, W.A.D.S.; Bezerra, U.H.; Nunes, M.V.A. Compression of electrical power signals from waveform records using genetic algorithm and artificial neural network. Electr. Power Syst. Res. 2017, 142, 207–214. [ Google Scholar] [ CrossRef] Sahasranand, K.R.; Joseph, F.C.; Tyagi, H.; Gurrala, G.; Joglekar, A. Anomaly-Aware Adaptive Sampling for Electrical Signal Compression. IEEE Trans. Smart Grid 2022, 13, 2185–2196. [ Google Scholar] [ CrossRef] Silva, L.R.M.; Kapisch, E.B.; Martins, C.H.N.; Filho, L.M.A.; Cerqueira, A.S.; Duque, C.A.; Ribeiro, P.F. Gapless Power-Quality Disturbance Recorder. IEEE Trans. Power Deliv. 2017, 32, 862–871. [ Google Scholar] [ CrossRef] Kapisch, E.B.; de Morais, V.V.; Silva, L.R.M.; Filho, L.M.A.; Duque, C.A. Spectral Variation-Based Signal Compression Technique for Gapless Power Quality Waveform Recording in Smart Grids. IEEE Trans. Ind. Inform. 2022, 18, 4488–4498. [ Google Scholar] [ CrossRef] \\n \\n Figure 1. \\n Electrical signal and cycle compression.\\n \\n \\n \\n Figure 1. \\n Electrical signal and cycle compression. \\n \\n \\n \\n Figure 2. \\n Percentage of retained energy, RTE.\\n \\n \\n \\n Figure 2. \\n Percentage of retained energy, RTE. \\n \\n \\n \\n Figure 3. \\n Correlation, COR.\\n \\n \\n \\n Figure 3. \\n Correlation, COR. \\n \\n \\n \\n Figure 4. \\n Normalized mean-squared error NMSE.\\n \\n \\n \\n Figure 4. \\n Normalized mean-squared error NMSE. \\n \\n \\n \\n Table 1. \\n Results summary.\\n \\n \\n \\n \\n Table 1. \\n Results summary. \\n \\n ATOMS TC [s] TR [s] Final Weight [Bytes] Compresion [%] Compresion Ratio K = 1 0.20772 0.01230 855 99.95476 2210 K = 2 0.03492 0.01009 1368 99.92762 1382 K = 3 0.02229 0.00673 1858 99.90169 1017 K = 4 0.02395 0.00642 2375 99.87433 796 K = 5 0.02351 0.00714 2903 99.84640 651 K = 6 0.02625 0.00714 3489 99.81539 542 K = 7 0.02952 0.00714 3982 99.78930 475 K = 8 0.03596 0.00795 4470 99.76348 423 K = 9 0.03184 0.00776 4990 99.73597 379 K = 10 0.04381 0.00624 5498 99.70909 344 \\n \\n \\n \\n Table 2. \\n Summary of the best results achieved from the most relevant articles.\\n \\n \\n \\n \\n Table 2. \\n Summary of the best results achieved from the most relevant articles. \\n \\n Signal Type RTE NMSE COR [%] TC [s] TR [s] Compression [%] Compression Ratio References Steady state k = 3 0.992270 0.010817960 99.0755565 0.02355 0.00378 99.90238 1024:1 This research Electrical fault 0.9998 0.001105 99.9534 0.03652 0.1447 99.421 800:1 Q1 [ 5] Electrical fault -- 37.5 × 10 −8 -- 0.04709 81 -- Q1 [ 17] Electrical fault -- 0.0217 -- 24:1 Q1 [ 18] Electrical fault -- 0.010780 -- -- -- -- 31:1 Q1 [ 19] Steady state 0.9780 0.0277 98.90 -- -- 99.776 507:1 Q1 [ 21] \\n \\n Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations. \\n © 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/). \\n \\n \\n \\n Share and Cite \\n \\n \\n \\n MDPI and ACS Style \\n \\n Ruiz, M.; Jaramillo, M.; Aguila, A.; Ortiz, L.; Varela, S. \\n A Novel Data Compression Methodology Focused on Power Quality Signals Using Compressive Sampling Matching Pursuit. Energies 2022, 15, 9345.\\n https://doi.org/10.3390/en15249345\\n \\n \\n AMA Style \\n Ruiz M, Jaramillo M, Aguila A, Ortiz L, Varela S. \\n A Novel Data Compression Methodology Focused on Power Quality Signals Using Compressive Sampling Matching Pursuit. Energies. 2022; 15(24):9345.\\n https://doi.org/10.3390/en15249345\\n \\n Chicago/Turabian Style \\n Ruiz, Milton, Manuel Jaramillo, Alexander Aguila, Leony Ortiz, and Silvana Varela. \\n 2022. \"A Novel Data Compression Methodology Focused on Power Quality Signals Using Compressive Sampling Matching Pursuit\" Energies 15, no. 24: 9345.\\n https://doi.org/10.3390/en15249345\\n \\n APA Style \\n Ruiz, M., Jaramillo, M., Aguila, A., Ortiz, L., &amp; Varela, S. \\n \\n (2022). A Novel Data Compression Methodology Focused on Power Quality Signals Using Compressive Sampling Matching Pursuit. Energies, 15 (24), 9345.\\n https://doi.org/10.3390/en15249345\\n \\n \\n \\n \\n Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here.\\n \\n \\n Article Metrics \\n \\n \\n \\n \\n \\n \\n', highlights=None, highlight_scores=None, summary=None), Result(url='https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8', id='https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8', title='Robust, practical and comprehensive analysis of soft compression image coding algorithms for big data', score=0.3843689560890198, published_date='2023-02-02T00:00:00.000Z', author='Gangtao Xin', image=None, favicon=None, subpages=None, extras=None, text='1\\nVol.:(0123456789)\\nScientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\\nwww.nature.com/scientificreports\\nRobust, practical \\nand comprehensive analysis \\nof soft compression image coding \\nalgorithms for big data\\nGangtao Xin1,2 & Pingyi Fan1,2*\\nWith the advancement of intelligent vision algorithms and devices, image reprocessing and secondary \\npropagation are becoming increasingly prevalent. A large number of similar images are being \\nproduced rapidly and widely, resulting in the homogeneity and similarity of images. Moreover, it \\nbrings new challenges to compression systems, which need to exploit the potential of deep features \\nand side information of images. However, traditional methods are incompetent for this issue. Soft \\ncompression is a novel data-driven image coding algorithm with superior performance. Compared \\nwith existing paradigms, it has distinctive characteristics: from hard to soft, from pixels to shapes, and \\nfrom fxed to random. Soft compression may hold promise for human-centric/data-centric intelligent \\nsystems, making them efcient and reliable and fnding potential in the metaverse and digital twins, \\netc. In this paper, we present a comprehensive and practical analysis of soft compression, revealing \\nthe functional role of each component in the system.\\nCompression and communication are key issues in the information age, which have been widely concerned and \\nexplored by academia and industry. Shannon’s information theory1\\n answers two fundamental questions for both \\ntasks2\\n: What is the ultimate data compression (answer: the entropy H), and what is the ultimate transmission rate \\nof communication (answer: the channel capacity C). Te frst answer theoretically gives a lower bound on data \\ncompression for independent and identically distributed (i.i.d.) random variables. Tis paper sets out to study \\ncompression, especially when the data is a general random process.\\nCompression is to reduce the cost of representing data as much as possible while ensuring the reconstruction \\nquality. It can greatly lessen the user’s demand and dependence on communication and storage resources. In a \\nnutshell, the goal of compression is to represent more with less. However, in the literature, theoretical and mod\\x02eling works for compression are mainly concentrated on streaming data. For data with more complex structures \\nsuch as images, it is still a challenging issue.\\nIn general, data compression includes lossy and lossless paradigms. Te former focuses on efciency, while \\nthe latter is more concerned with quality. As an important meta-carrier, image compression directly afects the \\nobjective efciency of the communication network and the subjective experience of clients. Image compression \\nmay play an important role in several felds, especially for metaverse and digital twins, such as virtual reality \\n(VR), augmented reality (AR), computer vision (CV), and the Internet of Everything (IoE). Figure\\xa01 shows the \\napplication of image compression in multiple scenarios. Moreover, data-driven image coding algorithms3\\n are \\nexpected to empower the Metaverse and 6G, becoming the backbone of next-generation intelligent technologies.\\nRecently, a novel data-driven and data-specifc image lossless compression algorithm called sof compres\\x02sion was proposed in4,5\\n. It has superior performance on the compression ratio, outperforming popular classical \\nstandards PNG and JPEG2000, as well as recent deep learning-based algorithms6\\n. Tis paper focuses on a com\\x02prehensive analysis of the image coding algorithm with sof compression. We will provide a detailed description \\nof the main properties of the image compression system in several scenarios.\\nOPEN\\n1\\nThe Department of Electronic Engineering, Tsinghua University, Beijing 100084, China. 2The Beijing National \\nResearch Center for Information Science and Technology, Tsinghua University, Beijing 100084, China. *email: \\nfpy@tsinghua.edu.cn\\n2\\nVol:.(1234567890)\\nScientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\\nwww.nature.com/scientificreports/\\nPreliminaries. In this paper, we mainly use several typical mutual information metrics and image quality \\nassessment metrics to evaluate the characteristics of the sof compression algorithm. In this regard, we frst give \\nsome defnitions and explain some basic concepts which are used throughout the paper.\\nMutual information. Mutual information is a measure of the amount of information that one random variable \\ncontains about the other2\\n. In general, the smaller the mutual information, the weaker the correlation between \\ntwo random variables from diferent event spaces. To date, mutual information has been efectively applied \\nin many felds, such as telecommunications7\\n and machine learning8,9. In this study, we focus on three notable \\nmutual information metrics, Shannon mutual information1\\n, α-mutual information10 and message importance \\nmeasure loss11.\\n(1) Shannon mutual information\\nShannon mutual information measures the reduction part in terms of the uncertainty for one random vari\\x02able due to the knowledge of the other2\\n. We use it to characterize the amount of information commonly shared \\nbetween the two variables. Te Shannon mutual information of two discrete random variables X and Y is defned \\nby\\nwhere p(x,\\xa0y) is the joint probability mass function of X and Y, and p(x) and p(y) are the marginal probability \\nmass functions of X and Y, respectively.\\n(2) α -Mutual information\\nRényi entropy and Rényi divergence12 evidence a long track record of usefulness in information theory and \\nits applications10. In the literature, there are several possible ways to generalize Shannon mutual information in \\na similar way, notably by Suguru Arimoto13, Imre Csiszár14 and Robin Sibson15. We use Sibson’s defnition for \\nthe measure. Te Rényi mutual information of order α, where α ≥ 0 and α = 1, is defned as\\nwhere p(y|x) is the conditional probability of Y given X.\\n(3) Message importance measure loss\\nMessage importance measure (MIM) was frst proposed in 201616. It is an important metric to describe the \\nmessage in the scenario of big data. Similar to the Shannon entropy and Renyi entropy, MIM is required to reveal \\n(1) I(X; Y) = \\x1f\\nx∈X\\n\\x1f\\ny∈Y\\np(x, y)log p(x, y)\\np(x)p(y)\\n,\\nIα(X; Y) = (2) α\\nα − 1 log\\x1f\\ny∈Y\\n\\x1e \\x1f\\nx∈X\\np(x)pα(y|x)\\n\\x1d 1\\nα\\n.\\nFigure\\xa01. Image compression plays an important role in several scenarios, such as virtual reality (VR), \\naugmented reality (AR), computer vision (CV), and the Internet of Tings (IoT).\\n3\\nVol.:(0123456789)\\nScientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\\nwww.nature.com/scientificreports/\\nthe uncertainty of a random process and some related statistical characteristics. Te following defnitions are \\nintended to introduce the message importance measure theory briefy.\\nDefnition 1 For a discrete random variable X, the exponential expression of message importance measure \\n(MIM) is given by\\nwhere the parameter ω is nonnegative and p(x)e{1−p(x)}\\n is viewed as the self-scoring value of event X = x.\\nDefnition 2 Te conditional message importance measure (CMIM) is given by\\nDefnition 3 Te message importance loss based on MIM and CMIM is given by\\nMIM loss has been used in several felds such as information compression, distribution estimation, anomaly \\ndetection, recommendation, and IoT11,17. Te parameter in MIM can be selected to amplify the weights of cer\\x02tain elements according to the user’s preference18. In Sect.\\xa0“Mutual information measure analysis”, we will use \\nShannon mutual information, α-mutual information, and MIM loss to measure the mutual information between \\ndiferent components of an image.\\nImage quality assessment. Te image quality assessment metric reveals the similarity between the distorted \\nimage and the original image. It measures the quality of an image from an observational and computational \\npoint of view. Te similarity of two images, P and O, is measured as\\nwhere f (·) is the image embedding function mapping an image to a point in Euclidean space. Te image embed\\x02ding function f (·) is the essential part for fnding the similarity between two images. Common image quality \\nassessment metrics include peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and image semantic \\nsimilarity. Tey are usually computed over all pixels in both images. We use them to measure the reconstruction \\nability of the Decoder in the compression algorithm.\\n(1) Peak signal-to-noise ratio (PSNR)\\nTe simplest and most widely used image quality assessment metric is the mean squared error (MSE). It is \\ncomputed by averaging the squared intensity diferences of the distorted and reference image. Let P and O denote \\na digital image with intensity levels in the range [0, D − 1], where D is a positive integer number that represents \\nthe pixel intensity level of images. Te row and column dimensions are M and N. Let P(i,\\xa0j) and O(i,\\xa0j) be the \\nintensity value on (i,\\xa0j) in P and O, respectively. MSE is defned as\\nPeak signal-to-noise ratio (PSNR) is an assessment metric for the ratio between the maximum possible power of \\na signal and the power of corrupting noise that afects the fdelity of its representation. It is defned as\\nIn general, the larger the PSNR, the better the quality of the reconstructed image.\\n(2) Structural similarity (SSIM)\\nObjective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of \\nerrors between a distorted image and a reference image using a variety of known properties of the human visual \\nsystem19. Te structural similarity index is a framework for quality assessment under the assumption that human \\nvisual perception is highly adapted for extracting structural information from a scene. It reveals the structural \\nsimilarity between the reconstructed image and the original image. For images P and O one can obtain\\n(3) L(ω, X) = \\x1f\\nx∈X\\np(x)eω{1−p(x)},\\n(4) L(ω, X|Y) = \\x1f\\ny∈Y\\np(y)\\n\\x1f\\nx∈X\\np(x|y)eω{1−p(x|y)}.\\n�ω(X�Y) = L(ω, X) − L(ω, X|Y). (5)\\nn(f (P), f (O)) = �f (P) − f (O)� (6) 2\\n2,\\nMSE = (7)\\n1\\nMN\\nM\\n\\x1f−1\\ni=0\\nN\\n\\x1f−1\\nj=0\\n[P(i, j) − O(i, j)]\\n2.\\nPSNR = 20 · log10 (8) \\x1f D − 1\\n√MSE \\x1e\\n.\\n(9) l(P, O) = 2µPµO + c1\\nµ2\\nP + µ2O + c1\\n(10) c(P, O) = 2σPO + c2\\nσ2\\nP + σ2O + c2\\n4\\nVol:.(1234567890)\\nScientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\\nwww.nature.com/scientificreports/\\nwhere µ, and σ2 represent the mean intensity and variance of an image, respectively. c1, c2 and c3 are some posi\\x02tive constants.\\nFinally, the three parts are combined to yield an overall similarity measure.\\nwhere α, β and γ are positive constants. SSIM has three main characteristics:\\n• Symmetry: S(P, O) = S(O, P)\\n• Boundedness: S(P, O) ≤ 1\\n• Unique maximum: S(P, O) = 1 if and only if P = O.\\nSSIM is between 0 and 1. Te larger the SSIM, the smaller the diference between the distorted image and the \\noriginal image, that is, the better the image quality. SSIM equals 1 when two images are exactly the same.\\n(3) Image semantic similarity\\nImage semantic similarity metric depends on the high-order image structure, which is usually context\\x02dependent20–22. DL-based image similarity metrics encode high invariance and capture image semantics. For \\nexample, one can measure the distance of two images in VGG feature space as the perceptual loss23. Richard et \\nal.24 introduced a perceptual similarity dataset and evaluated deep features across diferent architectures and \\ntasks. Te visual translation embedding (VTransE) network25 is an end-to-end and fully-convolutional architec\\x02ture that consists of an object detection module, a diferential feature extraction layer, and a novel visual transla\\x02tion embedding layer for classifcation. As a result, the model can place objects in a low-dimensional relation \\nspace and measure semantic similarity. Te relationship detection model in26 learns a visual and a semantic \\nmodule that maps features from the two modalities into a shared space, where matched pairs of features have to \\ndiscriminate against those unmatched, but also maintain close distances to semantically similar ones.\\nRelations to big data. Big data are becoming related to almost all aspects of human activity from just \\nrecording events to research, design, production, and digital services or product delivery, to the fnal consumer27. \\nIn a number of discussions, blog posts, and articles, big data are attributed to have such characteristics as Vol\\x02ume, Variety, Velocity, Value, and Veracity28. Volume refers to larger amounts of data being generated from a \\nrange of sources. Variety refers to using multiple kinds of data to analyze a situation or event. Te velocity of data \\nis increasing rapidly over time for both structured and unstructured data, and there’s a need for more frequent \\ndecision-making about that data. Value refers to the data of low-value density. Users are required to address the \\nneed to enrich raw and unprocessed data by extracting higher-level knowledge for exploitation across diferent \\nscenarios. Te Veracity dimension deals with data uncertainty due to various factors such as data inconsisten\\x02cies, incompleteness, and deliberate deception29.\\nWith the advancement of intelligent vision algorithms and devices, image reprocessing and secondary propa\\x02gation are becoming increasingly prevalent. A large number of similar images are being produced rapidly and \\nwidely, resulting in the homogeneity and similarity of images. In other words, as a data carrier, images also have \\nthe characteristics of big data. Tis brings new challenges to image compression systems, which need to exploit \\nthe potential of deep features and side information of images.\\nSof compression is a novel data-driven image coding algorithm with superior performance, exploiting sev\\x02eral characteristics of big data. Firstly, its codebook is not artifcially specifed but generated by training on the \\nimage database, which refects the ’Volume’ of big data. Secondly, sof compression can be adaptively updated \\nas the data changes, so it is suitable for diferent situations and keeps up with the ’Velocity’ of big data. Finally, \\nthe most important point is that sof compression algorithms can extract higher-level knowledge and features \\nto form a codebook, which takes advantage of big data’s ’Value’ characteristic.\\nPaper outline. Te main contributions of this work can be summarized as follows:\\n• We point out the relationship between sof compression algorithms and big data. Moreover, we discuss how \\nit exploits characteristics of Volume, Velocity, and Value of big data for image coding.\\n• We do a comprehensive and practical analysis of image coding algorithms with sof compression. In addition, \\nthe system was deployed in several scenarios, consisting of mutual information measure analysis, function \\nevaluation of each component, noise robustness, and data transmission.\\n• It lays the foundation and provides information for the deployment of the sof compression algorithm in real \\nscenarios. All these observations may have a bearing on general image coding methods and will be expected \\nto provide more insights into the future multi-media semantic communications and fnd promising applica\\x02tions in the metaverse and digital twins, etc.\\nWe introduces the sof compression strategy in Sect.\\xa0“Methods”. Te remaining part has been divided into \\nfour themed chapters, discussing the main properties of the sof compression algorithm in several scenarios. \\nTese analyses are more close to the ground truth since they are based on real datasets and experimental results. \\nSection “Mutual information measure analysis” analyzes the results of mutual information measure between two \\ncomponents of images. In Sect.\\xa0“Function evaluation of each component”, we lay out the experimental results \\ns(P, O) = (11) σPO + c3\\nσPσO + c3\\n,\\nS(P, O) = [l(P, O)] (12) α[c(P, O)]\\nβ [s(P, O)]γ ,', highlights=None, highlight_scores=None, summary=None), Result(url='https://export.arxiv.org/pdf/2304.01106v1.pdf', id='https://export.arxiv.org/pdf/2304.01106v1.pdf', title='Crossword: A Semantic Approach to Data Compression via Masking', score=0.38334229588508606, published_date='2023-04-03T16:04:06.000Z', author='Mingxiao Li,Rui Jin,Liyao Xiang,Kaiming Shen,Shuguang Cui', image=None, favicon=None, subpages=None, extras=None, text='Mingxiao Li mingxiaoli@link.cuhk.edu.cn \\n† \\nRui Jin ruijin@link.cuhk.edu.cn \\nLiyao Xiang xiangliyao08@sjtu.edu.cn \\nShanghai Jiao Tong University\\nChina\\n\\nKaiming Shen shenkaiming@cuhk.edu.cn \\nShuguang Cui shuguangcui@cuhk.edu.cn \\n\\nSchool of Science and Engineering\\nFNii\\nThe Chinese University of Hong Kong (Shenzhen)\\nChina\\n\\nCrossword: A Semantic Approach to Data Compression via Masking\\n3 Apr 2023Index Terms-Semantic source codingdata compressionword maskingcosine distanceTransformer\\nThe traditional methods for data compression are typically based on the symbol-level statistics, with the information source modeled as a long sequence of i.i.d. random variables or a stochastic process, thus establishing the fundamental limit as entropy for lossless compression and as mutual information for lossy compression. However, the source (including text, music, and speech) in the real world is often statistically ill-defined because of its close connection to human perception, and thus the model-driven approach can be quite suboptimal. This study places careful emphasis on English text and exploits its semantic aspect to enhance the compression efficiency further. The main idea stems from the puzzle crossword, observing that the hidden words can still be precisely reconstructed so long as some \"key\" letters are provided. The proposed masking-based strategy resembles the above game. In a nutshell, the encoder evaluates the semantic importance of each word according to the semantic loss and then masks the minor ones, while the decoder aims to recover the masked words from the semantic context by means of the Transformer. Our experiments show that the proposed semantic approach can achieve much higher compression efficiency than the traditional methods such as Huffman code and UTF-8 code, while preserving the meaning in the target text to a great extent.\\n\\nI. INTRODUCTION\\n\\nData compression, or source coding in a communication system, has been widely recognized as an artful trick that involves human perception, especially for sources like literature, music, speech etc. Indeed, Shannon asserted in his seminal paper [1] that \"these semantic aspects of communication are irrelevant to the engineering problem\" assuming that (i) the source can be modeled as a stochastic process or even a sequence of i.i.d. random variables; (ii) the source comprises infinitely many symbols. However, neither of the above assumptions may hold in practice. This work proposes a semantic approach to the English text compression, which improves upon the traditional modelbased approach by taking the meaning of words into account. The main idea is to mask those semantically less important words throughout the text, thus enhancing the compression efficiency while preserving the overall meaning of the text.\\n\\nThe proposed masking strategy for semantic data compression is inspired by the popular puzzle crossword-the goal of which is to fill the blank grids with letters and thus form words or phrases in accordance with the given grid layout and the fixed grids already filled with letters. It turns out that the above word puzzle has an analogy to data compression. The encoder can be thought of as the designer of the puzzle, who wishes to mask as many words as possible (i.e., maximize the portion of blank grids) to increase the difficulty of the puzzle. In the meanwhile, the decoder acts as the player and aims to recover all the words successfully. We seek the optimal word masking that attains an equilibrium satisfying both the puzzle designer and the player. Furthermore, as the main feature of this puzzle, words cross each other; it can be perceived that the letters lying in those intersection grids are often easier to guess. Likewise, with regard to the English text compression, some particular words can be identified as the \"intersection grid\" type if their removals do not cause much semantic loss, and thus can be masked without undermining the decoder\\'s capability to recover the text. Notice that the choice of such words typically depends on the context, e.g., the word \"affairs\" is very likely to be semantically minor if the sample text is taken from a political document owned by the European Parliament, as observed in our experiments. The cutting-edge artificial intelligence tools such as the Bidirectional Encoder Representation (BERT) [2], [3] and the Transformer [4] are the critical enabler for sensing the background knowledge, detecting the semantically minor words, and preserving the semantic information.\\n\\nThe efforts in semantic compression date back to [5] in the 1990s, wherein a simple idea of replacing every word with a shorter equivalent from the thesaurus is discussed. The authors of [5] further discuss how to mimic the genre (e.g., Hemingway\\'s) in a generative fashion. Another early attempt can be found in [6] with the attention focused on the Extensible Markup Language (XML). Its primary idea is to classify the XML data according to their importance, and then reduce the precision of those data that can tolerate higher loss. Moreover, [7] suggests defining the semantic entropy to be mutual information between the word model and syntactic message, and then uses that to measure the limit of semantic compression. Another line of studies [8], [9] use the connection between facts from the knowledge basis and hence reduce the redundancy in a semantic sense. The tool of sentence-BERT [3] has been applied extensively in the past few years for the word embedding purpose in a variety of semantic communication tasks ranging from source coding [10], [11] to joint source-channel coding [12], [13]. In contrast to [10] that aims at data compression on a per-word basis, our work adopts the sentence-BERT for data compression at a higher level that encompasses the overall context. In particular, with the revival of neural networks, the most recent trend in the area is towards deep learning-based methodology, such as [12] that considers resource allocation, [14] that considers the robust transmission against semantic noise, and [15] that considers multi-user semantic communications.\\n\\n\\nII. SYSTEM MODEL\\n\\nConsider an English text denoted by T that comprises a total of M sentences:\\nT = (S 1 , S 2 , . . . , S M ),(1)\\nwhere S i , i = 1, . . . , M , is the ith sentence. Assume that these sentences may vary in length. For the data compression purpose, T is converted into x in the set of finite-length bit strings {0, 1} * by the encoding function f (·) as\\nx = f (T ) ∈ {0, 1} * .(2)\\nConversely, the decoding function g(·) aims to recover the text T from the bit string x as\\nT = g(x).(3)\\nMoreover, denoting by S i the ith recovered sentence, we can split the decoded text into\\nT = ( S 1 , S 2 , . . . , S M ).(4)\\nLet δ(T, T ) ≥ 0 be the distortion cost it incurs for representing the ground truth T by the decoded text T . As a typical problem formulation of data compression, we seek the optimal encoderand-decoder pair (f, g) that minimizes the length of x, which is denoted by len(x), under the distortion constraint ǫ, i.e., minimize f (·), g(·)\\n\\nlen(x) (5a) subject to δ(T, T ) ≤ ǫ.\\n\\nNotice that T is not fixed a priori in the above problem, so we aim at a universal design of f (·) and g(·) that works universally for all possible texts. It remains to specify the distortion function δ(T, T ). In classical studies based on the classical rate-distortion theory, T and T are typically compared on a per-symbol basis; the symbol in English text is either a letter or a punctuation mark. Given the jth symbol ℓ j from T and the jth symboll j from T , the traditional method computes the symbol-level distortion d(ℓ j ,l j ), a common example of which is Hamming distortion:\\nd(ℓ j ,l j ) = 0 if ℓ j =l j , 1 otherwise.(6)\\nThen the overall text distortion δ(T, T ) amounts to the sum of the symbol distortions d(ℓ j ,l j ) for all j. In principle, the above strategy takes each symbol as the random variable unit and thus treats the text as a sequence of random variables. The resulting data compression algorithm can only take advantage of the language statistics at the symbol level. Rather, this work proposes to evaluate the distortion on the inter-word and inter-sentence context basis, thereby capturing the semantic aspect of the target text. Consider a pair of sentences S i and S i . First, convert them to two Euclidean vectors in R 384 by means of the sentence-BERT [3], i.e.,\\nµ i = BERT(S i ) andμ i = BERT( S i ).(7)\\nThen, in order to quantify the goodness of representing S i by S i , we use the cosine similarity between their respective sentence-BERT vectors\\nη(S i , S i ) = µ ⊤ iμ i µ i 2 · μ i 2(8)\\nand further consider the cosine distance\\nλ(S i , S i ) = 1 − η(S i , S i ).(9)\\nThus, from a semantic perspective, we suggest the following definition of δ(T, T ):\\nδ(T, T ) = M i=1 α i λ(S i , S i ),(10)\\nwhere the positive weight α i reflects the semantic importance of S i in terms of the overall text T . We set α i = N i out of the belief that the semantic information is proportional to the sentence length.\\n\\n\\nIII. MAIN RESULTS\\n\\nThe overall paradigm of the proposed semantic approach to data compression is visualized in Fig. 1. The encoding part and the decoding part are specified in the sequel, then followed by a discussion as to why the traditional entropy metric does not work for this paradigm.\\n\\n\\nA. Semantic Encoding\\n\\nThe proposed semantic encoder consists of two parts: masking followed by bit-string conversion. Assume that the sentence S i has N i words. With the nth word denoted by W in , the sentence S i can be expressed as\\nS i = (W i1 , W i2 , . . . , W iNi ).(11)\\nAlgorithm 1 Semantic Data Compression (Encoder Part) Input: English text T and masking ratio ρ ∈ [0, 1) 1: Initialization: Word list K = ∅ 2: for each sentence S i in T do 3:\\nCompute µ i = BERT(S i ) 4:\\nfor each word W in in S i do 5:\\nUpdate K = K ∪ {W in } 6: Computeμ i = BERT(S − in ) 7: Compute σ in = 1 − µ ⊤ iμ i /( µ i 2 · μ i 2 ) 8: end fo', highlights=None, highlight_scores=None, summary=None), Result(url='https://export.arxiv.org/pdf/2304.07342v2.pdf', id='https://export.arxiv.org/pdf/2304.07342v2.pdf', title='GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs', score=0.38286375999450684, published_date='2023-05-02T19:12:08.000Z', author='Boyuan Zhang,Jiannan Tian,Sheng Di,Xiangyao Yu,Martin Swany,Dingwen Tao,Franck Cappello', image=None, favicon=None, subpages=None, extras=None, text=\"Boyuan Zhang bozhan@iu.edu\\nJiannan Tian\\nSheng Di\\nXiaodong Yu\\nMartin Swany swany@indiana.edu\\nDingwen Tao ditao@iu.edu\\nFranck Cappello cappello@mcs.anl.gov\\nBoyuan Zhang\\nJiannan Tian\\nXiaodongSheng Di\\nYu\\nMartin Swany\\nDing-Wen Tao\\nFranck Cappello\\nIndiana University Bloomington\\nINUSA\\nArgonne National Laboratory Lemont\\nIndiana University Bloomington\\nIN, ILUSA, USA\\nArgonne National Laboratory Lemont\\nIndiana University Bloomington\\nIL, INUSA, USA\\nDepartment of Intelligent Systems Engineering, Luddy School of Informatics, Computing, and Engineering\\nArgonne National Laboratory Lemont\\nIndiana University Bloomington\\nIN, ILUSA, USA\\nIndiana University\\ngpuLZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\\n2023 International Conference on Supercomputing (ICS '23)\\nOrlando, FL, USA 2023; Orlando, FL, USA; New York, NY, USAACM12June 21-23, 2023. June 21-23, 202310.1145/3577193.3593706ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or affiliate of the United States government. As such, the United States government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for government purposes only. ACM ISBN 979-8-4007-0056-9/23/06... $15.00CCS CONCEPTS • Theory of computation → Massively parallel algorithmsData compressionKEYWORDS Lossless compressionLZSSGPUperformance * Corresponding author: Dingwen Tao,\\nToday's graphics processing unit (GPU) applications produce vast volumes of data, which are challenging to store and transfer efficiently. Thus, data compression is becoming a critical technique to mitigate the storage burden and communication cost. LZSS is the core algorithm in many widely used compressors, such as Deflate. However, existing GPU-based LZSS compressors suffer from low throughput due to the sequential nature of the LZSS algorithm. Moreover, many GPU applications produce multi-byte data (e.g., int16/int32 index, floating-point numbers), while the current LZSS compression only takes single-byte data as input. To this end, in this work, we propose gpuLZ, a highly efficient LZSS compression on modern GPUs for multi-byte data. The contribution of our work is fourfold: First, we perform an in-depth analysis of existing LZ compressors for GPUs and investigate their main issues. Then, we propose two main algorithm-level optimizations. Specifically, we (1) change prefix sum from one pass to two passes and fuse multiple kernels to reduce data movement between shared memory and global memory, and (2) optimize existing pattern-matching approach for multi-byte symbols to reduce computation complexity and explore longer repeated patterns. Third, we perform architectural performance optimizations, such as maximizing shared memory utilization by adapting data partitions to different GPU architectures. Finally, we evaluate gpuLZ on six datasets of various types with NVIDIA A100 and A4000 GPUs. Results show that gpuLZ achieves up to 272.1× speedup on A4000 and up to 1.4× higher compression ratio compared to state-of-the-art solutions.\\nINTRODUCTION\\nMany applications running on high-performance parallel and distributed systems generate large amounts of data, which leads to storage bottlenecks due to limited capacity. Meanwhile, interconnect technologies in distributed systems advance relatively more slowly than computing power, causing inter-node communication and I/O bottlenecks to become a severe issue [4]. This motivates the design of software solutions to increase the interconnect bandwidth, such as communication-avoiding linear algebra [2,14].\\nData compression is a popular solution to reduce communication and I/O overheads significantly. For example, due to the high data reduction capabilities, lossy compression has recently been extensively studied to alleviate I/O bottlenecks in large-scale distributed applications such as high-performance computing (HPC) simulations. Since the saved data is often used for post-analysis and visualization, errors introduced\", highlights=None, highlight_scores=None, summary=None), Result(url='https://link.springer.com/article/10.1007/s10791-024-09431-y?utm_medium=social&utm_source=twitter&utm_content=organic&utm_term=null&utm_campaign=CONR_DISCV_AWA1_GL_MPAS_Organ_Ed-TW', id='https://link.springer.com/article/10.1007/s10791-024-09431-y?utm_medium=social&utm_source=twitter&utm_content=organic&utm_term=null&utm_campaign=CONR_DISCV_AWA1_GL_MPAS_Organ_Ed-TW', title='Arithmetic N-gram: an efficient data compression technique', score=0.38913124799728394, published_date='2024-03-13T00:00:00.000Z', author='Hassan; Ali; Javed; Sadaf; Hussain; Sajjad; Ahmad; Rizwan; Qazi; Shams', image=None, favicon=None, subpages=None, extras=None, text=\"\\n Introduction With the significant development of digital technology, according to a statistics report [ 1], the number of Internet of Things (IoT) devices worldwide is forecast to triple from 9.7 billion in 2020 to more than 29 billion in 2030. IoT devices such as sensors, actuators, smart mobiles, appliances, or machines are programmed to transmit data over the internet for specific applications. Some IoT devices continuously monitor environmental factors and send critical information to the control center every second. Such IoT devices require high data rates, low latency, and space-efficient storage of this big data. Similarly, big data has gained attention in many fields. In the medical field, researchers use big data to find disease risk factors and symptoms to assist doctors in diagnosing diseases and medical disorders. Different companies use big data to enhance operations and boost customer service. In artificial intelligence (AI), learning models rely on big data to effectively learn future trends. However, the exponential growth of data from different sources, such as social media, smart sensors, IoT devices, mobile phones, medical devices, and online transactions, makes processing, storing, and transferring big data challenging. In this era of the digital world and limited resources, there is a need for efficient data compression techniques for the storage and transmission of data. Compression results in efficient usage of available storage and transmission bandwidth [ 2]. There are two main categories of data compression techniques: lossy compression and lossless compression [ 3, 4]. Lossy compression techniques compress the data with loss of certain information which can not be recovered. Effective compression can be achieved using lossy techniques but these techniques are only limited to the application where certain loss is acceptable such as video conferencing, stream media, multimedia, etc. On the other hand, lossless techniques compress the data while preserving all the information, which means original data can be reconstructed. Lossless techniques are used for medical image compression, text compression, etc., where losing details is unacceptable. In literature, most of the compression techniques are based on compressing individual letters of each character. Such compression techniques may not be effective in some cases where data is composed of repeated words or sequences of words. In such cases, compression based on words or a sequence of words can be more efficient and significantly reduce the space required for data storage applications such as digital libraries, archives, datasets for learning, sensor networks, etc. In addition, compression plays a significant role in compressing information in large language models (LLMs) like ChatGPT, PaLM2, and LLaMA (LLM Meta AI) for reducing storage and memory requirements. These models are mostly accessed over a cloud-based system. Transmitting large amounts of data over a network makes transmission slow and requires large bandwidth. The efficient compression can significantly enhance the transmission process and reduce transmission time to improve user experience. It can enable us to deploy these large models without massive hardware investment [ 5]. In this paper, we proposed the ArthNgram model, in which the N-gram language model coupled with arithmetic coding is used to compress data more efficiently for data storage applications. In this model, the N-gram language model is used to find the probability distribution and sequence of words. Then arithmetic encoding is used to compress data based on that distribution. The contributions of this work are as follows: \\n \\n Proposed the ArthNgram model that integrates the N-gram language model and arithmetic coding. \\n \\n \\n Evaluated proposed ArthNgram model for based on performance metrics: compression ratio, time, complexity, and speed. \\n \\n \\n Performed the comparative analysis of the proposed model with traditional compression techniques: Huffman coding, LZW coding, and arithmetic coding. \\n \\n The rest of this paper is structured as follows. In Sect. 2, traditional compression techniques are discussed. In Sect. 3, the proposed model is discussed. Section 4 provides the performance metrics. Results are discussed in Sect. 5. Finally, in Sect. 6, the proposed paper is concluded. Related work Since the use of the internet and the growing number of IoT devices, digital storage data systems, text file transfer, and embedded systems have significantly increased, text compression approaches have drawn more attention to managing and leveraging this data effectively. It is used in various applications, including data storage and transmission, digital archives, and cloud computing. It helps to reduce the storage space required for text data, improve transmission efficiency over slow or limited bandwidth networks, reduce the cost of storing and transmitting large amounts of text data, and allow quick and effortless transfer of large text files between devices. Text compression is a type of data compression in which text is compressed based on lossless compression techniques. The methods and technology for text compression are always being researched to be made better. Different lossless compression techniques have been used for text compression, such as Huffman, Shannon-Fano coding, arithmetic coding, run-length, Lempel-Ziv-Welch (LZW) coding, etc [ 6]. In [ 7], authors presented an alternative run-length approach based on Burrows-Wheeler Transform (BWT) and arithmetic coding for text compression. Their approach converts the large text file into smaller text files, with each containing the same number of characters. Each small file is transformed and compressed using BWT and run-length coding. Then, further compression is achieved using arithmetic coding. In [ 8], a text file in 8 different languages is compressed using compression techniques (LZW coding, Arithmetic Coding) to observe the effect of different languages on compression ratio. It is noted that the compression ratio varies with text language. Another lossless approach is proposed based on Huffman coding for text compression. This approach is also implemented on FPGA for validating the working of the proposed algorithm [ 9]. In [ 10], run-length coding and delta coding-based compression technique is proposed for compressing medical data with improved compression ratio. Authors in [ 11] proposed a technique based on Huffman coding in order to improve the compression ratio and to enhance image compression’s quality. In [ 12], a hybrid approach based on LZW and Huffman coding is proposed where data is first compressed by Huffman coding and then by LZW. It is observed that the proposed hybrid approach provides better compression than these two techniques individually. In [ 13], a dictionary-based technique is proposed for text compression based on the quaternary Huffman technique. Huffman with quaternary tree architecture is used instead of binary tree architecture to improve traversing time and compression ratio. In [ 14], n-gram dictionaries up to 5\\xa0gs are used to compress text in the Vietnamese language. The authors in [ 15] present a compression technique based on Shannon-Fano and Huffman algorithm to compress large files, more specifically in mobile devices. The dynamic text compression algorithm is proposed based on Huffman and LZW using n-gram dictionaries up to 2-grams [ 16]. In [ 17], a graph compression technique is proposed based on Huffman coding, pattern detecting, and matching principles to handle big data issues on resource-constrained IoT devices. A survey is conducted in [ 18] on deep learning-based models for compression. Authors in [ 19] provide a comparative study of Huffman and LZW techniques on 12 different test files of variable size. Compression plays a significant role in the practical deployment of LLMs by reducing their size and computational requirements. Authors in [ 20] proposed the activation-aware quantization method for LLMs compression. In [ 21], a structural pruning method is proposed for LLMs compression which compresses LLMs while maintaining their linguistics capabilities. Compression techniques This section gives a brief overview of different well-known compression techniques such as Huffman coding, LZW coding, arithmetic coding, etc. Huffman coding Huffman coding is a lossless compression technique that is based on the frequency of occurrence of symbols. It gives a variable length codeword to each symbol. It assigns shorter codewords to the symbols with a high frequency of occurrence and longer codewords to symbols with a low frequency of occurrence. It uses the prefix rule in order to ensure that the codeword assigned to any symbol is not the prefix of another symbol. The main steps of Huffman coding are following: \\n \\n Arrange the probabilities of symbols in descending order and consider them as the nodes \\n \\n \\n Repeat the below steps until all nodes form a single tree \\n \\n Select two nodes with the smallest probability \\n \\n \\n Merge them to form a new node whose probability is sum of these two nodes. \\n \\n \\n \\n \\n Traverse tree to get codewords for each symbol \\n \\n The time complexity of Huffman coding is O ( nlogn) where n represents the number of unique symbols in data to be compressed. LZW coding LZW is a dictionary-based lossless compression technique. It is used in many applications such as Win-zip, GIF, 7zip, etc. LZW replaces the character’s strings with a single code. The idea of this technique depends on reappearing patterns. It does not involve the analysis of incoming data. During the encoding process, it constructs the indexed dictionary in order to compress the data. LZW coding involves the following steps: \\n \\n Initialize by creating an indexed dictionary with the single-character entries \\n \\n \\n Read each character of the input data one at a time into a buffer. \\n \\n \\n Check whether the buffer exists in the dictionary or not \\n \\n \\n If it exists, add a subsequent character to the buffer and repeat step 3. \\n \\n \\n If it does not exist, add it to the dictionary as a new entry and get the buffer’s index in the dictionary as an output \\n \\n \\n Add the next subsequent character to the buffer and repeat from step 2 until the end of input data \\n \\n The time complexity of LZW coding is O ( n). This is because it constructs a dictionary while character-by-character processing of the input data containing n characters. Arithmetic coding Arithmetic encoding is a technique used for lossless data compression which encodes data to be transmitted into a string of 1\\xa0s and 0\\xa0s based on probabilities on the number line between 0 and 1 [ 22, 23]. It is used in various applications such as image compression, text compression, data transmission, etc. As the size of the data to be encoded increases, the interval on the real line between 0 and 1 becomes smaller and the corresponding binary code string of that interval grows. It involves the following steps: \\n \\n Map the input data over the range [0,\\xa01] based on frequency of occurrence of symbols. \\n \\n \\n Divide the current range into sub-ranges based on the probability of each symbol \\n \\n \\n Select the sub-range associated with the next symbol to be encoded and consider it as a new current range \\n \\n \\n Repeat step 2 and 3 until end of symbols \\n \\n The time complexity of this technique is O ( mn), where m is the total number of characters in data and n is the length of data. To summarize, we have presented three benchmark techniques that will be used for comparison of our proposed scheme. Proposed model This section presents the proposed model for text compression. The motivation for the proposed model comes from the fact that modern systems have repetition of data and it can be more advantageous to consider words or sequences of words rather than individual characters to get a better compression ratio. Figure\\xa0 1 shows the flow chart of the proposed model where we integrated the N-gram language model with arithmetic coding to compress text data effectively. In the proposed Model, first, we constructed the N-grams and calculated their probability distributions using the N-gram language model. Then, data is encoded using arithmetic coding based on these probability distributions. Fig. 1 Flow chart of proposed compression technique N-gram model and N-gram construction N-gram refers to a sequence of N words that is present in the text. The N-gram language model is used to predict the probabilities of N-grams in a given text. It is used in many applications such as spelling error detection and correction, text compression, language identification, etc. In order to generate N-grams from given text data considering respectively, the following steps are considered: \\n \\n split the text into tokens/smaller units with window size and added enough blank spaces before and after the token. \\n \\n \\n Scanned all the tokens to generate all possible N-grams for and set a counter to each N-gram to get the frequencies of each unigram, bigram, trigram, four-gram, and five-gram. \\n \\n \\n Calculated the probability of each N-gram based on their frequencies using the N-gram probability distribution model. \\n \\n We considered N-gram based compression model for which differ in their probability distributions. Fig. 2 ArthNgram Sequences Aithmetic N-gram (ArthNgram) ArthNgram model couples the N-gram language model with arithmetic coding to compress data more efficiently. In this work, the ArthNgram model is considered for five different N-gram cases: Arithmetic Unigram (Arth1\\xa0g), Arithmetic Bigram (Arth2\\xa0g), Arithmetic Trigram (Arth3\\xa0g), Arithmetic Four-gram (Arth4\\xa0g), and Arithmetic Five-gram (Arth5\\xa0g). Figure\\xa0 2 shows the sequence of words considered for five different N-gram cases. In an arithmetic unigram case, unigram-based arithmetic coding is used to compress the data. Each token in a unigram model is taken into account independently from every other token and the probability of each token depends on how frequently it appears in the data. In this case, the probability of a certain word sequence does not depend on its co-occurrence with the previous words. So, the probability of certain words sequence containing n unigrams can be calculated as follows: \\n (1)\\n In the case of other ArthNgram cases, the probability of a word in word sequence depends on its co-occurrence with the previous words in the data where . So, the generalized expression for the probability of certain word sequence containing n N-grams (bigrams, trigrams, four-grams, and five-grams ) can be expressed as follows: \\n (2)\\n Where , , and . N-gram dictionary N-gram dictionary contained all the N-grams (unigram, bigram, trigram, 4-gram, and 5-gram) of the data with their probability distributions. It is assumed to be available on both ends: compression and decompression. ArithNgram encoding The ArithNgram Encoding uses the N-gram probabilities obtained from the dictionary to compress the text data using an arithmetic encoding algorithm. The first step is to find the interval lie in [0,1) for data to be compressed. For that, it works as follows: \\n \\n We started with the current interval [l,h) as [0,1). \\n \\n \\n For each N-gram in the file, two steps are taken. \\n \\n For each possible N-gram, We split the current interval [l,h) into sub-intervals based on their probabilities. \\n \\n \\n The sub-interval corresponds to the next actual N-gram and is selected as a new interval. \\n \\n \\n \\n In the above step, we only compute the interval corresponding to the actual N-gram using their cumulative probabilities as: \\n (3)\\n \\n (4)\\n The new sub-interval is obtained as . To specify the end of the file, we assumed the file’s length to be known. This encoding process is summarized in Algorithm\\xa01, where seq represents the input text and vocab consists of N-grams, their probabilities, and and that represent the minimum and maximum value of the interval/ range of each n-gram. The vocab is assumed to be available on both encoding and decoding ends. After getting the interval lies on the number line, a binary code string is generated using a codebook encoding algorithm for given data based on that interval. Algorithm 1 Encoding Algorithm Codebook The codebook is used to convert the obtained interval from arithmetic encoding to binary string and vice versa. The working of the codebook is described in Algorithm\\xa02 and 3. A codebook Encoding Algorithm\\xa02 is used to encode the interval obtained from Algorithm\\xa01 into a binary string. In this algorithm, and represent the minimum and maximum value of the internal, and mp represents the midpoint. An empty codeword is defined to store the binary string. Every time when is less the mp, we append 0 in the codeword. The obtained codeword is considered as the binary string for given data. Algorithm 2 Codebook Encoding Algorithm ArithNgram decoding The ArithNgram decoding block reverses the encoding process. It converts an encoded binary string back into its original input sequence. The main steps of this block are as follows: \\n \\n Convert binary string to range using a codebook decoding algorithm \\n \\n \\n Repeat the following steps until the end of the encoded data is reached: \\n \\n Determine the n-gram symbol corresponding to the current range. \\n \\n \\n Update the range based on the probability of the n-gram symbol. \\n \\n \\n Read the next bits of the encoded data and update the value accordingly. \\n \\n \\n \\n \\n Output the original data. \\n \\n The whole decoding process is summarized in Algorithm\\xa03 and . In Algorithm\\xa03, codeword represents the binary string for encoded data, mp is the midpoint, and and represent the minimum and maximum value of the interval respectively. Every time when it reads a bit from codeword, it updates mp with until the last bit. When a 0 bit in the codeword is encountered, it updates with mp and when a 1 bit in the codeword is encountered, it updates with mp. The obtained value of and is considered as an interval that is used in the decoding algorithm to get back the original data. In decoding Algorithm\\xa04, represents the minimum value of interval obtained from the codebook decoding algorithm and represents the number of words in encoded data. Every time N-grams are stored in seq based on and is updated until the end of is reached. The obtained seq is the original data. Algorithm 3 Codebook Decoding Algorithm Algorithm 4 Decoding Algorithm The time complexity of the proposed model is the sum of the N-gram language model and arithmetic coding. The complexity of the N-gram model is O ( kn) where k represents N-gram size and n represents the length of input data. The overall complexity can be . Fig. 3 Number of sequences in different test file Results This section displays and explains simulation results. All the simulation work is done in MATLAB. For performance analysis, we considered the following performance metrics: number of bits, pre-processing time, compression/decompression time, compression ratio, and compression/decompression Speed. In order to validate the performance of the proposed model, we compared it with some other techniques including Huffman, Arithmetic, and LZW, based on performance metrics. We performed the result analysis for 5 different text files of size 8192, 13904, 32400, 40992, 98560 bits respectively. Figure\\xa0 3 show the frequency of occurrence of unigram, bigram, trigram, four-gram, and five-gram sequences of different test files respectively. Fig. 4 Preprocessing Time Compression time The compression time is the amount of time taken by the model to compress the data. Figure\\xa0 4 shows the preprocessing time taken by different schemes for different test files. Arth5\\xa0g takes the highest time among other schemes because the increased number of words in the N-gram sequence requires more processing time to meet the computational requirements. Figure\\xa0 5 and 6 show the amount of time taken by all test files for the compression and decompression process. It can be seen that Arth5garm takes the highest amounts of time for the compression and decompression process. Huffman takes the lowest amount of time in all test files. Fig. 5 Comparison of different compression schemes in terms of Compression Time Fig. 6 Comparison of different compression schemes in terms of Decompression Time Fig. 7 Comparison of different compression schemes in terms of Compression Ratio Fig. 8 Comparison of different compression schemes in terms of compressed File Size Compression ratio Compression ratio () is mostly used as a performance metric to evaluate the performance of the proposed algorithm. The value of depends on the text file to be compressed. Its value can vary for different algorithms. Even the same algorithm can have different values of for different text files. It is calculated by \\n (5)\\n Where is the size of the input file and is the size of the compressed file. Figure\\xa0 7 shows the compression ratio of test files. It can be observed that the compression ratio of the proposed Arth1\\xa0g, Arth2\\xa0g, arth3\\xa0g, arth4\\xa0g, and Arth5\\xa0g is more than benchmark techniques which infers the significant impact of N-grams on compression. As the higher value of the compression ratio denotes a more efficient compression strategy since it reduces the size of the compressed file in comparison to the uncompressed file. Space requirement Space requirement depends on compression ratio. The higher value of the compression ratio denotes a more efficient compression strategy since it reduces the size of the compressed file in comparison to the uncompressed file. This infers that the higher compression ratio results in less space required for storing data. Figure\\xa0 8 shows the size of test files after compression. The original size of 5 different test files is 8.192 Kb, 13.904 Kb, 32.400 Kb, 40.992 Kb, 98.560 Kb respectively. It can be seen that arth3\\xa0g is performing best than others. On average it reduces size as compared to traditional techniques which significantly reduces the space requirement for storage purposes. Compression speed Compression speed measures how quickly data can be compressed into a smaller size. It is measured in the number of megabits per second . It is calculated as \\n (6)\\n Similarly decompression speed measures how quickly compressed data can be converted into original data. It is measured in Mbits / s. It is calculated as \\n (7)\\n Figure\\xa0 9 and 10 show the compression and decompression speed of test files respectively. It can be noted that Arth1\\xa0g and convention techniques (Huffman, Arithmetic, LZW) have a higher compression speed. But Arth2\\xa0g, arth3\\xa0g, arth4\\xa0g, and Arth5\\xa0g have a lower compression speed because higher N-gram based compression results in increased complexity that leads to a lower compression speed as it requires more processing time to process larger N-grams. Fig. 9 Comparison of different compression schemes in terms of Compression Speed Fig. 10 Comparison of different compression schemes in terms of Decompression Speed From the above results, it is observed that Arth1\\xa0g has low performance in terms of both compression ratio and high compression speed compared to other ArthNgram techniques. Because in the case of Arth1\\xa0g, single words are considered that leads to a large pool of words to be compressed which results in a very low value of interval/range for the whole data to be compressed. That low value is converted into a longer binary string. However, in the case of other ArthNgram techniques, the sequence of words is considered. This sequence of words leads to a small pool of words to be compressed which results in a high value of range for the whole data to be compressed. That high value is converted into a smaller binary string compared to Arth1\\xa0g which makes ArthNgram more efficient in terms of compression ratio. Furthermore, in the case of ArthNgarm, N-grams calculation for the probability of word sequences requires more time than Arth1\\xa0g. It is important to note that compression speed and compression time are inversely related. Therefore, Arth1\\xa0g has a higher compression ratio than conventional techniques and a higher compression speed than other ArthNgram techniques. Thus, it is concluded that, in general, selecting a larger value of N for N-gram based compression will provide a better compression ratio but slower compression speed, whereas selecting a smaller value of N will provide a higher compression speed but low compression ratio. The ideal value of N depends on the particular application and its specifications. Conclusion Text compression involves the process of reducing the size of text data in order to increase processing speed, decrease transmission time, and conserve storage space. In this paper, we integrate the N-gram language model with arithmetic coding. The N-gram model captures the structure and patterns of the text data effectively and efficiently in order to model the probability distribution of data. Then text data is encoded using arithmetic coding based on this obtained probability distribution which results in a high compression ratio. The simulation results show the effectiveness of the proposed model. It is observed that this model significantly increased the compression ratio. \\n References Statista. iot-connected-devices-worldwide. https://www.statista.com/statistics/1183457/iot-connected-devices-worldwide/. Accessed 16 Feb 2023. Jayasankar U, Thirumal V, Ponnurangam D. A survey on data compression techniques: from the perspective of data quality, coding schemes, data type and applications. J King Saud Univ Comput Inform Sci. 2021;33(2):119–40. \\n Google Scholar \\xa0\\n Gupta A, Nigam S. A review on different types of lossless data compression techniques 2021. Hussain AJ, Al-Fayadh A, Radi N. Image compression techniques: a survey in lossless and lossy algorithms. Neurocomputing. 2018;300:44–69. Article \\xa0\\n \\n Google Scholar \\xa0\\n Zhu X, Li J, Liu Y, Ma C, Wang W. A survey on model compression for large language models. arXiv preprint arXiv:2308.07633 2023. Shanmugasundaram S, Lourdusamy R. A comparative study of text compression algorithms. Int J Wisdom Based Comput. 2011;1(3):68–76. \\n Google Scholar \\xa0\\n Rahman MA, Hamada M, Rahman MA. In 2021 IEEE 14th international symposium on embedded multicore/many-core systems-on-chip (MCSoC) 2021;287–291 Ignatoski M, Lerga J, Stanković L, Daković M. Comparison of entropy and dictionary based text compression in English, German, French, Italian, Czech, Hungarian, Finnish, and Croatian. Mathematics. 2020;8(7):1059. Article \\xa0\\n \\n Google Scholar \\xa0\\n Hameed M, Khmag A, Zaman F, Ramli AR. A new lossless method of Huffman coding for text data compression and decompression process with fpga implementation. J Eng Appl Sci. 2016;100(3):402–7. \\n Google Scholar \\xa0\\n Banerjee S, Singh GK. A new real-time lossless data compression algorithm for ECG and PPG signals. Biomed Signal Process Contr. 2023;79:104–27. Article \\xa0\\n \\n Google Scholar \\xa0\\n Otair M, Abualigah L, Qawaqzeh MK. Improved near-lossless technique using the huffman coding for enhancing the quality of image compression. Multimed Tools Appl. 2022;81(20):28509–29. Article \\xa0\\n \\n Google Scholar \\xa0\\n Shrividhiya G, Srujana KS, Kashyap SN, Gururaj C. In 2021 international conference on emerging smart computing and informatics (ESCI) 2021;234–237 Habib A, Islam MJ, Rahman MS. A dictionary-based text compression technique using quaternary code. Iran J Comput Sci. 2020;3(3):127–36. Article \\xa0\\n \\n Google Scholar \\xa0\\n Nguyen VH, Nguyen HT, Duong HN, Snasel V. n-gram-based text compression. Computational intelligence and neuroscience 2016;2016 Mantoro T, Ayu MA, Anggraini Y. In 2017 International Conference on Computing, Engineering, and Design (ICCED) (IEEE), 2017;1–5 Aburomman FTA. Dynamic with dictionary technique for arabic text compression. Int J Comput Appl. 2016;975:8887. \\n Google Scholar \\xa0\\n Chatterjee A, Shah RJ, Hasan KS. In: 2018 IEEE International conference on big data (big data) 2018;5137–5141 Gupta M, Agrawal P. Compression of deep learning models for text: a survey. ACM Trans Knowl Discov Data (TKDD). 2022;16(4):1–55. Article \\xa0\\n \\n Google Scholar \\xa0\\n Fauzan MN, Alif M, Prianto C. Comparison of Huffman algorithm and Lempel Ziv Welch algorithm in text file compression. IT J Res Develop. 2023;7(2):155–69. Article \\xa0\\n \\n Google Scholar \\xa0\\n Lin J, Tang J, Tang H, Yang S, Dang X, Han S. Awq: activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 2023 Ma X, Fang G, Wang X. Llm-pruner: on the structural pruning of large language models. arXiv preprint arXiv:2305.11627 2023. Langdon GG. An introduction to arithmetic coding. IBM J Res Develop. 1984;28(2):135–49. Article \\xa0\\n MathSciNet \\xa0\\n \\n Google Scholar \\xa0\\n Kotha HD, Tummanapally M, Upadhyay VK. In J Phys Conf Ser. 2019;1228:012007. Article \\xa0\\n \\n Google Scholar \\xa0\\n Download references ||||I|||| Your privacy, your choice\\n\\n We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.\\n\\n By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.\\n\\n See our privacy policy for more information on the use of your personal data.\\n\\n Manage preferences for further information and to change your choices.\\n\\n Accept all cookies\\nSkip to main content\\n Log in\\n Menu\\n Find a journal Publish with us Track your research\\n Search\\n Cart\\n\\n Search\\n\\n Search by keyword or author\\n Search\\n\\n Navigation\\n\\n * Find a journal\\n * Publish with us\\n * Track your research\\n 1. Home\\n 2. Discover Computing\\n 3. Article\\n\\n Arithmetic N-gram: an efficient data compression technique\\n\\n * Open access\\n * Published: 13 March 2024\\n * Volume 27 , article number 1 , ( 2024 )\\n * Cite this article\\n Download PDF\\n\\n You have full access to this open access article\\n\\n Discover Computing Aims and scope Submit manuscript\\n Arithmetic N-gram: an efficient data compression technique\\n Download PDF\\n * Ali Hassan1,\\n * Sadaf Javed1,\\n * Sajjad Hussain1,\\n * Rizwan Ahmad1 &\\n * …\\n * Shams Qazi1\\n Show authors\\n\\n * 135 Accesses\\n\\n * 1 Altmetric\\n\\n * Explore all metrics\\n\\n Abstract\\n\\n Due to the increase in the growth of data in this era of the digital world and limited resources, there is a need for more efficient data compression techniques for storing and transmitting data. Data compression can significantly reduce the amount of storage space and transmission time to store and transmit given data. More specifically, text compression has got more attention for effectively managing and processing data due to the increased use of the internet, digital devices, data transfer, etc. Over the years, various algorithms have been used for text compression such as Huffman coding, Lempel-Ziv-Welch (LZW) coding, arithmetic coding, etc. However, these methods have a limited compression ratio specifically for data storage applications where a considerable amount of data must be compressed to use storage resources efficiently. They consider individual characters to compress data. It can be more advantageous to consider words or sequences of words rather than individual characters to get a better compression ratio. Compressing individual characters results in a sizeable compressed representation due to their less repetition and structure in the data. In this paper, we proposed the ArthNgram model, in which the N-gram language model coupled with arithmetic coding is used to compress data more efficiently for data storage applications. The performance of the proposed model is evaluated based on compression ratio and compression speed. Results show that the proposed model performs better than traditional techniques.\\n\\n Similar content being viewed by others\\n\\n An Efficient Compression Scheme for Natural Language Text by Hashing\\n\\n Article 04 June 2022\\n\\n Md. Ashiq Mahmood & K. M. Azharul Hasan\\n\\n Trigram-Based Vietnamese Text Compression\\n\\n Chapter © 2016\\n\\n Multi-Stream Word-Based Compression Algorithm for Compressed Text Search\\n\\n Article 12 June 2018\\n\\n Emir Öztürk, Altan Mesut & Banu Diri\\n\\n Use our pre-submission checklist\\n\\n Avoid common mistakes on your manuscript.\\n\\n 1 Introduction\\n\\n With the significant development of digital technology, according to a statistics report [1], the number of Internet of Things (IoT) devices worldwide is forecast to triple from 9.7 billion in 2020 to more than 29 billion in 2030. IoT devices such as sensors, actuators, smart mobiles, appliances, or machines are programmed to transmit data over the internet for specific applications. Some IoT devices continuously monitor environmental factors and send critical information to the control center every second. Such IoT devices require high data rates, low latency, and space-efficient storage of this big data. Similarly, big data has gained attention in many fields. In the medical field, researchers use big data to find disease risk factors and symptoms to assist doctors in diagnosing diseases and medical disorders. Different companies use big data to enhance operations and boost customer service. In artificial intelligence (AI), learning models rely on big data to effectively learn future trends. However, the exponential growth of data from different sources, such as social media, smart sensors, IoT devices, mobile phones, medical devices, and online transactions, makes processing, storing, and transferring big data challenging.\\n\\n In this era of the digital world and limited resources, there is a need for efficient data compression techniques for the storage and transmission of data. Compression results in efficient usage of available storage and transmission bandwidth [2]. There are two main categories of data compression techniques: lossy compression and lossless compression [3, 4]. Lossy compression techniques compress the data with loss of certain information which can not be recovered. Effective compression can be achieved using lossy techniques but these techniques are only limited to the application where certain loss is acceptable such as video conferencing, stream media, multimedia, etc. On the other hand, lossless techniques compress the data while preserving all the information, which means original data can be reconstructed. Lossless techniques are used for medical image compression, text compression, etc., where losing details is unacceptable.\\n\\n In literature, most of the compression techniques are based on compressing individual letters of each character. Such compression techniques may not be effective in some cases where data is composed of repeated words or sequences of words. In such cases, compression based on words or a sequence of words can be more efficient and significantly reduce the space required for data storage applications such as digital libraries, archives, datasets for learning, sensor networks, etc. In addition, compression plays a significant role in compressing information in large language models (LLMs) like ChatGPT, PaLM2, and LLaMA (LLM Meta AI) for reducing storage and memory requirements. These models are mostly accessed over a cloud-based system. Transmitting large amounts of data over a network makes transmission slow and requires large bandwidth. The efficient compression can significantly enhance the transmission process and reduce transmission time to improve user experience. It can enable us to deploy these large models without massive hardware investment [5]. In this paper, we proposed the ArthNgram model, in which the N-gram language model coupled with arithmetic coding is used to compress data more efficiently for data storage applications. In this model, the N-gram language model is used to find the probability distribution and sequence of words. Then arithmetic encoding is used to compress data based on that distribution. The contributions of this work are as follows:\\n\\n * Proposed the ArthNgram model that integrates the N-gram language model and arithmetic coding.\\n\\n * Evaluated proposed ArthNgram model for \\\\(N=1, 2, 3, 4, 5\\\\) based on performance metrics: compression ratio, time, complexity, and speed.\\n\\n * Performed the comparative analysis of the proposed model with traditional compression techniques: Huffman coding, LZW coding, and arithmetic coding.\\n\\n The rest of this paper is structured as follows. In Sect. 2, traditional compression techniques are discussed. In Sect. 3, the proposed model is discussed. Section 4 provides the performance metrics. Results are discussed in Sect. 5. Finally, in Sect. 6, the proposed paper is concluded.\\n\\n 2 Related work\\n\\n Since the use of the internet and the growing number of IoT devices, digital storage data systems, text file transfer, and embedded systems have significantly increased, text compression approaches have drawn more attention to managing and leveraging this data effectively. It is used in various applications, including data storage and transmission, digital archives, and cloud computing. It helps to reduce the storage space required for text data, improve transmission efficiency over slow or limited bandwidth networks, reduce the cost of storing and transmitting large amounts of text data, and allow quick and effortless transfer of large text files between devices. Text compression is a type of data compression in which text is compressed based on lossless compression techniques. The methods and technology for text compression are always being researched to be made better. Different lossless compression techniques have been used for text compression, such as Huffman, Shannon-Fano coding, arithmetic coding, run-length, Lempel-Ziv-Welch (LZW) coding, etc [6]. In [7], authors presented an alternative run-length approach based on Burrows-Wheeler Transform (BWT) and arithmetic coding for text compression. Their approach converts the large text file into smaller text files, with each containing the same number of characters. Each small file is transformed and compressed using BWT and run-length coding. Then, further compression is achieved using arithmetic coding. In [8], a text file in 8 different languages is compressed using compression techniques (LZW coding, Arithmetic Coding) to observe the effect of different languages on compression ratio. It is noted that the compression ratio varies with text language. Another lossless approach is proposed based on Huffman coding for text compression. This approach is also implemented on FPGA for validating the working of the proposed algorithm [9]. In [10], run-length coding and delta coding-based compression technique is proposed for compressing medical data with improved compression ratio. Authors in [11] proposed a technique based on Huffman coding in order to improve the compression ratio and to enhance image compression’s quality. In [12], a hybrid approach based on LZW and Huffman coding is proposed where data is first compressed by Huffman coding and then by LZW. It is observed that the proposed hybrid approach provides better compression than these two techniques individually. In [13], a dictionary-based technique is proposed for text compression based on the quaternary Huffman technique. Huffman with quaternary tree architecture is used instead of binary tree architecture to improve traversing time and compression ratio. In [14], n-gram dictionaries up to 5 gs are used to compress text in the Vietnamese language. The authors in [15] present a compression technique based on Shannon-Fano and Huffman algorithm to compress large files, more specifically in mobile devices. The dynamic text compression algorithm is proposed based on Huffman and LZW using n-gram dictionaries up to 2-grams [16]. In [17], a graph compression technique is proposed based on Huffman coding, pattern detecting, and matching principles to handle big data issues on resource-constrained IoT devices. A survey is conducted in [18] on deep learning-based models for compression. Authors in [19] provide a comparative study of Huffman and LZW techniques on 12 different test files of variable size. Compression plays a significant role in the practical deployment of LLMs by reducing their size and computational requirements. Authors in [20] proposed the activation-aware quantization method for LLMs compression. In [21], a structural pruning method is proposed for LLMs compression which compresses LLMs while maintaining their linguistics capabilities.\\n\\n 3 Compression techniques\\n\\n This section gives a brief overview of different well-known compression techniques such as Huffman coding, LZW coding, arithmetic coding, etc.\\n\\n 3.1 Huffman coding\\n\\n Huffman coding is a lossless compression technique that is based on the frequency of occurrence of symbols. It gives a variable length codeword to each symbol. It assigns shorter codewords to the symbols with a high frequency of occurrence and longer codewords to symbols with a low frequency of occurrence. It uses the prefix rule in order to ensure that the codeword assigned to any symbol is not the prefix of another symbol. The main steps of Huffman coding are following:\\n\\n 1. 1.\\n\\n Arrange the probabilities of symbols in descending order and consider them as the nodes\\n\\n 2. 2.\\n\\n Repeat the below steps until all nodes form a single tree\\n\\n 1. (a)\\n\\n Select two nodes with the smallest probability\\n\\n 2. (b)\\n\\n Merge them to form a new node whose probability is sum of these two nodes.\\n\\n 3. 3.\\n\\n Traverse tree to get codewords for each symbol\\n\\n The time complexity of Huffman coding is O(nlogn) where n represents the number of unique symbols in data to be compressed.\\n\\n 3.2 LZW coding\\n\\n LZW is a dictionary-based lossless compression technique. It is used in many applications such as Win-zip, GIF, 7zip, etc. LZW replaces the character’s strings with a single code. The idea of this technique depends on reappearing patterns. It does not involve the analysis of incoming data. During the encoding process, it constructs the indexed dictionary in order to compress the data. LZW coding involves the following steps:\\n\\n 1. 1.\\n\\n Initialize by creating an indexed dictionary with the single-character entries\\n\\n 2. 2.\\n\\n Read each character of the input data one at a time into a buffer.\\n\\n 3. 3.\\n\\n Check whether the buffer exists in the dictionary or not\\n\\n 4. 4.\\n\\n If it exists, add a subsequent character to the buffer and repeat step 3.\\n\\n 5. 5.\\n\\n If it does not exist, add it to the dictionary as a new entry and get the buffer’s index in the dictionary as an output\\n\\n 6. 6.\\n\\n Add the next subsequent character to the buffer and repeat from step 2 until the end of input data\\n\\n The time complexity of LZW coding is O(n). This is because it constructs a dictionary while character-by-character processing of the input data containing n characters.\\n\\n 3.3 Arithmetic coding\\n\\n Arithmetic encoding is a technique used for lossless data compression which encodes data to be transmitted into a string of 1 s and 0 s based on probabilities on the number line between 0 and 1 [22, 23]. It is used in various applications such as image compression, text compression, data transmission, etc. As the size of the data to be encoded increases, the interval on the real line between 0 and 1 becomes smaller and the corresponding binary code string of that interval grows. It involves the following steps:\\n\\n 1. 1.\\n\\n Map the input data over the range [0, 1] based on frequency of occurrence of symbols.\\n\\n 2. 2.\\n\\n Divide the current range into sub-ranges based on the probability of each symbol\\n\\n 3. 3.\\n\\n Select the sub-range associated with the next symbol to be encoded and consider it as a new current range\\n\\n 4. 4.\\n\\n Repeat step 2 and 3 until end of symbols\\n\\n The time complexity of this technique is O(mn), where m is the total number of characters in data and n is the length of data.\\n\\n To summarize, we have presented three benchmark techniques that will be used for comparison of our proposed scheme.\\n\\n 4 Proposed model\\n\\n This section presents the proposed model for text compression. The motivation for the proposed model comes from the fact that modern systems have repetition of data and it can be more advantageous to consider words or sequences of words rather than individual characters to get a better compression ratio. Figure 1 shows the flow chart of the proposed model where we integrated the N-gram language model with arithmetic coding to compress text data effectively. In the proposed Model, first, we constructed the N-grams and calculated their probability distributions using the N-gram language model. Then, data is encoded using arithmetic coding based on these probability distributions.\\n\\n Fig. 1\\n\\n Flow chart of proposed compression technique\\n\\n Full size image\\n\\n 4.1 N-gram model and N-gram construction\\n\\n N-gram refers to a sequence of N words that is present in the text. The N-gram language model is used to predict the probabilities of N-grams in a given text. It is used in many applications such as spelling error detection and correction, text compression, language identification, etc. In order to generate N-grams from given text data considering \\\\((N=1,2,3,4,5)\\\\) respectively, the following steps are considered:\\n\\n * split the text into tokens/smaller units with window size and added enough blank spaces before and after the token.\\n\\n * Scanned all the tokens to generate all possible N-grams for \\\\((N= 1, 2, 3,\\\\dots 5)\\\\) and set a counter to each N-gram to get the frequencies of each unigram, bigram, trigram, four-gram, and five-gram.\\n\\n * Calculated the probability of each N-gram based on their frequencies using the N-gram probability distribution model.\\n\\n We considered N-gram based compression model for \\\\((N=1,2,3,4,5)\\\\) which differ in their probability distributions.\\n\\n Fig. 2\\n\\n ArthNgram Sequences\\n\\n Full size image\\n\\n 4.2 Aithmetic N-gram (ArthNgram)\\n\\n ArthNgram model couples the N-gram language model with arithmetic coding to compress data more efficiently. In this work, the ArthNgram model is considered for five different N-gram cases: Arithmetic Unigram (Arth1 g), Arithmetic Bigram (Arth2 g), Arithmetic Trigram (Arth3 g), Arithmetic Four-gram (Arth4 g), and Arithmetic Five-gram (Arth5 g). Figure 2 shows the sequence of words considered for five different N-gram cases. In an arithmetic unigram case, unigram-based arithmetic coding is used to compress the data. Each token in a unigram model is taken into account independently from every other token and the probability of each token depends on how frequently it appears in the data. In this case, the probability of a certain word sequence does not depend on its co-occurrence with the previous words. So, the probability of certain words sequence containing n unigrams can be calculated as follows:\\n\\n $$\\\\begin{aligned} \\\\begin{aligned} P_{r}(w_{1},w_{2},...,w_{n}) \\\\approx \\\\prod _{k=1}^{n} P_{r}(w_{k}) \\\\end{aligned} \\\\end{aligned}$$\\n (1)\\n\\n In the case of other ArthNgram cases, the probability of a word in word sequence depends on its co-occurrence with the \\\\((N-1)\\\\) previous words in the data where \\\\(N=1,2,3,4,5\\\\) . So, the generalized expression for the probability of certain word sequence containing n N-grams (bigrams, trigrams, four-grams, and five-grams ) can be expressed as follows:\\n\\n $$\\\\begin{aligned}{} & {} P_{r}(w_{1},w_{2},...,w_{n}) \\\\approx \\\\prod _{i}^{K-1} P_{r}(w_{i}\\\\mid w_{i-1} ) \\\\nonumber \\\\\\\\{} & {} \\\\prod _{k=m}^{n} P_{r}(w_{k}\\\\mid w_{k-1, k-2,...,k-i}) \\\\end{aligned}$$\\n (2)\\n\\n Where \\\\(i=1,2,.., (k-1)\\\\) , \\\\(m=2,3,4,5\\\\) , and \\\\(w_{0}=1\\\\) .\\n\\n 4.3 N-gram dictionary\\n\\n N-gram dictionary contained all the N-grams (unigram, bigram, trigram, 4-gram, and 5-gram) of the data with their probability distributions. It is assumed to be available on both ends: compression and decompression.\\n\\n 4.4 ArithNgram encoding\\n\\n The ArithNgram Encoding uses the N-gram probabilities obtained from the dictionary to compress the text data using an arithmetic encoding algorithm. The first step is to find the interval lie in [0,1) for data to be compressed. For that, it works as follows:\\n\\n 1. 1.\\n\\n We started with the current interval [l,h) as [0,1).\\n\\n 2. 2.\\n\\n For each N-gram in the file, two steps are taken.\\n\\n 1. (a)\\n\\n For each possible N-gram, We split the current interval [l,h) into sub-intervals based on their probabilities.\\n\\n 2. (b)\\n\\n The sub-interval corresponds to the next actual N-gram and is selected as a new interval.\\n\\n In the above step, we only compute the interval corresponding to the actual N-gram \\\\(g_{i}\\\\) using their cumulative probabilities as:\\n\\n $$\\\\begin{aligned} \\\\begin{aligned} P_{cur} = \\\\sum _{k=1}^{i-1} p_{k}(g) \\\\end{aligned} \\\\end{aligned}$$\\n (3)\\n $$\\\\begin{aligned} \\\\begin{aligned} P_{next} = P_{cur} + \\\\sum _{k=1}^{i-1} p_{k}(g) \\\\end{aligned} \\\\end{aligned}$$\\n (4)\\n\\n The new sub-interval is obtained as \\\\((l + P_{cur}(h-l), l + P_{next}(h-l))\\\\) . To specify the end of the file, we assumed the file’s length to be known. This encoding process is summarized in Algorithm 1, where seq represents the input text and vocab consists of N-grams, their probabilities, and \\\\(min_{r}\\\\) and \\\\(max_{r}\\\\) that represent the minimum and maximum value of the interval/ range of each n-gram. The vocab is assumed to be available on both encoding and decoding ends.\\n\\n After getting the interval lies on the number line, a binary code string is generated using a codebook encoding algorithm for given data based on that interval.\\n\\n Algorithm 1\\n\\n Encoding Algorithm\\n\\n Full size image\\n\\n 4.5 Codebook\\n\\n The codebook is used to convert the obtained interval from arithmetic encoding to binary string and vice versa. The working of the codebook is described in Algorithm 2 and 3. A codebook Encoding Algorithm 2 is used to encode the interval obtained from Algorithm 1 into a binary string. In this algorithm, \\\\(min_{r}\\\\) and \\\\(max_{r}\\\\) represent the minimum and maximum value of the internal, and mp represents the midpoint. An empty codeword is defined to store the binary string. Every time when \\\\(min_{r}\\\\) is less the mp, we append 0 in the codeword. The obtained codeword is considered as the binary string for given data.\\n\\n Algorithm 2\\n\\n Codebook Encoding Algorithm\\n\\n Full size image\\n\\n 4.6 ArithNgram decoding\\n\\n The ArithNgram decoding block reverses the encoding process. It converts an encoded binary string back into its original input sequence. The main steps of this block are as follows:\\n\\n 1. 1.\\n\\n Convert binary string to range using a codebook decoding algorithm\\n\\n 2. 2.\\n\\n Repeat the following steps until the end of the encoded data is reached:\\n\\n 1. (a)\\n\\n Determine the n-gram symbol corresponding to the current range.\\n\\n 2. (b)\\n\\n Update the range based on the probability of the n-gram symbol.\\n\\n 3. (c)\\n\\n Read the next bits of the encoded data and update the value accordingly.\\n\\n 3. 3.\\n\\n Output the original data.\\n\\n The whole decoding process is summarized in Algorithm 3 and . In Algorithm 3, codeword represents the binary string for encoded data, mp is the midpoint, and \\\\(min_{r}\\\\) and \\\\(max_{r}\\\\) represent the minimum and maximum value of the interval respectively. Every time when it reads a bit from codeword, it updates mp with \\\\(\\\\frac{mid}{2}+min_{r}\\\\) until the last bit. When a 0 bit in the codeword is encountered, it updates \\\\(max_{r}\\\\) with mp and when a 1 bit in the codeword is encountered, it updates \\\\(min_{r}\\\\) with mp. The obtained value of \\\\(min_{r}\\\\) and \\\\(max_{r}\\\\) is considered as an interval that is used in the decoding algorithm to get back the original data. In decoding Algorithm 4, \\\\(min_{r}\\\\) represents the minimum value of interval obtained from the codebook decoding algorithm and \\\\(N_{w}\\\\) represents the number of words in encoded data. Every time N-grams are stored in seq based on \\\\(min_{r}\\\\) and \\\\(min_{r}\\\\) is updated until the end of \\\\(N_{w}\\\\) is reached. The obtained seq is the original data.\\n\\n Algorithm 3\\n\\n Codebook Decoding Algorithm\\n\\n Full size image\\n\\n Algorithm 4\\n\\n Decoding Algorithm\\n\\n Full size image\\n\\n The time complexity of the proposed model is the sum of the N-gram language model and arithmetic coding. The complexity of the N-gram model is O(kn) where k represents N-gram size and n represents the length of input data. The overall complexity can be \\\\(O(kn) + O(mn)\\\\) .\\n\\n Fig. 3\\n\\n Number of sequences in different test file\\n\\n Full size image\\n\\n 5 Results\\n\\n This section displays and explains simulation results. All the simulation work is done in MATLAB. For performance analysis, we considered the following performance metrics: number of bits, pre-processing time, compression/decompression time, compression ratio, and compression/decompression Speed. In order to validate the performance of the proposed model, we compared it with some other techniques including Huffman, Arithmetic, and LZW, based on performance metrics. We performed the result analysis for 5 different text files of size 8192, 13904, 32400, 40992, 98560 bits respectively. Figure 3 show the frequency of occurrence of unigram, bigram, trigram, four-gram, and five-gram sequences of different test files respectively.\\n\\n Fig. 4\\n\\n Preprocessing Time\\n\\n Full size image\\n\\n 5.1 Compression time\\n\\n The compression time is the amount of time taken by the model to compress the data. Figure 4 shows the preprocessing time taken by different schemes for different test files. Arth5 g takes the highest time among other schemes because the increased number of words in the N-gram sequence requires more processing time to meet the computational requirements. Figure 5 and 6 show the amount of time taken by all test files for the compression and decompression process. It can be seen that Arth5garm takes the highest amounts of time for the compression and decompression process. Huffman takes the lowest amount of time in all test files.\\n\\n Fig. 5\\n\\n Comparison of different compression schemes in terms of Compression Time\\n\\n Full size image\\n\\n Fig. 6\\n\\n Comparison of different compression schemes in terms of Decompression Time\\n\\n Full size image\\n\\n Fig. 7\\n\\n Comparison of different compression schemes in terms of Compression Ratio\\n\\n Full size image\\n\\n Fig. 8\\n\\n Comparison of different compression schemes in terms of compressed File Size\\n\\n Full size image\\n\\n 5.2 Compression ratio\\n\\n Compression ratio ( \\\\(C_{r}\\\\) ) is mostly used as a performance metric to evaluate the performance of the proposed algorithm. The value of \\\\(C_{r}\\\\) depends on the text file to be compressed. Its value can vary for different algorithms. Even the same algorithm can have different values of \\\\(C_{r}\\\\) for different text files. It is calculated by\\n\\n $$\\\\begin{aligned} \\\\begin{aligned} C_{r} = \\\\frac{\\\\text {Uncompressed file size}}{ \\\\text { Compressed file size}} \\\\end{aligned} \\\\end{aligned}$$\\n (5)\\n\\n Where \\\\(\\\\text {Uncompressed file size}\\\\) is the size of the input file and \\\\(\\\\text { Compressed file size}\\\\) is the size of the compressed file.\\n\\n Figure 7 shows the compression ratio of test files. It can be observed that the compression ratio of the proposed Arth1 g, Arth2 g, arth3 g, arth4 g, and Arth5 g is more than benchmark techniques which infers the significant impact of N-grams on compression. As the higher value of the compression ratio denotes a more efficient compression strategy since it reduces the size of the compressed file in comparison to the uncompressed file.\\n\\n 5.3 Space requirement\\n\\n Space requirement depends on compression ratio. The higher value of the compression ratio denotes a more efficient compression strategy since it reduces the size of the compressed file in comparison to the uncompressed file. This infers that the higher compression ratio results in less space required for storing data. Figure 8 shows the size of test files after compression. The original size of 5 different test files is 8.192Kb, 13.904Kb, 32.400Kb, 40.992Kb, 98.560Kb respectively. It can be seen that arth3 g is performing best than others. On average it reduces \\\\(65\\\\%\\\\) size as compared to traditional techniques which significantly reduces the space requirement for storage purposes.\\n\\n 5.4 Compression speed\\n\\n Compression speed \\\\((s_{com})\\\\) measures how quickly data can be compressed into a smaller size. It is measured in the number of megabits per second \\\\((\\\\text {Mbits/sec})\\\\) . It is calculated as\\n\\n $$\\\\begin{aligned} \\\\begin{aligned} s_{com} = \\\\frac{\\\\text {Uncompressed file size}}{ \\\\text{ compression } \\\\text{ time }} \\\\end{aligned} \\\\end{aligned}$$\\n (6)\\n\\n Similarly decompression speed \\\\((s_{decom})\\\\) measures how quickly compressed data can be converted into original data. It is measured in Mbits/s. It is calculated as\\n\\n $$\\\\begin{aligned} \\\\begin{aligned} s_{decom} = \\\\frac{\\\\text {Uncompressed file size}}{\\\\text { decompression time}} \\\\end{aligned} \\\\end{aligned}$$\\n (7)\\n\\n Figure 9 and 10 show the compression and decompression speed of test files respectively. It can be noted that Arth1 g and convention techniques (Huffman, Arithmetic, LZW) have a higher compression speed. But Arth2 g, arth3 g, arth4 g, and Arth5 g have a lower compression speed because higher N-gram based compression results in increased complexity that leads to a lower compression speed as it requires more processing time to process larger N-grams.\\n\\n Fig. 9\\n\\n Comparison of different compression schemes in terms of Compression Speed\\n\\n Full size image\\n\\n Fig. 10\\n\\n Comparison of different compression schemes in terms of Decompression Speed\\n\\n Full size image\\n\\n From the above results, it is observed that Arth1 g has low performance in terms of both compression ratio and high compression speed compared to other ArthNgram techniques. Because in the case of Arth1 g, single words are considered that leads to a large pool of words to be compressed which results in a very low value of interval/range for the whole data to be compressed. That low value is converted into a longer binary string. However, in the case of other ArthNgram techniques, the sequence of words is considered. This sequence of words leads to a small pool of words to be compressed which results in a high value of range for the whole data to be compressed. That high value is converted into a smaller binary string compared to Arth1 g which makes ArthNgram more efficient in terms of compression ratio. Furthermore, in the case of ArthNgarm, N-grams \\\\((N=2,3,4,5)\\\\) calculation for the probability of word sequences requires more time than Arth1 g. It is important to note that compression speed and compression time are inversely related. Therefore, Arth1 g has a higher compression ratio than conventional techniques and a higher compression speed than other ArthNgram techniques. Thus, it is concluded that, in general, selecting a larger value of N for N-gram based compression will provide a better compression ratio but slower compression speed, whereas selecting a smaller value of N will provide a higher compression speed but low compression ratio. The ideal value of N depends on the particular application and its specifications.\\n\\n 6 Conclusion\\n\\n Text compression involves the process of reducing the size of text data in order to increase processing speed, decrease transmission time, and conserve storage space. In this paper, we integrate the N-gram language model with arithmetic coding. The N-gram model captures the structure and patterns of the text data effectively and efficiently in order to model the probability distribution of data. Then text data is encoded using arithmetic coding based on this obtained probability distribution which results in a high compression ratio. The simulation results show the effectiveness of the proposed model. It is observed that this model significantly increased the compression ratio.\\n\\n Data availability\\n\\n Not applicable.\\n\\n References\\n\\n 1. Statista. iot-connected-devices-worldwide. https://www.statista.com/statistics/1183457/iot-connected-devices-worldwide/. Accessed 16 Feb 2023.\\n\\n 2. Jayasankar U, Thirumal V, Ponnurangam D. A survey on data compression techniques: from the perspective of data quality, coding schemes, data type and applications. J King Saud Univ Comput Inform Sci. 2021;33(2):119–40.\\n\\n Google Scholar\\n\\n 3. Gupta A, Nigam S. A review on different types of lossless data compression techniques 2021.\\n\\n 4. Hussain AJ, Al-Fayadh A, Radi N. Image compression techniques: a survey in lossless and lossy algorithms. Neurocomputing. 2018;300:44–69.\\n\\n Article Google Scholar\\n\\n 5. Zhu X, Li J, Liu Y, Ma C, Wang W. A survey on model compression for large language models. arXiv preprint arXiv:2308.07633 2023.\\n\\n 6. Shanmugasundaram S, Lourdusamy R. A comparative study of text compression algorithms. Int J Wisdom Based Comput. 2011;1(3):68–76.\\n\\n Google Scholar\\n\\n 7. Rahman MA, Hamada M, Rahman MA. In 2021 IEEE 14th international symposium on embedded multicore/many-core systems-on-chip (MCSoC)2021;287–291\\n\\n 8. Ignatoski M, Lerga J, Stanković L, Daković M. Comparison of entropy and dictionary based text compression in English, German, French, Italian, Czech, Hungarian, Finnish, and Croatian. Mathematics. 2020;8(7):1059.\\n\\n Article Google Scholar\\n\\n 9. Hameed M, Khmag A, Zaman F, Ramli AR. A new lossless method of Huffman coding for text data compression and decompression process with fpga implementation. J Eng Appl Sci. 2016;100(3):402–7.\\n\\n Google Scholar\\n\\n 10. Banerjee S, Singh GK. A new real-time lossless data compression algorithm for ECG and PPG signals. Biomed Signal Process Contr. 2023;79:104–27.\\n\\n Article Google Scholar\\n\\n 11. Otair M, Abualigah L, Qawaqzeh MK. Improved near-lossless technique using the huffman coding for enhancing the quality of image compression. Multimed Tools Appl. 2022;81(20):28509–29.\\n\\n Article Google Scholar\\n\\n 12. Shrividhiya G, Srujana KS, Kashyap SN, Gururaj C. In 2021 international conference on emerging smart computing and informatics (ESCI) 2021;234–237\\n\\n 13. Habib A, Islam MJ, Rahman MS. A dictionary-based text compression technique using quaternary code. Iran J Comput Sci. 2020;3(3):127–36.\\n\\n Article Google Scholar\\n\\n 14. Nguyen VH, Nguyen HT, Duong HN, Snasel V. n-gram-based text compression. Computational intelligence and neuroscience 2016;2016\\n\\n 15. Mantoro T, Ayu MA, Anggraini Y. In 2017 International Conference on Computing, Engineering, and Design (ICCED) (IEEE), 2017;1–5\\n\\n 16. Aburomman FTA. Dynamic with dictionary technique for arabic text compression. Int J Comput Appl. 2016;975:8887.\\n\\n Google Scholar\\n\\n 17. Chatterjee A, Shah RJ, Hasan KS. In: 2018 IEEE International conference on big data (big data) 2018;5137–5141\\n\\n 18. Gupta M, Agrawal P. Compression of deep learning models for text: a survey. ACM Trans Knowl Discov Data (TKDD). 2022;16(4):1–55.\\n\\n Article Google Scholar\\n\\n 19. Fauzan MN, Alif M, Prianto C. Comparison of Huffman algorithm and Lempel Ziv Welch algorithm in text file compression. IT J Res Develop. 2023;7(2):155–69.\\n\\n Article Google Scholar\\n\\n 20. Lin J, Tang J, Tang H, Yang S, Dang X, Han S. Awq: activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 2023\\n\\n 21. Ma X, Fang G, Wang X. Llm-pruner: on the structural pruning of large language models. arXiv preprint arXiv:2305.11627 2023.\\n\\n 22. Langdon GG. An introduction to arithmetic coding. IBM J Res Develop. 1984;28(2):135–49.\\n\\n Article MathSciNet Google Scholar\\n\\n 23. Kotha HD, Tummanapally M, Upadhyay VK. In J Phys Conf Ser. 2019;1228:012007.\\n\\n Article Google Scholar\\n\\n Download references\\n\\n Funding\\n\\n Not applicable.\\n\\n Author information\\n\\n Authors and Affiliations\\n\\n 1. School of Electrical Engineering and Computer Sciences (SEECS), National University of Sciences and Technology (NUST), Islamabad, 44000, Pakistan\\n\\n Ali Hassan, Sadaf Javed, Sajjad Hussain, Rizwan Ahmad & Shams Qazi\\n\\n Authors\\n 1. Ali Hassan\\n View author publications\\n\\n You can also search for this author in PubMed Google Scholar\\n\\n 2. Sadaf Javed\\n View author publications\\n\\n You can also search for this author in PubMed Google Scholar\\n\\n 3. Sajjad Hussain\\n View author publications\\n\\n You can also search for this author in PubMed Google Scholar\\n\\n 4. Rizwan Ahmad\\n View author publications\\n\\n You can also search for this author in PubMed Google Scholar\\n\\n 5. Shams Qazi\\n View author publications\\n\\n You can also search for this author in PubMed Google Scholar\\n\\n Contributions\\n\\n AH, SJ and SH presented the concept. AH, SJ, SH and RA wrote the main manuscript text. AH and SJ prepared all the figures and performed all the simulations. SH, SQ and RA helped with the performance comparison. RA, SH and SQ provided supervision. All authors reviewed the manuscript.\\n\\n Corresponding author\\n\\n Correspondence to Rizwan Ahmad.\\n\\n Ethics declarations\\n\\n Ethics approval and consent to participate\\n\\n Not applicable.\\n\\n Competing interests\\n\\n The authors have no conflicts of interest to declare relevant to the content of this work.\\n\\n Additional information\\n\\n Publisher's Note\\n\\n Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n Rights and permissions\\n\\n Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\n\\n Reprints and permissions\\n\\n About this article\\n\\n Cite this article\\n\\n Hassan, A., Javed, S., Hussain, S. et al. Arithmetic N-gram: an efficient data compression technique. Discov Computing 27, 1 (2024). https://doi.org/10.1007/s10791-024-09431-y\\n\\n Download citation\\n\\n * Received : 03 April 2023\\n\\n * Accepted : 05 February 2024\\n\\n * Published : 13 March 2024\\n\\n * DOI : https://doi.org/10.1007/s10791-024-09431-y\\n\\n Share this article\\n\\n Anyone you share the following link with will be able to read this content:\\n\\n Get shareable link\\n\\n Sorry, a shareable link is not currently available for this article.\\n\\n Copy to clipboard\\n\\n Provided by the Springer Nature SharedIt content-sharing initiative\\n\\n Keywords\\n\\n * Data compression\\n * Huffman\\n * LZW\\n * N-gram\\n * Arithmetic coding\\n * Compression ratio\\n * Large language models\\n Use our pre-submission checklist\\n\\n Avoid common mistakes on your manuscript.\\n\\n Advertisement\\n\\n Discover content\\n\\n * Journals A-Z\\n * Books A-Z\\n\\n Publish with us\\n\\n * Publish your research\\n * Open access publishing\\n\\n Products and services\\n\\n * Our products\\n * Librarians\\n * Societies\\n * Partners and advertisers\\n\\n Our imprints\\n\\n * Springer\\n * Nature Portfolio\\n * BMC\\n * Palgrave Macmillan\\n * Apress\\n * Your privacy choices/Manage cookies\\n * Your US state privacy rights\\n * Accessibility statement\\n * Terms and conditions\\n * Privacy policy\\n * Help and support\\n\\n 3.133.104.73\\n\\n Not affiliated\\n\\n © 2024 Springer Nature\", highlights=None, highlight_scores=None, summary=None)]\n",
      "Downloaded: 1911.03572v1.pdf\n",
      "Error downloading A novel lossless encoding algorithm for data compression–genomics data as an exemplar: 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC11799261/\n",
      "Downloaded: A_Novel_Data_Compression_Methodology_Focused_on_Power_Quality_Signals_Using_Compressive_Sampling_Matching_Pursuit.pdf\n",
      "Downloaded: s41598-023-29068-z.pdf\n",
      "Downloaded: 2304.01106v1.pdf\n",
      "Downloaded: 2304.07342v2.pdf\n",
      "Downloaded: Arithmetic_N-gram_an_efficient_data_compression_technique.pdf\n"
     ]
    }
   ],
   "source": [
    "def download_breakthrough_papers(results):\n",
    "    results = result.results\n",
    "    print(results)\n",
    "    for paper in results:\n",
    "        url = paper.url\n",
    "        title = paper.title\n",
    "        import os\n",
    "        import requests\n",
    "        from urllib.parse import urlparse\n",
    "        \n",
    "        # Create papers directory if it doesn't exist\n",
    "        if not os.path.exists('papers'):\n",
    "            os.makedirs('papers')\n",
    "            \n",
    "        try:\n",
    "            # Download the paper\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get filename from URL or use title if URL has no filename\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            if not filename or '.' not in filename:\n",
    "                # Clean title and add .pdf extension\n",
    "                filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_'))\n",
    "                filename = filename.strip().replace(' ', '_') + '.pdf'\n",
    "                \n",
    "            filepath = os.path.join('papers', filename)\n",
    "            \n",
    "            # Save the paper\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                \n",
    "            print(f\"Downloaded: {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {title}: {str(e)}\")\n",
    "download_breakthrough_papers(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing s41598-023-29068-z.pdf...\n",
      "Analysis saved to paper_analysis/s41598-023-29068-z/analysis.json\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing 2304.07342v2.pdf...\n",
      "Analysis saved to paper_analysis/2304.07342v2/analysis.json\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing 2304.01106v1.pdf...\n",
      "Analysis saved to paper_analysis/2304.01106v1/analysis.json\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing Arithmetic_N-gram_an_efficient_data_compression_technique.pdf...\n",
      "Error reading PDF papers/Arithmetic_N-gram_an_efficient_data_compression_technique.pdf: EOF marker not found\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing A_Novel_Data_Compression_Methodology_Focused_on_Power_Quality_Signals_Using_Compressive_Sampling_Matching_Pursuit.pdf...\n",
      "Error reading PDF papers/A_Novel_Data_Compression_Methodology_Focused_on_Power_Quality_Signals_Using_Compressive_Sampling_Matching_Pursuit.pdf: EOF marker not found\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Processing 1911.03572v1.pdf...\n",
      "Analysis saved to paper_analysis/1911.03572v1/analysis.json\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import PyPDF2\n",
    "\n",
    "# Initialize Claude\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
    "\n",
    "def extract_paper_details(pdf_path):\n",
    "    \"\"\"Extract key details from a PDF paper using Claude\"\"\"\n",
    "    \n",
    "    # Read PDF content\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            \n",
    "            # Extract all pages from the PDF\n",
    "            for i in range(len(reader.pages)):\n",
    "                text += reader.pages[i].extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Prompt Claude to extract key details\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "    You are an expert at analyzing academic papers focused on compression optimization. Extract and summarize the key details including:\n",
    "    - Title and Authors\n",
    "    - Main research question/objective\n",
    "    - Key methodology and compression techniques used\n",
    "    - Performance metrics and compression ratios achieved\n",
    "    - Target data types and use cases\n",
    "    - Hardware/software requirements and constraints\n",
    "    - Main findings/conclusions\n",
    "    - Limitations and trade-offs\n",
    "    - Potential application domains where this compression could be valuable\n",
    "    - Integration considerations for existing systems\n",
    "    \n",
    "    For each paper, identify:\n",
    "    1. What types of data can this compression method handle?\n",
    "    2. What are the key performance characteristics (compression ratio, speed, quality)?\n",
    "    3. What are the computational requirements?\n",
    "    4. What existing systems could benefit from this compression method?\n",
    "    5. What would be needed to integrate this into an existing system?\n",
    "    \n",
    "    Present the information in a clear, structured format that helps evaluate potential applications.\n",
    "\n",
    "    Return only the information in the formatted JSON response and nothing else. Format the response as JSON with the following structure:\n",
    "    {\n",
    "        \"metadata\": {\n",
    "            \"title\": \"\",\n",
    "            \"authors\": []\n",
    "        },\n",
    "        \"research\": {\n",
    "            \"objective\": \"\",\n",
    "            \"methodology\": [],\n",
    "            \"compression_techniques\": [],\n",
    "            \"performance_metrics\": {\n",
    "                \"compression_ratio\": \"\",\n",
    "                \"speed\": \"\",\n",
    "                \"quality\": \"\",\n",
    "                \"other_metrics\": []\n",
    "            }\n",
    "        },\n",
    "        \"technical_details\": {\n",
    "            \"target_data_types\": [],\n",
    "            \"use_cases\": [],\n",
    "            \"requirements\": {\n",
    "                \"hardware\": [],\n",
    "                \"software\": [],\n",
    "                \"constraints\": []\n",
    "            }\n",
    "        },\n",
    "        \"findings\": {\n",
    "            \"main_conclusions\": [],\n",
    "            \"limitations\": [],\n",
    "            \"trade_offs\": []\n",
    "        },\n",
    "        \"applications\": {\n",
    "            \"potential_domains\": [],\n",
    "            \"integration_considerations\": [],\n",
    "            \"beneficiary_systems\": []\n",
    "        },\n",
    "        \"evaluation\": {\n",
    "            \"data_types_supported\": [],\n",
    "            \"performance_characteristics\": {\n",
    "                \"compression_ratio\": \"\",\n",
    "                \"speed\": \"\",\n",
    "                \"quality\": \"\"\n",
    "            },\n",
    "            \"computational_requirements\": [],\n",
    "            \"integration_requirements\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Ensure all fields are filled with relevant information from the paper. If any field is not applicable, use an empty array or string as appropriate.\n",
    "    \"\"\")\n",
    "    \n",
    "    human_message = HumanMessage(content=f\"Extract key details from this paper text: {text}\")\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([system_message, human_message])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details from {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "# Process each paper in the papers directory\n",
    "papers_dir = 'papers'\n",
    "for filename in os.listdir(papers_dir):\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        filepath = os.path.join(papers_dir, filename)\n",
    "        \n",
    "        # Create a folder for this paper (using filename without extension)\n",
    "        paper_name = os.path.splitext(filename)[0]\n",
    "        paper_dir = os.path.join('paper_analysis', paper_name)\n",
    "        os.makedirs(paper_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy the original paper to the new directory\n",
    "        import shutil\n",
    "        shutil.copy2(filepath, os.path.join(paper_dir, filename))\n",
    "        \n",
    "        # Extract and save the details\n",
    "        details = extract_paper_details(filepath)\n",
    "        if details:\n",
    "            # Save the analysis as JSON\n",
    "            import json\n",
    "            analysis_path = os.path.join(paper_dir, 'analysis.json')\n",
    "            with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(details, f, indent=2)\n",
    "            print(f\"Analysis saved to {analysis_path}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paper: 2304.07342v2\n",
      "Search query: research papers about GPU-accelerated lossless compression for multi-byte data in high-performance computing and deep learning applications that could benefit from LZSS optimization techniques\n",
      "Query saved to paper_analysis/2304.07342v2/query.txt\n",
      "\n",
      "Paper: 1911.03572v1\n",
      "Search query: research papers about general-purpose lossless compression for sequential data that could benefit from neural network modeling and arithmetic coding techniques\n",
      "Query saved to paper_analysis/1911.03572v1/query.txt\n",
      "\n",
      "Paper: s41598-023-29068-z\n",
      "Search query: research papers about image compression techniques for virtual reality, augmented reality, and metaverse applications that could benefit from soft compression algorithms with superior compression ratios\n",
      "Query saved to paper_analysis/s41598-023-29068-z/query.txt\n",
      "\n",
      "Paper: 2304.01106v1\n",
      "Search query: research papers about semantic text compression techniques that could benefit from BERT and Transformer models for preserving meaning in literature and political documents\n",
      "Query saved to paper_analysis/2304.01106v1/query.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def generate_search_query(analysis_path):\n",
    "    with open(analysis_path, 'r', encoding='utf-8') as f:\n",
    "        analysis_text = f.read()\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "    Generate a search query to find research papers that could benefit from the applications described in the analysis.\n",
    "    Focus on:\n",
    "    1. The target data types and use cases\n",
    "    2. The potential domains and beneficiary systems\n",
    "    3. The performance characteristics and requirements\n",
    "    \n",
    "    Format the query as a natural language search query. For instance: 'research papers about <potential application> that could benefit from <breakthrough technique>'\n",
    "    \"\"\")\n",
    "    \n",
    "    human_message = HumanMessage(content=f\"Generate a natural language search query based on this analysis: {analysis_text}. Return only the query, no other text. The query should be concise and to the point, and in natural language. \")\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke([system_message, human_message])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating search query: {str(e)}\")\n",
    "    None\n",
    "# Process each analysis.json file\n",
    "papers_dir = 'paper_analysis'\n",
    "for paper_folder in os.listdir(papers_dir):\n",
    "    analysis_path = os.path.join(papers_dir, paper_folder, 'analysis.json')\n",
    "    if os.path.exists(analysis_path):\n",
    "        print(f\"\\nPaper: {paper_folder}\")\n",
    "        query = generate_search_query(analysis_path)\n",
    "        print(f\"Search query: {query}\")\n",
    "        \n",
    "        # Save query to query.txt\n",
    "        query_path = os.path.join(papers_dir, paper_folder, 'query.txt')\n",
    "        with open(query_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(query)\n",
    "        print(f\"Query saved to {query_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for papers related to: 2304.07342v2\n",
      "Title: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "Score: 0.4447816014289856\n",
      "Published Date: 2023-06-21T00:00:00.000Z\n",
      "Author: Boyuan Zhang,Jiannan Tian,Sheng Di,Xiangyao Yu,Martin Swany,Dingwen Tao,Franck Cappello\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Boyuan Zhang bozhan@iu.edu\n",
      "Jiannan Tian\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Martin Swany swany@indiana.edu\n",
      "Dingwen Tao ditao@iu.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Boyuan Zhang\n",
      "Jiannan Tian\n",
      "XiaodongSheng Di\n",
      "Yu\n",
      "Martin Swany\n",
      "Ding-Wen Tao\n",
      "Franck Cappello\n",
      "Indiana University Bloomington\n",
      "INUSA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IL, INUSA, USA\n",
      "Department of Intelligent Systems Engineering, Luddy School of Informatics, Computing, and Engineering\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Indiana University\n",
      "gpuLZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "2023 International Conference on Supercomputing (ICS '23)\n",
      "Orlando, FL, USA 2023; Orlando, FL, USA; New York, NY, USAACM12June 21-23, 2023. June 21-23, 202310.1145/3577193.3593706ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: GitHub - hipdac-lab/ICS23-GPULZ: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "ID: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "Score: 0.36544671654701233\n",
      "Published Date: 2023-04-11T00:00:00.000Z\n",
      "Author: hipdac-lab\n",
      "Image: https://opengraph.githubassets.com/3bcd20c09c13ef98258430e4e4835415b7941df14b40919298e813daa3c87c41/hipdac-lab/ICS23-GPULZ\n",
      "Favicon: https://github.com/fluidicon.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: [Skip to content](https://github.com/hipdac-lab/ICS23-GPULZ#start-of-content)\n",
      "\n",
      "You signed in with another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.Dismiss alert\n",
      "\n",
      "{{ message }}\n",
      "\n",
      "[hipdac-lab](https://github.com/hipdac-lab)/ **[ICS23-GPULZ](https://github.com/hipdac-lab/ICS23-GPULZ)** Public\n",
      "\n",
      "- [Notifications](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ) You must be signed in to change notification settings\n",
      "- [Fork\\\n",
      "0](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "- [Star\\\n",
      "13](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "\n",
      "\n",
      "GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "\n",
      "[13\\\n",
      "stars](https://github.com/hipdac-lab/ICS23\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Light Loss-Less Data Compression, with GPU Implementation\n",
      "URL: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "ID: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "Score: 0.36278703808784485\n",
      "Published Date: 2016-11-05T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, funasaka@cs.hiroshima-u.ac.jp, Koji  Nakano, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, yasuaki@cs.hiroshima-u.ac.jp\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: There is no doubt that data compression is very important in computer engineering. However, most lossless data compression and decompression algorithms are very hard to parallelize, because they use dictionaries updated sequentially. The main contribution of this paper is to present a new lossless data compression method that we call Light Loss-Less (LLL) compression. It is designed so that decompression can be highly parallelized and run very efficiently on the GPU. This makes sense for many applications in which compressed data is read and decompressed many times and decompression performed more frequently than compression. We show optimal sequential and parallel algorithms for LLL decompression and implement them to run on Core i7-4790 CPU and GeForce GTX 1080 GPU, respectively. To show the potentiality of LLL compression method, we have evaluated the running time using five images and compared with well-known compression methods LZW and LZSS. Our GPU implementation of LLL decompres\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Adaptive loss‐less data compression method optimized for GPU decompression\n",
      "URL: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "ID: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "Score: 0.3729551434516907\n",
      "Published Date: 2017-08-17T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Koji  Nakano, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Adaptive loss‐less data compression method optimized for GPU decompression - Funasaka - 2017 - Concurrency and Computation: Practice and Experience - Wiley Online Library\n",
      "\n",
      "Opens in a new windowOpens an external websiteOpens an external website in a new window\n",
      "\n",
      "Close this dialog\n",
      "\n",
      "This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link: [Privacy Policy](https://www.wiley.com/privacy)\n",
      "\n",
      "Close Cookie Preferences\n",
      "\n",
      "[Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "[Volume 29, Issue 24](https://onlinelibrary.wiley.com/toc/15320634/2017/29/24) e4283 [![Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/pb-assets/journal-banners/15320634-1501384732273.jpg)](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "SPECIAL ISSUE PAPER\n",
      "\n",
      "# Adaptive loss-less data compression \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "ID: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "Score: None\n",
      "Published Date: 2022-07-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Enabling faster processing of big-data using GPU\n",
      "decompression\n",
      "Andrei Gorgan\n",
      "a.gorgan@student.utwente.nl\n",
      "University of Twente\n",
      "Enschede, The Netherlands\n",
      "Abstract\n",
      "Processing big-data has been shown to have a many fold\n",
      "speedup for GPU hardware, however the process of retriev\u0002ing ready-to-use data from storage devices still requires a\n",
      "process of decompression, currently performed on the CPU.\n",
      "Due to the increasing computational power of GPUs, the\n",
      "decompression step starves the GPU of data, effectively do\u0002ing nothing until more data is available to process. This\n",
      "research analyses the minimum required speed of decom\u0002pression on a GPU, such that offloading the decompression\n",
      "step to the GPU, is faster than traditional methods that uti\u0002lize the CPU. Results show that GPUs cannot outperform\n",
      "CPUs when considering compression ratio, however the im\u0002proved parallelism of GPUs allows for a 2 times reduction in\n",
      "decompression times.\n",
      "Keywords: GPGPU, bypassing CPU decompression, big-data\n",
      "compression and \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: CODAG: Characterizing and Optimizing Decompression Algorithms for GPUs\n",
      "URL: https://arxiv.org/abs/2307.03760\n",
      "ID: https://arxiv.org/abs/2307.03760\n",
      "Score: 0.3703548312187195\n",
      "Published Date: 2023-12-04T00:00:00.000Z\n",
      "Author: Park; Jeongmin; Qureshi; Zaid; Mailthody; Vikram; Gacek; Andrew; Shao; Shunfan; AlMasri; Mohammad; Gelado; Isaac; Xiong; Jinjun; Newburn; Chris; Chung; I-hsin; Garland; Michael; Sakharnykh; Nikolay; Hwu; Wen-mei\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Jeongmin Park, Zaid Qureshi, Vikram Mailthody, Andrew Gacek, Shunfan Shao, Mohammad AlMasri, Isaac Gelado, Jinjun Xiong, Chris Newburn, I-hsin Chung, Michael Garland, Nikolay Sakharnykh, Wen-mei Hwu \n",
      " Download PDF \n",
      "Data compression and decompression have become vital components of big-data applications to manage the exponential growth in the amount of data collected and stored. Furthermore, big-data applications have increasingly adopted GPUs due to their high compute throughput and memory bandwidth. Prior works presume that decompression is memory-bound and have dedicated most of the GPU's threads to data movement and adopted complex software techniques to hide memory latency for reading compressed data and writing uncompressed data. This paper shows that these techniques lead to poor GPU resource utilization as most threads end up waiting for the few decoding threads, exposing compute and synchronization latencies.\n",
      " Based on this observation, we propose CODAG, a novel and simple ke\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "URL: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "Score: 0.353089839220047\n",
      "Published Date: 2023-08-09T00:00:00.000Z\n",
      "Author: Jiajun Huang,Sheng Di,Xiaodong Yu,Yujia Zhai,Jinyang Liu,Yafan Huang,Ken Raffenetti,Hui Zhou,Kai Zhao,Zizhong Chen,Franck Cappello,Yanfei Guo,Rajeev Thakur\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Jiajun Huang\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Yujia Zhai\n",
      "Jinyang Liu\n",
      "Yafan Huang yafan-huang@uiowa.edu\n",
      "Ken Raffenetti raffenet@anl.gov\n",
      "Hui Zhou zhouh@anl.gov\n",
      "Kai Zhao kzhao@cs.fsu.edu\n",
      "Zizhong Chen chen@cs.ucr.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Yanfei Guo yguo@anl.gov\n",
      "Rajeev Thakur thakur@anl.gov\n",
      "Argonne National Laboratory Lemont\n",
      "University of Iowa\n",
      "Iowa CityUnited States of America, United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "University of California, Riverside Riverside, United States of America Stevens Institute of Technology Hoboken, United States of America University of California, Riverside Riverside, United States of America Un\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about GPU-accelerated lossless compression for multi-byte data in high-performance computing and deep learning applications that could benefit from LZSS optimization techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.012\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.007}\n",
      "Search results:\n",
      "Title: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "Score: 0.4447816014289856\n",
      "Published Date: 2023-06-21T00:00:00.000Z\n",
      "Author: Boyuan Zhang,Jiannan Tian,Sheng Di,Xiangyao Yu,Martin Swany,Dingwen Tao,Franck Cappello\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Boyuan Zhang bozhan@iu.edu\n",
      "Jiannan Tian\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Martin Swany swany@indiana.edu\n",
      "Dingwen Tao ditao@iu.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Boyuan Zhang\n",
      "Jiannan Tian\n",
      "XiaodongSheng Di\n",
      "Yu\n",
      "Martin Swany\n",
      "Ding-Wen Tao\n",
      "Franck Cappello\n",
      "Indiana University Bloomington\n",
      "INUSA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IL, INUSA, USA\n",
      "Department of Intelligent Systems Engineering, Luddy School of Informatics, Computing, and Engineering\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Indiana University\n",
      "gpuLZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "2023 International Conference on Supercomputing (ICS '23)\n",
      "Orlando, FL, USA 2023; Orlando, FL, USA; New York, NY, USAACM12June 21-23, 2023. June 21-23, 202310.1145/3577193.3593706ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: GitHub - hipdac-lab/ICS23-GPULZ: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "ID: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "Score: 0.36544671654701233\n",
      "Published Date: 2023-04-11T00:00:00.000Z\n",
      "Author: hipdac-lab\n",
      "Image: https://opengraph.githubassets.com/3bcd20c09c13ef98258430e4e4835415b7941df14b40919298e813daa3c87c41/hipdac-lab/ICS23-GPULZ\n",
      "Favicon: https://github.com/fluidicon.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: [Skip to content](https://github.com/hipdac-lab/ICS23-GPULZ#start-of-content)\n",
      "\n",
      "You signed in with another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.Dismiss alert\n",
      "\n",
      "{{ message }}\n",
      "\n",
      "[hipdac-lab](https://github.com/hipdac-lab)/ **[ICS23-GPULZ](https://github.com/hipdac-lab/ICS23-GPULZ)** Public\n",
      "\n",
      "- [Notifications](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ) You must be signed in to change notification settings\n",
      "- [Fork\\\n",
      "0](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "- [Star\\\n",
      "13](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "\n",
      "\n",
      "GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "\n",
      "[13\\\n",
      "stars](https://github.com/hipdac-lab/ICS23\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Light Loss-Less Data Compression, with GPU Implementation\n",
      "URL: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "ID: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "Score: 0.36278703808784485\n",
      "Published Date: 2016-11-05T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, funasaka@cs.hiroshima-u.ac.jp, Koji  Nakano, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, yasuaki@cs.hiroshima-u.ac.jp\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: There is no doubt that data compression is very important in computer engineering. However, most lossless data compression and decompression algorithms are very hard to parallelize, because they use dictionaries updated sequentially. The main contribution of this paper is to present a new lossless data compression method that we call Light Loss-Less (LLL) compression. It is designed so that decompression can be highly parallelized and run very efficiently on the GPU. This makes sense for many applications in which compressed data is read and decompressed many times and decompression performed more frequently than compression. We show optimal sequential and parallel algorithms for LLL decompression and implement them to run on Core i7-4790 CPU and GeForce GTX 1080 GPU, respectively. To show the potentiality of LLL compression method, we have evaluated the running time using five images and compared with well-known compression methods LZW and LZSS. Our GPU implementation of LLL decompres\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Adaptive loss‐less data compression method optimized for GPU decompression\n",
      "URL: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "ID: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "Score: 0.3729551434516907\n",
      "Published Date: 2017-08-17T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Koji  Nakano, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Adaptive loss‐less data compression method optimized for GPU decompression - Funasaka - 2017 - Concurrency and Computation: Practice and Experience - Wiley Online Library\n",
      "\n",
      "Opens in a new windowOpens an external websiteOpens an external website in a new window\n",
      "\n",
      "Close this dialog\n",
      "\n",
      "This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link: [Privacy Policy](https://www.wiley.com/privacy)\n",
      "\n",
      "Close Cookie Preferences\n",
      "\n",
      "[Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "[Volume 29, Issue 24](https://onlinelibrary.wiley.com/toc/15320634/2017/29/24) e4283 [![Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/pb-assets/journal-banners/15320634-1501384732273.jpg)](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "SPECIAL ISSUE PAPER\n",
      "\n",
      "# Adaptive loss-less data compression \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "ID: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "Score: None\n",
      "Published Date: 2022-07-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Enabling faster processing of big-data using GPU\n",
      "decompression\n",
      "Andrei Gorgan\n",
      "a.gorgan@student.utwente.nl\n",
      "University of Twente\n",
      "Enschede, The Netherlands\n",
      "Abstract\n",
      "Processing big-data has been shown to have a many fold\n",
      "speedup for GPU hardware, however the process of retriev\u0002ing ready-to-use data from storage devices still requires a\n",
      "process of decompression, currently performed on the CPU.\n",
      "Due to the increasing computational power of GPUs, the\n",
      "decompression step starves the GPU of data, effectively do\u0002ing nothing until more data is available to process. This\n",
      "research analyses the minimum required speed of decom\u0002pression on a GPU, such that offloading the decompression\n",
      "step to the GPU, is faster than traditional methods that uti\u0002lize the CPU. Results show that GPUs cannot outperform\n",
      "CPUs when considering compression ratio, however the im\u0002proved parallelism of GPUs allows for a 2 times reduction in\n",
      "decompression times.\n",
      "Keywords: GPGPU, bypassing CPU decompression, big-data\n",
      "compression and \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: CODAG: Characterizing and Optimizing Decompression Algorithms for GPUs\n",
      "URL: https://arxiv.org/abs/2307.03760\n",
      "ID: https://arxiv.org/abs/2307.03760\n",
      "Score: 0.3703548312187195\n",
      "Published Date: 2023-12-04T00:00:00.000Z\n",
      "Author: Park; Jeongmin; Qureshi; Zaid; Mailthody; Vikram; Gacek; Andrew; Shao; Shunfan; AlMasri; Mohammad; Gelado; Isaac; Xiong; Jinjun; Newburn; Chris; Chung; I-hsin; Garland; Michael; Sakharnykh; Nikolay; Hwu; Wen-mei\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Jeongmin Park, Zaid Qureshi, Vikram Mailthody, Andrew Gacek, Shunfan Shao, Mohammad AlMasri, Isaac Gelado, Jinjun Xiong, Chris Newburn, I-hsin Chung, Michael Garland, Nikolay Sakharnykh, Wen-mei Hwu \n",
      " Download PDF \n",
      "Data compression and decompression have become vital components of big-data applications to manage the exponential growth in the amount of data collected and stored. Furthermore, big-data applications have increasingly adopted GPUs due to their high compute throughput and memory bandwidth. Prior works presume that decompression is memory-bound and have dedicated most of the GPU's threads to data movement and adopted complex software techniques to hide memory latency for reading compressed data and writing uncompressed data. This paper shows that these techniques lead to poor GPU resource utilization as most threads end up waiting for the few decoding threads, exposing compute and synchronization latencies.\n",
      " Based on this observation, we propose CODAG, a novel and simple ke\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "URL: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "Score: 0.353089839220047\n",
      "Published Date: 2023-08-09T00:00:00.000Z\n",
      "Author: Jiajun Huang,Sheng Di,Xiaodong Yu,Yujia Zhai,Jinyang Liu,Yafan Huang,Ken Raffenetti,Hui Zhou,Kai Zhao,Zizhong Chen,Franck Cappello,Yanfei Guo,Rajeev Thakur\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Jiajun Huang\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Yujia Zhai\n",
      "Jinyang Liu\n",
      "Yafan Huang yafan-huang@uiowa.edu\n",
      "Ken Raffenetti raffenet@anl.gov\n",
      "Hui Zhou zhouh@anl.gov\n",
      "Kai Zhao kzhao@cs.fsu.edu\n",
      "Zizhong Chen chen@cs.ucr.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Yanfei Guo yguo@anl.gov\n",
      "Rajeev Thakur thakur@anl.gov\n",
      "Argonne National Laboratory Lemont\n",
      "University of Iowa\n",
      "Iowa CityUnited States of America, United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "University of California, Riverside Riverside, United States of America Stevens Institute of Technology Hoboken, United States of America University of California, Riverside Riverside, United States of America Un\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about GPU-accelerated lossless compression for multi-byte data in high-performance computing and deep learning applications that could benefit from LZSS optimization techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.012\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.007}\n",
      "\n",
      "Searching for papers related to: 1911.03572v1\n",
      "Title: DZip: improved general-purpose lossless compression based on novel neural network modeling\n",
      "URL: https://arxiv.org/abs/1911.03572\n",
      "ID: https://arxiv.org/abs/1911.03572\n",
      "Score: 0.4118901491165161\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: We consider lossless compression based on statistical data modeling followed\n",
      "by prediction-based encoding, where an accurate statistical model for the input\n",
      "data leads to substantial improvements in compression. We propose DZip, a\n",
      "general-purpose compressor for sequential data that exploits the well-known\n",
      "modeling capabilities of neural networks (NNs) for prediction, followed by\n",
      "arithmetic coding. Dzip uses a novel hybrid architecture based on adaptive and\n",
      "semi-adaptive training. Unlike most NN based compressors, DZip does not require\n",
      "additional training data and is not restricted to specific data types, only\n",
      "needing the alphabet size of the input data. The proposed compressor\n",
      "outperforms general-purpose compressors such as Gzip (on average 26% reduction)\n",
      "on a variety of real datasets, achieves near-optimal compression on synthetic\n",
      "datasets, and performs close to specialized compressors for large sequence\n",
      "lengths, without any human input. The main\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/abs/1811.08162\n",
      "ID: https://arxiv.org/abs/1811.08162\n",
      "Score: 0.39063823223114014\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: Sequential data is being generated at an unprecedented pace in various forms,\n",
      "including text and genomic data. This creates the need for efficient\n",
      "compression mechanisms to enable better storage, transmission and processing of\n",
      "such data. To solve this problem, many of the existing compressors attempt to\n",
      "learn models for the data and perform prediction-based compression. Since\n",
      "neural networks are known as universal function approximators with the\n",
      "capability to learn arbitrarily complex mappings, and in practice show\n",
      "excellent performance in prediction tasks, we explore and devise methods to\n",
      "compress sequential data using neural network predictors. We combine recurrent\n",
      "neural network predictors with an arithmetic coder and losslessly compress a\n",
      "variety of synthetic, text and genomic datasets. The proposed compressor\n",
      "outperforms Gzip on the real datasets and achieves near-optimal compression for\n",
      "the synthetic datasets. The results also help understan\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Deep Lossless Compression Algorithm Based on Arithmetic Coding for Power Data\n",
      "URL: https://www.mdpi.com/1424-8220/22/14/5331\n",
      "ID: https://www.mdpi.com/1424-8220/22/14/5331\n",
      "Score: None\n",
      "Published Date: 2022-07-16T00:00:00.000Z\n",
      "Author: Fuyuan Song\n",
      "Image: https://pub.mdpi-res.com/sensors/sensors-22-05331/article_deploy/html/images/sensors-22-05331-g001-550.jpg?1657977118\n",
      "Favicon: https://pub.mdpi-res.com/img/mask-icon-128.svg\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Next Article in Journal\n",
      "\n",
      "[Reconstructing Superquadrics from Intensity and Color Images](https://www.mdpi.com/1424-8220/22/14/5332)\n",
      "\n",
      "Next Article in Special Issue\n",
      "\n",
      "[A Local Electricity and Carbon Trading Method for Multi-Energy Microgrids Considering Cross-Chain Interaction](https://www.mdpi.com/1424-8220/22/18/6935)\n",
      "\n",
      "Previous Article in Journal\n",
      "\n",
      "[Vision-Based Module for Herding with a Sheepdog Robot](https://www.mdpi.com/1424-8220/22/14/5321)\n",
      "\n",
      "## Journals\n",
      "\n",
      "[Active Journals](https://www.mdpi.com/about/journals) [Find a Journal](https://www.mdpi.com/about/journalfinder) [Proceedings Series](https://www.mdpi.com/about/proceedings)\n",
      "\n",
      "[**Topics**](https://www.mdpi.com/topics)\n",
      "\n",
      "## Information\n",
      "\n",
      "[For Authors](https://www.mdpi.com/authors) [For Reviewers](https://www.mdpi.com/reviewers) [For Editors](https://www.mdpi.com/editors) [For Librarians](https://www.mdpi.com/librarians) [For Publishers](https://www.mdpi.com/publishing_services) [For Societies](https://www.mdpi.com/societies) [For Confer\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: https://bellard.org/nncp/nncp.pdf\n",
      "ID: https://bellard.org/nncp/nncp.pdf\n",
      "Score: 0.37134554982185364\n",
      "Published Date: 2019-05-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Lossless Data Compression with Neural Networks\n",
      "Fabrice Bellard\n",
      "May 4, 2019\n",
      "Abstract\n",
      "We describe our implementation of a lossless data compressor using neu\u0002ral networks. We tuned Long Short-Term Memory and Transformer based\n",
      "models in order to achieve a fast training convergence. We evaluated the\n",
      "performance on the widely used enwik8 Hutter Prize benchmark.\n",
      "1 Introduction\n",
      "Although the best lossless data compressors are based on neural network models\n",
      "[7], they use them mostly to mix the statistics from a large number of hand coded\n",
      "models. Here we present lossless data compressors using pure neural network\n",
      "models based on Long Short-Term Memory (LSTM) and Transformer models.\n",
      "The lossless data compressor employs the traditional predictive approach: at\n",
      "each time t, the encoder uses the neural network model to compute the probability\n",
      "vector p of the next symbol value st knowing all the preceding symbols s0 up to\n",
      "st−1. The actual symbol value stis encoded using an arithmetic encoder with\n",
      "appro\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression Using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "ID: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "Score: 0.38720083236694336\n",
      "Published Date: 2019-03-01T00:00:00.000Z\n",
      "Author: Mohit Goyal,Kedar Tatwawadi,Shubham Chandak,Idoia Ochoa\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mohit Goyal goyal.mohit999@gmail.com \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "Kedar Tatwawadi kedart@stanford.edu \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Shubham Chandak \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Idoia Ochoa \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "\n",
      "Department of Electrical Engineering\n",
      "Indian Institute of Technology\n",
      "DelhiIndia\n",
      "\n",
      "DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "\n",
      "Sequential data is being generated at an unprecedented pace in various forms, including text and genomic data. This creates the need for efficient compression mechanisms to enable better storage, transmission and processing of such data. To solve this problem, many of the existing compressors attempt to learn models for the data and perform predictionbased compression. Since neural networks are known as universal function approximators with the capability to learn arbit\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: https://arxiv.org/pdf/1811.08162\n",
      "ID: https://arxiv.org/pdf/1811.08162\n",
      "Score: None\n",
      "Published Date: 2018-11-20T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: DeepZip: Lossless Data Compression using\n",
      "Recurrent Neural Networks\n",
      "Mohit Goyal†,γ, Kedar Tatwawadi∗, Shubham Chandak∗ and Idoia Ochoaγ\n",
      "†Department of Electrical Engineering, Indian Institute of Technology Delhi, India\n",
      "∗Department of Electrical Engineering, Stanford University, CA, USA\n",
      "γElectrical and Computer Engineering, University of Illinois, Urbana, IL, USA\n",
      "goyal.mohit999@gmail.com, kedart@stanford.edu\n",
      "Abstract\n",
      "Sequential data is being generated at an unprecedented pace in various forms, including text\n",
      "and genomic data. This creates the need for efficient compression mechanisms to enable\n",
      "better storage, transmission and processing of such data. To solve this problem, many\n",
      "of the existing compressors attempt to learn models for the data and perform prediction\u0002based compression. Since neural networks are known as universal function approximators\n",
      "with the capability to learn arbitrarily complex mappings, and in practice show excellent\n",
      "performance in prediction tasks, we explore and de\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.37147095799446106\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: TRACE: A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "URL: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "Score: 0.37043696641921997\n",
      "Published Date: 2022-04-25T00:00:00.000Z\n",
      "Author: Yu Mao,Yufei Cui,Tei-Wei Kuo,Chun Jason Xue\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Yu Mao\n",
      "Yufei Cui\n",
      "Tei-Wei Kuo\n",
      "Chun Jason Xue\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science and Information Engineering\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science\n",
      "National Taiwan University\n",
      "City University of Hong\n",
      "Kong\n",
      "A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "Deep-learning-based compressor has received interests recently due to much improved compression ratio. However, modern approaches suffer from long execution time. To ease this problem, this paper targets on cutting down the execution time of deep-learning-based compressors. Building historydependencies sequentially (e.g., recurrent neural networks) is responsible for long inference latency. Instead, we introduce transformer into deep learning compressors to build historydependencies in parallel. However, existing transformer is too heavy in computation and incomp\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: A Lossless Compression Technique for the Downlink Control Information Message\n",
      "URL: https://arxiv.org/abs/2407.16319\n",
      "ID: https://arxiv.org/abs/2407.16319\n",
      "Score: 0.37019482254981995\n",
      "Published Date: 2024-07-23T00:00:00.000Z\n",
      "Author: Liu; Bryan; Valcarce; Alvaro; Srinath; K Pavan\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "Improving the reliability and spectral efficiency of wireless systems is a key goal in wireless systems. However, most efforts have been devoted to improving data channel capacity, whereas control-plane capacity bottlenecks are often neglected. In this paper, we propose a means of improving the control-plane capacity and reliability by shrinking the bit size of a key signaling message - the 5G Downlink Control Information (DCI). In particular, a transformer model is studied as a probability distribution estimator for Arithmetic coding to achieve lossless compression. Feature engineering, neural model design, and training technique are comprehensively discussed in this paper. Both temporal and spatial correlations among DCI messages are explored by the transformer model to achieve reasonable lossless compression performance. Numerical results show that the proposed method achieves 21.7% higher compression ratio than Huffman coding in DCI compression for\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about general-purpose lossless compression for sequential data that could benefit from neural network modeling and arithmetic coding techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.014\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.009}\n",
      "Search results:\n",
      "Title: DZip: improved general-purpose lossless compression based on novel neural network modeling\n",
      "URL: https://arxiv.org/abs/1911.03572\n",
      "ID: https://arxiv.org/abs/1911.03572\n",
      "Score: 0.4118901491165161\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: We consider lossless compression based on statistical data modeling followed\n",
      "by prediction-based encoding, where an accurate statistical model for the input\n",
      "data leads to substantial improvements in compression. We propose DZip, a\n",
      "general-purpose compressor for sequential data that exploits the well-known\n",
      "modeling capabilities of neural networks (NNs) for prediction, followed by\n",
      "arithmetic coding. Dzip uses a novel hybrid architecture based on adaptive and\n",
      "semi-adaptive training. Unlike most NN based compressors, DZip does not require\n",
      "additional training data and is not restricted to specific data types, only\n",
      "needing the alphabet size of the input data. The proposed compressor\n",
      "outperforms general-purpose compressors such as Gzip (on average 26% reduction)\n",
      "on a variety of real datasets, achieves near-optimal compression on synthetic\n",
      "datasets, and performs close to specialized compressors for large sequence\n",
      "lengths, without any human input. The main\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/abs/1811.08162\n",
      "ID: https://arxiv.org/abs/1811.08162\n",
      "Score: 0.39063823223114014\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: Sequential data is being generated at an unprecedented pace in various forms,\n",
      "including text and genomic data. This creates the need for efficient\n",
      "compression mechanisms to enable better storage, transmission and processing of\n",
      "such data. To solve this problem, many of the existing compressors attempt to\n",
      "learn models for the data and perform prediction-based compression. Since\n",
      "neural networks are known as universal function approximators with the\n",
      "capability to learn arbitrarily complex mappings, and in practice show\n",
      "excellent performance in prediction tasks, we explore and devise methods to\n",
      "compress sequential data using neural network predictors. We combine recurrent\n",
      "neural network predictors with an arithmetic coder and losslessly compress a\n",
      "variety of synthetic, text and genomic datasets. The proposed compressor\n",
      "outperforms Gzip on the real datasets and achieves near-optimal compression for\n",
      "the synthetic datasets. The results also help understan\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Deep Lossless Compression Algorithm Based on Arithmetic Coding for Power Data\n",
      "URL: https://www.mdpi.com/1424-8220/22/14/5331\n",
      "ID: https://www.mdpi.com/1424-8220/22/14/5331\n",
      "Score: None\n",
      "Published Date: 2022-07-16T00:00:00.000Z\n",
      "Author: Fuyuan Song\n",
      "Image: https://pub.mdpi-res.com/sensors/sensors-22-05331/article_deploy/html/images/sensors-22-05331-g001-550.jpg?1657977118\n",
      "Favicon: https://pub.mdpi-res.com/img/mask-icon-128.svg\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Next Article in Journal\n",
      "\n",
      "[Reconstructing Superquadrics from Intensity and Color Images](https://www.mdpi.com/1424-8220/22/14/5332)\n",
      "\n",
      "Next Article in Special Issue\n",
      "\n",
      "[A Local Electricity and Carbon Trading Method for Multi-Energy Microgrids Considering Cross-Chain Interaction](https://www.mdpi.com/1424-8220/22/18/6935)\n",
      "\n",
      "Previous Article in Journal\n",
      "\n",
      "[Vision-Based Module for Herding with a Sheepdog Robot](https://www.mdpi.com/1424-8220/22/14/5321)\n",
      "\n",
      "## Journals\n",
      "\n",
      "[Active Journals](https://www.mdpi.com/about/journals) [Find a Journal](https://www.mdpi.com/about/journalfinder) [Proceedings Series](https://www.mdpi.com/about/proceedings)\n",
      "\n",
      "[**Topics**](https://www.mdpi.com/topics)\n",
      "\n",
      "## Information\n",
      "\n",
      "[For Authors](https://www.mdpi.com/authors) [For Reviewers](https://www.mdpi.com/reviewers) [For Editors](https://www.mdpi.com/editors) [For Librarians](https://www.mdpi.com/librarians) [For Publishers](https://www.mdpi.com/publishing_services) [For Societies](https://www.mdpi.com/societies) [For Confer\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: https://bellard.org/nncp/nncp.pdf\n",
      "ID: https://bellard.org/nncp/nncp.pdf\n",
      "Score: 0.37134554982185364\n",
      "Published Date: 2019-05-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Lossless Data Compression with Neural Networks\n",
      "Fabrice Bellard\n",
      "May 4, 2019\n",
      "Abstract\n",
      "We describe our implementation of a lossless data compressor using neu\u0002ral networks. We tuned Long Short-Term Memory and Transformer based\n",
      "models in order to achieve a fast training convergence. We evaluated the\n",
      "performance on the widely used enwik8 Hutter Prize benchmark.\n",
      "1 Introduction\n",
      "Although the best lossless data compressors are based on neural network models\n",
      "[7], they use them mostly to mix the statistics from a large number of hand coded\n",
      "models. Here we present lossless data compressors using pure neural network\n",
      "models based on Long Short-Term Memory (LSTM) and Transformer models.\n",
      "The lossless data compressor employs the traditional predictive approach: at\n",
      "each time t, the encoder uses the neural network model to compute the probability\n",
      "vector p of the next symbol value st knowing all the preceding symbols s0 up to\n",
      "st−1. The actual symbol value stis encoded using an arithmetic encoder with\n",
      "appro\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression Using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "ID: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "Score: 0.38720083236694336\n",
      "Published Date: 2019-03-01T00:00:00.000Z\n",
      "Author: Mohit Goyal,Kedar Tatwawadi,Shubham Chandak,Idoia Ochoa\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mohit Goyal goyal.mohit999@gmail.com \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "Kedar Tatwawadi kedart@stanford.edu \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Shubham Chandak \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Idoia Ochoa \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "\n",
      "Department of Electrical Engineering\n",
      "Indian Institute of Technology\n",
      "DelhiIndia\n",
      "\n",
      "DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "\n",
      "Sequential data is being generated at an unprecedented pace in various forms, including text and genomic data. This creates the need for efficient compression mechanisms to enable better storage, transmission and processing of such data. To solve this problem, many of the existing compressors attempt to learn models for the data and perform predictionbased compression. Since neural networks are known as universal function approximators with the capability to learn arbit\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: https://arxiv.org/pdf/1811.08162\n",
      "ID: https://arxiv.org/pdf/1811.08162\n",
      "Score: None\n",
      "Published Date: 2018-11-20T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: DeepZip: Lossless Data Compression using\n",
      "Recurrent Neural Networks\n",
      "Mohit Goyal†,γ, Kedar Tatwawadi∗, Shubham Chandak∗ and Idoia Ochoaγ\n",
      "†Department of Electrical Engineering, Indian Institute of Technology Delhi, India\n",
      "∗Department of Electrical Engineering, Stanford University, CA, USA\n",
      "γElectrical and Computer Engineering, University of Illinois, Urbana, IL, USA\n",
      "goyal.mohit999@gmail.com, kedart@stanford.edu\n",
      "Abstract\n",
      "Sequential data is being generated at an unprecedented pace in various forms, including text\n",
      "and genomic data. This creates the need for efficient compression mechanisms to enable\n",
      "better storage, transmission and processing of such data. To solve this problem, many\n",
      "of the existing compressors attempt to learn models for the data and perform prediction\u0002based compression. Since neural networks are known as universal function approximators\n",
      "with the capability to learn arbitrarily complex mappings, and in practice show excellent\n",
      "performance in prediction tasks, we explore and de\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.37147095799446106\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: TRACE: A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "URL: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "Score: 0.37043696641921997\n",
      "Published Date: 2022-04-25T00:00:00.000Z\n",
      "Author: Yu Mao,Yufei Cui,Tei-Wei Kuo,Chun Jason Xue\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Yu Mao\n",
      "Yufei Cui\n",
      "Tei-Wei Kuo\n",
      "Chun Jason Xue\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science and Information Engineering\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science\n",
      "National Taiwan University\n",
      "City University of Hong\n",
      "Kong\n",
      "A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "Deep-learning-based compressor has received interests recently due to much improved compression ratio. However, modern approaches suffer from long execution time. To ease this problem, this paper targets on cutting down the execution time of deep-learning-based compressors. Building historydependencies sequentially (e.g., recurrent neural networks) is responsible for long inference latency. Instead, we introduce transformer into deep learning compressors to build historydependencies in parallel. However, existing transformer is too heavy in computation and incomp\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: A Lossless Compression Technique for the Downlink Control Information Message\n",
      "URL: https://arxiv.org/abs/2407.16319\n",
      "ID: https://arxiv.org/abs/2407.16319\n",
      "Score: 0.37019482254981995\n",
      "Published Date: 2024-07-23T00:00:00.000Z\n",
      "Author: Liu; Bryan; Valcarce; Alvaro; Srinath; K Pavan\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "Improving the reliability and spectral efficiency of wireless systems is a key goal in wireless systems. However, most efforts have been devoted to improving data channel capacity, whereas control-plane capacity bottlenecks are often neglected. In this paper, we propose a means of improving the control-plane capacity and reliability by shrinking the bit size of a key signaling message - the 5G Downlink Control Information (DCI). In particular, a transformer model is studied as a probability distribution estimator for Arithmetic coding to achieve lossless compression. Feature engineering, neural model design, and training technique are comprehensively discussed in this paper. Both temporal and spatial correlations among DCI messages are explored by the transformer model to achieve reasonable lossless compression performance. Numerical results show that the proposed method achieves 21.7% higher compression ratio than Huffman coding in DCI compression for\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about general-purpose lossless compression for sequential data that could benefit from neural network modeling and arithmetic coding techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.014\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.009}\n",
      "\n",
      "Searching for papers related to: s41598-023-29068-z\n",
      "Title: Robust, practical and comprehensive analysis of soft compression image coding algorithms for big data\n",
      "URL: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "ID: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "Score: 0.43025484681129456\n",
      "Published Date: 2023-02-02T00:00:00.000Z\n",
      "Author: Gangtao Xin\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: 1\n",
      "Vol.:(0123456789)\n",
      "Scientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\n",
      "www.nature.com/scientificreports\n",
      "Robust, practical \n",
      "and comprehensive analysis \n",
      "of soft compression image coding \n",
      "algorithms for big data\n",
      "Gangtao Xin1,2 & Pingyi Fan1,2*\n",
      "With the advancement of intelligent vision algorithms and devices, image reprocessing and secondary \n",
      "propagation are becoming increasingly prevalent. A large number of similar images are being \n",
      "produced rapidly and widely, resulting in the homogeneity and similarity of images. Moreover, it \n",
      "brings new challenges to compression systems, which need to exploit the potential of deep features \n",
      "and side information of images. However, traditional methods are incompetent for this issue. Soft \n",
      "compression is a novel data-driven image coding algorithm with superior performance. Compared \n",
      "with existing paradigms, it has distinctive characteristics: from hard to soft, from pixels to shapes, and \n",
      "from fxed to random. Soft compressio\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "URL: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "ID: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "Score: 0.41191521286964417\n",
      "Published Date: 2021-12-25T00:00:00.000Z\n",
      "Author: Mu Li,Kede Ma,Jinxing Li,David Zhang\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mu Li\n",
      "Member, IEEEKede Ma\n",
      "Jinxing Li\n",
      "Life Fellow, IEEEDavid Zhang\n",
      "Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "1Index Terms-Omnidirectional image compressionpseudocylindrical representationpseudocylindrical convolutionmap projection!\n",
      "Although equirectangular projection (ERP) is a convenient form to store omnidirectional images (also known as 360°i mages), it is neither equal-area nor conformal, thus not friendly to subsequent visual communication. In the context of image compression, ERP will over-sample and deform things and stuff near the poles, making it difficult for perceptually optimal bit allocation. In conventional 360°image compression, techniques such as region-wise packing and tiled representation are introduced to alleviate the over-sampling problem, achieving limited success. In this paper, we make one of the first attempts to learn deep neural networks for omnidirectional image compression. We first describe parametric pseudocylindrical rep\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Foveation-based Deep Video Compression without Motion Search\n",
      "URL: https://arxiv.org/pdf/2203.16490.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16490.pdf\n",
      "Score: 0.41032645106315613\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: Meixu Chen,R. Webb,A. Bovik\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Foveation-based Deep Video Compression without Motion Search\n",
      "\n",
      "\n",
      "Student Member, IEEEMeixu Chen \n",
      "Richard Webb \n",
      "Fellow, IEEEAlan C Bovik \n",
      "Foveation-based Deep Video Compression without Motion Search\n",
      "1Index Terms-Foveationmotionvideo compression\n",
      "Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing, and displaying high-quality VR content. At the same time, the great potential of deep learning to advance progress on the video compression problem has driven a significant research effort. Because of the high bandwidth requirements of VR, there has also been significant interest in the use of space-variant, foveated compression protocols. We have integrated these techniques to create an endto-end deep learning video compression framework. A feature of our new com\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Texture Image Compression Algorithm Based on Self-Organizing Neural Network\n",
      "URL: https://downloads.hindawi.com/journals/cin/2022/4865808.pdf\n",
      "ID: https://downloads.hindawi.com/journals/cin/2022/4865808.pdf\n",
      "Score: 0.40787768363952637\n",
      "Published Date: 2022-04-10T00:00:00.000Z\n",
      "Author: Jianmin Han\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Jianmin Han hanjm@peczzu.edu.cn \n",
      "School of Computer Engineering\n",
      "Henan Economic and Trade Vocational College\n",
      "450046ZhengzhouHenanChina\n",
      "\n",
      "Texture Image Compression Algorithm Based on Self-Organizing Neural Network\n",
      "Published 10 April 202210.1155/2022/4865808Received 18 January 2022; Revised 21 February 2022; Accepted 28 February 2022;Research Article Correspondence should be addressed to Jianmin Han; Academic Editor: Shahid Mumtaz\n",
      "With the rapid development of science and technology, human beings have gradually stepped into a brand-new digital era. Virtual reality technology has brought people an immersive experience. In order to enable users to get a better virtual reality experience, the pictures produced by virtual skillfully must be realistic enough and support users' real-time interaction. So interactive realtime photorealistic rendering becomes the focus of research. Texture mapping is a technology proposed to solve the contradiction between real time and reality. It has been widely \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Deep Image Compression Using Scene Text Quality Assessment\n",
      "URL: https://export.arxiv.org/pdf/2305.11373v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2305.11373v1.pdf\n",
      "Score: 0.40701422095298767\n",
      "Published Date: 2023-01-01T00:00:00.000Z\n",
      "Author: Shohei Uchigasaki,Tomo Miyazaki,Shinichiro Omachi\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Shohei Uchigasaki \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Tomo Miyazaki \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Shinichiro Omachi \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Deep Image Compression Using Scene Text Quality Assessment\n",
      "19 May 2023Preprint submitted to Pattern Recognition May 22, 2023(a) Original Image (b) Image reconstructed using JPEG Figure 1: Results of high-compression image compression.image compressionscene text imagetext qualityquality assessmentregression model * Corresponding author Email addresses: uchiga@iiceceitohokuacjp (Shohei Uchigasaki)tomo@tohokuacjp (Tomo Miyazaki)machi@eceitohokuacjp (Shinichiro Omachi)\n",
      "Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resul\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "URL: https://arxiv.org/pdf/2309.11661.pdf\n",
      "ID: https://arxiv.org/pdf/2309.11661.pdf\n",
      "Score: 0.4045531749725342\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Wei  Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com, Wei  Wang, Futurewei Technologies Inc, Santa Clara, CA, Yue  Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "\n",
      "Date Published: 2023-09-20\n",
      "\n",
      "Authors:\n",
      "Wei Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com\n",
      "Wei Wang, Futurewei Technologies Inc, Santa Clara, CA\n",
      "Yue Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent fe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model\n",
      "URL: https://arxiv.org/abs/2402.16749\n",
      "ID: https://arxiv.org/abs/2402.16749\n",
      "Score: 0.4044540226459503\n",
      "Published Date: 2024-02-26T00:00:00.000Z\n",
      "Author: Li; Chunyi; Lu; Guo; Feng; Donghui; Wu; Haoning; Zhang; Zicheng; Xiaohong; Zhai; Guangtao; Weisi; Wenjun\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Download PDF \n",
      " HTML (experimental) \n",
      "With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Multi-Realism Image Compression with a Conditional Generator\n",
      "URL: https://arxiv.org/pdf/2212.13824.pdf\n",
      "ID: https://arxiv.org/pdf/2212.13824.pdf\n",
      "Score: 0.4043355882167816\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Eirikur  Agustsson, eirikur@google.com, Google  Research, David  Minnen, dminnen@google.com, George  Toderici, gtoderici@google.com, Fabian  Mentzer, mentzer@google.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Multi-Realism Image Compression with a Conditional Generator\n",
      "\n",
      "Date Published: 2023-03-30\n",
      "\n",
      "Authors:\n",
      "Eirikur Agustsson, Google Research Reykjavík, Iceland, eirikur@google.com\n",
      "David Minnen, Google Research Mountain View, USA, dminnen@google.com\n",
      "George Toderici, Google Research Mountain View, USA, gtoderici@google.com\n",
      "Fabian Mentzer, mentzer@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed represe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "URL: https://arxiv.org/pdf/2303.03028.pdf\n",
      "ID: https://arxiv.org/pdf/2303.03028.pdf\n",
      "Score: 0.40360027551651\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: B. Damodaran,M. Balcilar,Franck Galpin,P. Hellier\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "\n",
      "Bharath Bhushan Damodaran bharath.damodaran@interdigital.com \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Muhammet Balcilar \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Franck Galpin \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Pierre Hellier \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "Deep variational autoencoders for image and video compression have gained significant attraction in the recent years, due to their potential to offer competitive or better compression rates compared to the decades long traditional codecs such as AVC, HEVC or VVC. However, because of complexity and energy consumption, these approaches are still far away from practical usage in industry. More recently, implicit neural representation (INR) based codecs have emerged, and have lower complexity and energy usage to classical approaches at decoding. However, their performances are not in par at the moment with state-of-the-art methods. In this researc\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "URL: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "Score: 0.3996758460998535\n",
      "Published Date: 2023-09-05T00:00:00.000Z\n",
      "Author: Haisheng Fu,Feng Li,Jie Liang,Yongqiang Wang,Guohe Zhang,Jingning Han\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Haisheng Fu\n",
      "Feng Liang\n",
      "Jie Liang\n",
      "Yongqiang Wang\n",
      "Guohe Zhang\n",
      "Jingning Han\n",
      "Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "SUBMITTED TO TRANS. JOURNAL 1\n",
      "Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the ratedistortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the tradeoff between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design an improved checkerboard context model with two separate distribution parameter estimati\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about image compression techniques for virtual reality, augmented reality, and metaverse applications that could benefit from soft compression algorithms with superior compression ratios\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Search results:\n",
      "Title: Robust, practical and comprehensive analysis of soft compression image coding algorithms for big data\n",
      "URL: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "ID: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "Score: 0.43025484681129456\n",
      "Published Date: 2023-02-02T00:00:00.000Z\n",
      "Author: Gangtao Xin\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: 1\n",
      "Vol.:(0123456789)\n",
      "Scientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\n",
      "www.nature.com/scientificreports\n",
      "Robust, practical \n",
      "and comprehensive analysis \n",
      "of soft compression image coding \n",
      "algorithms for big data\n",
      "Gangtao Xin1,2 & Pingyi Fan1,2*\n",
      "With the advancement of intelligent vision algorithms and devices, image reprocessing and secondary \n",
      "propagation are becoming increasingly prevalent. A large number of similar images are being \n",
      "produced rapidly and widely, resulting in the homogeneity and similarity of images. Moreover, it \n",
      "brings new challenges to compression systems, which need to exploit the potential of deep features \n",
      "and side information of images. However, traditional methods are incompetent for this issue. Soft \n",
      "compression is a novel data-driven image coding algorithm with superior performance. Compared \n",
      "with existing paradigms, it has distinctive characteristics: from hard to soft, from pixels to shapes, and \n",
      "from fxed to random. Soft compressio\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "URL: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "ID: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "Score: 0.41191521286964417\n",
      "Published Date: 2021-12-25T00:00:00.000Z\n",
      "Author: Mu Li,Kede Ma,Jinxing Li,David Zhang\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mu Li\n",
      "Member, IEEEKede Ma\n",
      "Jinxing Li\n",
      "Life Fellow, IEEEDavid Zhang\n",
      "Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "1Index Terms-Omnidirectional image compressionpseudocylindrical representationpseudocylindrical convolutionmap projection!\n",
      "Although equirectangular projection (ERP) is a convenient form to store omnidirectional images (also known as 360°i mages), it is neither equal-area nor conformal, thus not friendly to subsequent visual communication. In the context of image compression, ERP will over-sample and deform things and stuff near the poles, making it difficult for perceptually optimal bit allocation. In conventional 360°image compression, techniques such as region-wise packing and tiled representation are introduced to alleviate the over-sampling problem, achieving limited success. In this paper, we make one of the first attempts to learn deep neural networks for omnidirectional image compression. We first describe parametric pseudocylindrical rep\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Foveation-based Deep Video Compression without Motion Search\n",
      "URL: https://arxiv.org/pdf/2203.16490.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16490.pdf\n",
      "Score: 0.41032645106315613\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: Meixu Chen,R. Webb,A. Bovik\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Foveation-based Deep Video Compression without Motion Search\n",
      "\n",
      "\n",
      "Student Member, IEEEMeixu Chen \n",
      "Richard Webb \n",
      "Fellow, IEEEAlan C Bovik \n",
      "Foveation-based Deep Video Compression without Motion Search\n",
      "1Index Terms-Foveationmotionvideo compression\n",
      "Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing, and displaying high-quality VR content. At the same time, the great potential of deep learning to advance progress on the video compression problem has driven a significant research effort. Because of the high bandwidth requirements of VR, there has also been significant interest in the use of space-variant, foveated compression protocols. We have integrated these techniques to create an endto-end deep learning video compression framework. A feature of our new com\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Texture Image Compression Algorithm Based on Self-Organizing Neural Network\n",
      "URL: https://downloads.hindawi.com/journals/cin/2022/4865808.pdf\n",
      "ID: https://downloads.hindawi.com/journals/cin/2022/4865808.pdf\n",
      "Score: 0.40787768363952637\n",
      "Published Date: 2022-04-10T00:00:00.000Z\n",
      "Author: Jianmin Han\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Jianmin Han hanjm@peczzu.edu.cn \n",
      "School of Computer Engineering\n",
      "Henan Economic and Trade Vocational College\n",
      "450046ZhengzhouHenanChina\n",
      "\n",
      "Texture Image Compression Algorithm Based on Self-Organizing Neural Network\n",
      "Published 10 April 202210.1155/2022/4865808Received 18 January 2022; Revised 21 February 2022; Accepted 28 February 2022;Research Article Correspondence should be addressed to Jianmin Han; Academic Editor: Shahid Mumtaz\n",
      "With the rapid development of science and technology, human beings have gradually stepped into a brand-new digital era. Virtual reality technology has brought people an immersive experience. In order to enable users to get a better virtual reality experience, the pictures produced by virtual skillfully must be realistic enough and support users' real-time interaction. So interactive realtime photorealistic rendering becomes the focus of research. Texture mapping is a technology proposed to solve the contradiction between real time and reality. It has been widely \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Deep Image Compression Using Scene Text Quality Assessment\n",
      "URL: https://export.arxiv.org/pdf/2305.11373v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2305.11373v1.pdf\n",
      "Score: 0.40701422095298767\n",
      "Published Date: 2023-01-01T00:00:00.000Z\n",
      "Author: Shohei Uchigasaki,Tomo Miyazaki,Shinichiro Omachi\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Shohei Uchigasaki \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Tomo Miyazaki \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Shinichiro Omachi \n",
      "Graduate School of Engineering\n",
      "Tohoku University\n",
      "6-6-05, Aoba Aramaki980-8579Aoba, Sendai, MiyagiJapan\n",
      "\n",
      "Deep Image Compression Using Scene Text Quality Assessment\n",
      "19 May 2023Preprint submitted to Pattern Recognition May 22, 2023(a) Original Image (b) Image reconstructed using JPEG Figure 1: Results of high-compression image compression.image compressionscene text imagetext qualityquality assessmentregression model * Corresponding author Email addresses: uchiga@iiceceitohokuacjp (Shohei Uchigasaki)tomo@tohokuacjp (Tomo Miyazaki)machi@eceitohokuacjp (Shinichiro Omachi)\n",
      "Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resul\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "URL: https://arxiv.org/pdf/2309.11661.pdf\n",
      "ID: https://arxiv.org/pdf/2309.11661.pdf\n",
      "Score: 0.4045531749725342\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Wei  Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com, Wei  Wang, Futurewei Technologies Inc, Santa Clara, CA, Yue  Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "\n",
      "Date Published: 2023-09-20\n",
      "\n",
      "Authors:\n",
      "Wei Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com\n",
      "Wei Wang, Futurewei Technologies Inc, Santa Clara, CA\n",
      "Yue Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent fe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model\n",
      "URL: https://arxiv.org/abs/2402.16749\n",
      "ID: https://arxiv.org/abs/2402.16749\n",
      "Score: 0.4044540226459503\n",
      "Published Date: 2024-02-26T00:00:00.000Z\n",
      "Author: Li; Chunyi; Lu; Guo; Feng; Donghui; Wu; Haoning; Zhang; Zicheng; Xiaohong; Zhai; Guangtao; Weisi; Wenjun\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Download PDF \n",
      " HTML (experimental) \n",
      "With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Multi-Realism Image Compression with a Conditional Generator\n",
      "URL: https://arxiv.org/pdf/2212.13824.pdf\n",
      "ID: https://arxiv.org/pdf/2212.13824.pdf\n",
      "Score: 0.4043355882167816\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Eirikur  Agustsson, eirikur@google.com, Google  Research, David  Minnen, dminnen@google.com, George  Toderici, gtoderici@google.com, Fabian  Mentzer, mentzer@google.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Multi-Realism Image Compression with a Conditional Generator\n",
      "\n",
      "Date Published: 2023-03-30\n",
      "\n",
      "Authors:\n",
      "Eirikur Agustsson, Google Research Reykjavík, Iceland, eirikur@google.com\n",
      "David Minnen, Google Research Mountain View, USA, dminnen@google.com\n",
      "George Toderici, Google Research Mountain View, USA, gtoderici@google.com\n",
      "Fabian Mentzer, mentzer@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed represe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "URL: https://arxiv.org/pdf/2303.03028.pdf\n",
      "ID: https://arxiv.org/pdf/2303.03028.pdf\n",
      "Score: 0.40360027551651\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: B. Damodaran,M. Balcilar,Franck Galpin,P. Hellier\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "\n",
      "Bharath Bhushan Damodaran bharath.damodaran@interdigital.com \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Muhammet Balcilar \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Franck Galpin \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Pierre Hellier \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "Deep variational autoencoders for image and video compression have gained significant attraction in the recent years, due to their potential to offer competitive or better compression rates compared to the decades long traditional codecs such as AVC, HEVC or VVC. However, because of complexity and energy consumption, these approaches are still far away from practical usage in industry. More recently, implicit neural representation (INR) based codecs have emerged, and have lower complexity and energy usage to classical approaches at decoding. However, their performances are not in par at the moment with state-of-the-art methods. In this researc\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "URL: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "Score: 0.3996758460998535\n",
      "Published Date: 2023-09-05T00:00:00.000Z\n",
      "Author: Haisheng Fu,Feng Li,Jie Liang,Yongqiang Wang,Guohe Zhang,Jingning Han\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Haisheng Fu\n",
      "Feng Liang\n",
      "Jie Liang\n",
      "Yongqiang Wang\n",
      "Guohe Zhang\n",
      "Jingning Han\n",
      "Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "SUBMITTED TO TRANS. JOURNAL 1\n",
      "Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the ratedistortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the tradeoff between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design an improved checkerboard context model with two separate distribution parameter estimati\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about image compression techniques for virtual reality, augmented reality, and metaverse applications that could benefit from soft compression algorithms with superior compression ratios\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "\n",
      "Searching for papers related to: 2304.01106v1\n",
      "Title: An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "URL: https://arxiv.org/abs/2412.15250\n",
      "ID: https://arxiv.org/abs/2412.15250\n",
      "Score: 0.3578655421733856\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Computation and Language\n",
      "\n",
      "**arXiv:2412.15250** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "\n",
      "Authors: [Chowdhury Mofizur Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman,+C+M), [Mahbub E Sobhani](https://arxiv.org/search/cs?searchtype=author&query=Sobhani,+M+E), [Anika Tasnim Rodela](https://arxiv.org/search/cs?searchtype=author&query=Rodela,+A+T), [Swakkhar Shatabda](https://arxiv.org/search/cs?searchtype=author&query=Shatabda,+S)\n",
      "\n",
      "View a PDF of the paper titled An Enhanced Text Compression Approach Using Transformer-based Language Models, by Chowdhury Mofizur Rahman and Mahbub E Sobhani and Anika Tasnim Rodela and Swakkhar Shatabda\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.15250) [HTML (experimental)](https://arxiv.org/html/2412.15250v1)\n",
      "\n",
      "> Abstract:Text compression shrinks textual data while keeping crucial information, eradicating constraints on storage, bandwidth, and c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.3218926787376404\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Extending Context Window of Large Language Models via Semantic Compression\n",
      "URL: https://browse.arxiv.org/html/2312.09571v1\n",
      "ID: https://browse.arxiv.org/html/2312.09571v1\n",
      "Score: 0.3260715901851654\n",
      "Published Date: 2023-12-15T00:00:00.000Z\n",
      "Author: Correspondence to: niuxueyan3@huawei.com.\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: License: CC BY-NC-SA 4.0\n",
      "\n",
      "arXiv:2312.09571v1 \\[cs.CL\\] 15 Dec 2023\n",
      "\n",
      "# Extending Context Window of Large Language Models via Semantic Compression\n",
      "\n",
      "Weizhi Fei††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT & Xueyan Niu  ‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT\n",
      "\n",
      "††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT Department of Mathematical Sciences, Tsinghua University, Beijing, China\n",
      "\n",
      "‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.\n",
      "\n",
      "\\\\ANDPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han\n",
      "\n",
      "Huawei Technologies Co., Ltd\n",
      "Correspondence to: niuxueyan3@huawei.com.\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Crossword: A Semantic Approach to Data Compression via Masking\n",
      "URL: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "Score: 0.35012781620025635\n",
      "Published Date: 2023-04-03T00:00:00.000Z\n",
      "Author: Mingxiao Li,Rui Jin,Liyao Xiang,Kaiming Shen,Shuguang Cui\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mingxiao Li mingxiaoli@link.cuhk.edu.cn \n",
      "† \n",
      "Rui Jin ruijin@link.cuhk.edu.cn \n",
      "Liyao Xiang xiangliyao08@sjtu.edu.cn \n",
      "Shanghai Jiao Tong University\n",
      "China\n",
      "\n",
      "Kaiming Shen shenkaiming@cuhk.edu.cn \n",
      "Shuguang Cui shuguangcui@cuhk.edu.cn \n",
      "\n",
      "School of Science and Engineering\n",
      "FNii\n",
      "The Chinese University of Hong Kong (Shenzhen)\n",
      "China\n",
      "\n",
      "Crossword: A Semantic Approach to Data Compression via Masking\n",
      "3 Apr 2023Index Terms-Semantic source codingdata compressionword maskingcosine distanceTransformer\n",
      "The traditional methods for data compression are typically based on the symbol-level statistics, with the information source modeled as a long sequence of i.i.d. random variables or a stochastic process, thus establishing the fundamental limit as entropy for lossless compression and as mutual information for lossy compression. However, the source (including text, music, and speech) in the real world is often statistically ill-defined because of its close connection to human perception, and thus the model-driven\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Text Compression-Aided Transformer Encoding\n",
      "URL: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "ID: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "Score: 0.36277955770492554\n",
      "Published Date: 2021-01-01T00:00:00.000Z\n",
      "Author: Zuchao Li,Zhuosheng Zhang,Hai Zhao,Rui Wang,Kehai Chen,Masao Utiyama,Eiichiro Sumita\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Zuchao Li \n",
      "Zhuosheng Zhang \n",
      "Hai Zhao \n",
      "Rui Wang \n",
      "Kehai Chen \n",
      "Masao Utiyama \n",
      "Eiichiro Sumita \n",
      "Text Compression-aided Transformer Encoding\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, MARCH 2020 1 !Index Terms-Natural Language ProcessingText CompressionTransformer EncodingNeural Machine TranslationMachine Reading Comprehension\n",
      "Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "URL: https://arxiv.org/abs/2412.12204\n",
      "ID: https://arxiv.org/abs/2412.12204\n",
      "Score: 0.33777424693107605\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Machine Learning\n",
      "\n",
      "**arXiv:2412.12204** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "\n",
      "Authors: [Jing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Shuzhen Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+S), [Peng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+P), [Guangxing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao,+G), [Hui Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+H), [Xindian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+X), [Nan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+N), [Yuexian Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou,+Y)\n",
      "\n",
      "View a PDF of the paper titled SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression, by Jing Zhang and 7 other authors\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.12204) [HTML (experimental)](https://ar\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "URL: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "Score: 0.3293420374393463\n",
      "Published Date: 2023-07-23T00:00:00.000Z\n",
      "Author: Shicong Liu,Zhen Gao,Gaojie Chen,Yu Shibata,Peixiang Lu\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Shicong Liu scliu@bit.edu.cn\n",
      "Zhen Gao gaozhen16@bit.edu.cn\n",
      "Advanced Research Institute of Multidisciplinary Science\n",
      "Beijing Institute of Technology\n",
      "250307JinanChina\n",
      "Yangtze Delta Region Academy of Beijing Institute of Technology\n",
      "Beijing Institute of Technology\n",
      "314000JiaxingChina\n",
      "† ‡\n",
      "Gaojie Chen\n",
      "Institute for Communication Systems\n",
      "University of Surrey\n",
      "GuildfordUnited Kingdom\n",
      "Yu Su\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "¶\n",
      "Lu Peng\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "Field Intelligent Sensing\n",
      "MIIT Key Laboratory of Complex\n",
      "Beiing Institute of Technology\n",
      "100081BeijingChina\n",
      "Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "Index Terms-Semantic communciationtransformerjoint source channel codingpretrained language model\n",
      "The Space-Air-Ground-Sea integrated network calls for more robust and secure transmission techniques against jamming. In this paper, we propose a t\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Semantic Compression With Large Language Models\n",
      "URL: https://arxiv.org/abs/2304.12512\n",
      "ID: https://arxiv.org/abs/2304.12512\n",
      "Score: 0.36365997791290283\n",
      "Published Date: 2023-05-16T00:00:00.000Z\n",
      "Author: Gilbert; Henry; Sandborn; Michael; Schmidt; Douglas C; Spencer-Smith; Jesse; White; Jules\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: The rise of large language models (LLMs) is revolutionizing information\n",
      "retrieval, question answering, summarization, and code generation tasks.\n",
      "However, in addition to confidently presenting factually inaccurate information\n",
      "at times (known as \"hallucinations\"), LLMs are also inherently limited by the\n",
      "number of input and output tokens that can be processed at once, making them\n",
      "potentially less effective on tasks that require processing a large set or\n",
      "continuous stream of information. A common approach to reducing the size of\n",
      "data is through lossless or lossy compression. Yet, in some cases it may not be\n",
      "strictly necessary to perfectly recover every detail from the original data, as\n",
      "long as a requisite level of semantic precision or intent is conveyed.\n",
      " This paper presents three contributions to research on LLMs. First, we\n",
      "present the results from experiments exploring the viability of approximate\n",
      "compression using LLMs, focusing specifically\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
      "URL: https://arxiv.org/abs/2406.02376\n",
      "ID: https://arxiv.org/abs/2406.02376\n",
      "Score: 0.320854514837265\n",
      "Published Date: 2024-06-04T00:00:00.000Z\n",
      "Author: Cao; Zhiwei; Qian; Lu; Yu; Peng; Ningxin; Huang; Cheng; Shanbo; Su; Jinsong\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Tiny Transformers Excel at Sentence Compression\n",
      "URL: https://arxiv.org/abs/2410.23510\n",
      "ID: https://arxiv.org/abs/2410.23510\n",
      "Score: 0.3383108973503113\n",
      "Published Date: 2024-10-30T00:00:00.000Z\n",
      "Author: Belcak; Peter; Wattenhofer; Roger\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "It is staggering that words of the English language, which are on average represented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served to large language models. We show that there is room for more information in every token embedding. We demonstrate that 1--3-layer transformers are capable of encoding and subsequently decoding standard English sentences into as little as a single 3-kilobyte token. Our work implies that even small networks can learn to construct valid English sentences and suggests the possibility of optimising large language models by moving from sub-word token embeddings towards larger fragments of text.\n",
      " \n",
      " \n",
      " Submission history From: Peter Belcak [ view email] [v1] \n",
      "Wed, 30 Oct 2024 23:34:45 UTC (2,194 KB) \n",
      "\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about semantic text compression techniques that could benefit from BERT and Transformer models for preserving meaning in literature and political documents\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Search results:\n",
      "Title: An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "URL: https://arxiv.org/abs/2412.15250\n",
      "ID: https://arxiv.org/abs/2412.15250\n",
      "Score: 0.3578655421733856\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Computation and Language\n",
      "\n",
      "**arXiv:2412.15250** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "\n",
      "Authors: [Chowdhury Mofizur Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman,+C+M), [Mahbub E Sobhani](https://arxiv.org/search/cs?searchtype=author&query=Sobhani,+M+E), [Anika Tasnim Rodela](https://arxiv.org/search/cs?searchtype=author&query=Rodela,+A+T), [Swakkhar Shatabda](https://arxiv.org/search/cs?searchtype=author&query=Shatabda,+S)\n",
      "\n",
      "View a PDF of the paper titled An Enhanced Text Compression Approach Using Transformer-based Language Models, by Chowdhury Mofizur Rahman and Mahbub E Sobhani and Anika Tasnim Rodela and Swakkhar Shatabda\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.15250) [HTML (experimental)](https://arxiv.org/html/2412.15250v1)\n",
      "\n",
      "> Abstract:Text compression shrinks textual data while keeping crucial information, eradicating constraints on storage, bandwidth, and c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.3218926787376404\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Extending Context Window of Large Language Models via Semantic Compression\n",
      "URL: https://browse.arxiv.org/html/2312.09571v1\n",
      "ID: https://browse.arxiv.org/html/2312.09571v1\n",
      "Score: 0.3260715901851654\n",
      "Published Date: 2023-12-15T00:00:00.000Z\n",
      "Author: Correspondence to: niuxueyan3@huawei.com.\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: License: CC BY-NC-SA 4.0\n",
      "\n",
      "arXiv:2312.09571v1 \\[cs.CL\\] 15 Dec 2023\n",
      "\n",
      "# Extending Context Window of Large Language Models via Semantic Compression\n",
      "\n",
      "Weizhi Fei††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT & Xueyan Niu  ‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT\n",
      "\n",
      "††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT Department of Mathematical Sciences, Tsinghua University, Beijing, China\n",
      "\n",
      "‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.\n",
      "\n",
      "\\\\ANDPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han\n",
      "\n",
      "Huawei Technologies Co., Ltd\n",
      "Correspondence to: niuxueyan3@huawei.com.\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Crossword: A Semantic Approach to Data Compression via Masking\n",
      "URL: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "Score: 0.35012781620025635\n",
      "Published Date: 2023-04-03T00:00:00.000Z\n",
      "Author: Mingxiao Li,Rui Jin,Liyao Xiang,Kaiming Shen,Shuguang Cui\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mingxiao Li mingxiaoli@link.cuhk.edu.cn \n",
      "† \n",
      "Rui Jin ruijin@link.cuhk.edu.cn \n",
      "Liyao Xiang xiangliyao08@sjtu.edu.cn \n",
      "Shanghai Jiao Tong University\n",
      "China\n",
      "\n",
      "Kaiming Shen shenkaiming@cuhk.edu.cn \n",
      "Shuguang Cui shuguangcui@cuhk.edu.cn \n",
      "\n",
      "School of Science and Engineering\n",
      "FNii\n",
      "The Chinese University of Hong Kong (Shenzhen)\n",
      "China\n",
      "\n",
      "Crossword: A Semantic Approach to Data Compression via Masking\n",
      "3 Apr 2023Index Terms-Semantic source codingdata compressionword maskingcosine distanceTransformer\n",
      "The traditional methods for data compression are typically based on the symbol-level statistics, with the information source modeled as a long sequence of i.i.d. random variables or a stochastic process, thus establishing the fundamental limit as entropy for lossless compression and as mutual information for lossy compression. However, the source (including text, music, and speech) in the real world is often statistically ill-defined because of its close connection to human perception, and thus the model-driven\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Text Compression-Aided Transformer Encoding\n",
      "URL: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "ID: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "Score: 0.36277955770492554\n",
      "Published Date: 2021-01-01T00:00:00.000Z\n",
      "Author: Zuchao Li,Zhuosheng Zhang,Hai Zhao,Rui Wang,Kehai Chen,Masao Utiyama,Eiichiro Sumita\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Zuchao Li \n",
      "Zhuosheng Zhang \n",
      "Hai Zhao \n",
      "Rui Wang \n",
      "Kehai Chen \n",
      "Masao Utiyama \n",
      "Eiichiro Sumita \n",
      "Text Compression-aided Transformer Encoding\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, MARCH 2020 1 !Index Terms-Natural Language ProcessingText CompressionTransformer EncodingNeural Machine TranslationMachine Reading Comprehension\n",
      "Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "URL: https://arxiv.org/abs/2412.12204\n",
      "ID: https://arxiv.org/abs/2412.12204\n",
      "Score: 0.33777424693107605\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Machine Learning\n",
      "\n",
      "**arXiv:2412.12204** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "\n",
      "Authors: [Jing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Shuzhen Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+S), [Peng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+P), [Guangxing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao,+G), [Hui Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+H), [Xindian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+X), [Nan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+N), [Yuexian Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou,+Y)\n",
      "\n",
      "View a PDF of the paper titled SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression, by Jing Zhang and 7 other authors\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.12204) [HTML (experimental)](https://ar\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "URL: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "Score: 0.3293420374393463\n",
      "Published Date: 2023-07-23T00:00:00.000Z\n",
      "Author: Shicong Liu,Zhen Gao,Gaojie Chen,Yu Shibata,Peixiang Lu\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Shicong Liu scliu@bit.edu.cn\n",
      "Zhen Gao gaozhen16@bit.edu.cn\n",
      "Advanced Research Institute of Multidisciplinary Science\n",
      "Beijing Institute of Technology\n",
      "250307JinanChina\n",
      "Yangtze Delta Region Academy of Beijing Institute of Technology\n",
      "Beijing Institute of Technology\n",
      "314000JiaxingChina\n",
      "† ‡\n",
      "Gaojie Chen\n",
      "Institute for Communication Systems\n",
      "University of Surrey\n",
      "GuildfordUnited Kingdom\n",
      "Yu Su\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "¶\n",
      "Lu Peng\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "Field Intelligent Sensing\n",
      "MIIT Key Laboratory of Complex\n",
      "Beiing Institute of Technology\n",
      "100081BeijingChina\n",
      "Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "Index Terms-Semantic communciationtransformerjoint source channel codingpretrained language model\n",
      "The Space-Air-Ground-Sea integrated network calls for more robust and secure transmission techniques against jamming. In this paper, we propose a t\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Semantic Compression With Large Language Models\n",
      "URL: https://arxiv.org/abs/2304.12512\n",
      "ID: https://arxiv.org/abs/2304.12512\n",
      "Score: 0.36365997791290283\n",
      "Published Date: 2023-05-16T00:00:00.000Z\n",
      "Author: Gilbert; Henry; Sandborn; Michael; Schmidt; Douglas C; Spencer-Smith; Jesse; White; Jules\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: The rise of large language models (LLMs) is revolutionizing information\n",
      "retrieval, question answering, summarization, and code generation tasks.\n",
      "However, in addition to confidently presenting factually inaccurate information\n",
      "at times (known as \"hallucinations\"), LLMs are also inherently limited by the\n",
      "number of input and output tokens that can be processed at once, making them\n",
      "potentially less effective on tasks that require processing a large set or\n",
      "continuous stream of information. A common approach to reducing the size of\n",
      "data is through lossless or lossy compression. Yet, in some cases it may not be\n",
      "strictly necessary to perfectly recover every detail from the original data, as\n",
      "long as a requisite level of semantic precision or intent is conveyed.\n",
      " This paper presents three contributions to research on LLMs. First, we\n",
      "present the results from experiments exploring the viability of approximate\n",
      "compression using LLMs, focusing specifically\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
      "URL: https://arxiv.org/abs/2406.02376\n",
      "ID: https://arxiv.org/abs/2406.02376\n",
      "Score: 0.320854514837265\n",
      "Published Date: 2024-06-04T00:00:00.000Z\n",
      "Author: Cao; Zhiwei; Qian; Lu; Yu; Peng; Ningxin; Huang; Cheng; Shanbo; Su; Jinsong\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Tiny Transformers Excel at Sentence Compression\n",
      "URL: https://arxiv.org/abs/2410.23510\n",
      "ID: https://arxiv.org/abs/2410.23510\n",
      "Score: 0.3383108973503113\n",
      "Published Date: 2024-10-30T00:00:00.000Z\n",
      "Author: Belcak; Peter; Wattenhofer; Roger\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "It is staggering that words of the English language, which are on average represented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served to large language models. We show that there is room for more information in every token embedding. We demonstrate that 1--3-layer transformers are capable of encoding and subsequently decoding standard English sentences into as little as a single 3-kilobyte token. Our work implies that even small networks can learn to construct valid English sentences and suggests the possibility of optimising large language models by moving from sub-word token embeddings towards larger fragments of text.\n",
      " \n",
      " \n",
      " Submission history From: Peter Belcak [ view email] [v1] \n",
      "Wed, 30 Oct 2024 23:34:45 UTC (2,194 KB) \n",
      "\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about semantic text compression techniques that could benefit from BERT and Transformer models for preserving meaning in literature and political documents\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Title: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.07342v2.pdf\n",
      "Score: 0.4447816014289856\n",
      "Published Date: 2023-06-21T00:00:00.000Z\n",
      "Author: Boyuan Zhang,Jiannan Tian,Sheng Di,Xiangyao Yu,Martin Swany,Dingwen Tao,Franck Cappello\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Boyuan Zhang bozhan@iu.edu\n",
      "Jiannan Tian\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Martin Swany swany@indiana.edu\n",
      "Dingwen Tao ditao@iu.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Boyuan Zhang\n",
      "Jiannan Tian\n",
      "XiaodongSheng Di\n",
      "Yu\n",
      "Martin Swany\n",
      "Ding-Wen Tao\n",
      "Franck Cappello\n",
      "Indiana University Bloomington\n",
      "INUSA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IL, INUSA, USA\n",
      "Department of Intelligent Systems Engineering, Luddy School of Informatics, Computing, and Engineering\n",
      "Argonne National Laboratory Lemont\n",
      "Indiana University Bloomington\n",
      "IN, ILUSA, USA\n",
      "Indiana University\n",
      "gpuLZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "2023 International Conference on Supercomputing (ICS '23)\n",
      "Orlando, FL, USA 2023; Orlando, FL, USA; New York, NY, USAACM12June 21-23, 2023. June 21-23, 202310.1145/3577193.3593706ACM acknowledges that this contribution was authored or co-authored by an employee, contractor, or \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: GitHub - hipdac-lab/ICS23-GPULZ: GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "URL: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "ID: https://github.com/hipdac-lab/ICS23-GPULZ\n",
      "Score: 0.36544671654701233\n",
      "Published Date: 2023-04-11T00:00:00.000Z\n",
      "Author: hipdac-lab\n",
      "Image: https://opengraph.githubassets.com/3bcd20c09c13ef98258430e4e4835415b7941df14b40919298e813daa3c87c41/hipdac-lab/ICS23-GPULZ\n",
      "Favicon: https://github.com/fluidicon.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: [Skip to content](https://github.com/hipdac-lab/ICS23-GPULZ#start-of-content)\n",
      "\n",
      "You signed in with another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/hipdac-lab/ICS23-GPULZ) to refresh your session.Dismiss alert\n",
      "\n",
      "{{ message }}\n",
      "\n",
      "[hipdac-lab](https://github.com/hipdac-lab)/ **[ICS23-GPULZ](https://github.com/hipdac-lab/ICS23-GPULZ)** Public\n",
      "\n",
      "- [Notifications](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ) You must be signed in to change notification settings\n",
      "- [Fork\\\n",
      "0](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "- [Star\\\n",
      "13](https://github.com/login?return_to=%2Fhipdac-lab%2FICS23-GPULZ)\n",
      "\n",
      "\n",
      "GPULZ: Optimizing LZSS Lossless Compression for Multi-byte Data on Modern GPUs\n",
      "\n",
      "[13\\\n",
      "stars](https://github.com/hipdac-lab/ICS23\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Light Loss-Less Data Compression, with GPU Implementation\n",
      "URL: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "ID: https://link.springer.com/chapter/10.1007/978-3-319-49583-5_22\n",
      "Score: 0.36278703808784485\n",
      "Published Date: 2016-11-05T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, funasaka@cs.hiroshima-u.ac.jp, Koji  Nakano, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito, Hiroshima University, Kagamiyama 1-4-1, 739-8527, Higashihiroshima, Japan, yasuaki@cs.hiroshima-u.ac.jp\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: There is no doubt that data compression is very important in computer engineering. However, most lossless data compression and decompression algorithms are very hard to parallelize, because they use dictionaries updated sequentially. The main contribution of this paper is to present a new lossless data compression method that we call Light Loss-Less (LLL) compression. It is designed so that decompression can be highly parallelized and run very efficiently on the GPU. This makes sense for many applications in which compressed data is read and decompressed many times and decompression performed more frequently than compression. We show optimal sequential and parallel algorithms for LLL decompression and implement them to run on Core i7-4790 CPU and GeForce GTX 1080 GPU, respectively. To show the potentiality of LLL compression method, we have evaluated the running time using five images and compared with well-known compression methods LZW and LZSS. Our GPU implementation of LLL decompres\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Adaptive loss‐less data compression method optimized for GPU decompression\n",
      "URL: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "ID: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "Score: 0.3729551434516907\n",
      "Published Date: 2017-08-17T00:00:00.000Z\n",
      "Author: Shunji  Funasaka, Koji  Nakano, nakano@cs.hiroshima-u.ac.jp, Yasuaki  Ito\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Adaptive loss‐less data compression method optimized for GPU decompression - Funasaka - 2017 - Concurrency and Computation: Practice and Experience - Wiley Online Library\n",
      "\n",
      "Opens in a new windowOpens an external websiteOpens an external website in a new window\n",
      "\n",
      "Close this dialog\n",
      "\n",
      "This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link: [Privacy Policy](https://www.wiley.com/privacy)\n",
      "\n",
      "Close Cookie Preferences\n",
      "\n",
      "[Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "[Volume 29, Issue 24](https://onlinelibrary.wiley.com/toc/15320634/2017/29/24) e4283 [![Concurrency and Computation: Practice and Experience](https://onlinelibrary.wiley.com/pb-assets/journal-banners/15320634-1501384732273.jpg)](https://onlinelibrary.wiley.com/journal/15320634)\n",
      "\n",
      "SPECIAL ISSUE PAPER\n",
      "\n",
      "# Adaptive loss-less data compression \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "ID: http://essay.utwente.nl/91724/1/Gorgan_BA_TCS.pdf\n",
      "Score: None\n",
      "Published Date: 2022-07-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Enabling faster processing of big-data using GPU\n",
      "decompression\n",
      "Andrei Gorgan\n",
      "a.gorgan@student.utwente.nl\n",
      "University of Twente\n",
      "Enschede, The Netherlands\n",
      "Abstract\n",
      "Processing big-data has been shown to have a many fold\n",
      "speedup for GPU hardware, however the process of retriev\u0002ing ready-to-use data from storage devices still requires a\n",
      "process of decompression, currently performed on the CPU.\n",
      "Due to the increasing computational power of GPUs, the\n",
      "decompression step starves the GPU of data, effectively do\u0002ing nothing until more data is available to process. This\n",
      "research analyses the minimum required speed of decom\u0002pression on a GPU, such that offloading the decompression\n",
      "step to the GPU, is faster than traditional methods that uti\u0002lize the CPU. Results show that GPUs cannot outperform\n",
      "CPUs when considering compression ratio, however the im\u0002proved parallelism of GPUs allows for a 2 times reduction in\n",
      "decompression times.\n",
      "Keywords: GPGPU, bypassing CPU decompression, big-data\n",
      "compression and \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: CODAG: Characterizing and Optimizing Decompression Algorithms for GPUs\n",
      "URL: https://arxiv.org/abs/2307.03760\n",
      "ID: https://arxiv.org/abs/2307.03760\n",
      "Score: 0.3703548312187195\n",
      "Published Date: 2023-12-04T00:00:00.000Z\n",
      "Author: Park; Jeongmin; Qureshi; Zaid; Mailthody; Vikram; Gacek; Andrew; Shao; Shunfan; AlMasri; Mohammad; Gelado; Isaac; Xiong; Jinjun; Newburn; Chris; Chung; I-hsin; Garland; Michael; Sakharnykh; Nikolay; Hwu; Wen-mei\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Jeongmin Park, Zaid Qureshi, Vikram Mailthody, Andrew Gacek, Shunfan Shao, Mohammad AlMasri, Isaac Gelado, Jinjun Xiong, Chris Newburn, I-hsin Chung, Michael Garland, Nikolay Sakharnykh, Wen-mei Hwu \n",
      " Download PDF \n",
      "Data compression and decompression have become vital components of big-data applications to manage the exponential growth in the amount of data collected and stored. Furthermore, big-data applications have increasingly adopted GPUs due to their high compute throughput and memory bandwidth. Prior works presume that decompression is memory-bound and have dedicated most of the GPU's threads to data movement and adopted complex software techniques to hide memory latency for reading compressed data and writing uncompressed data. This paper shows that these techniques lead to poor GPU resource utilization as most threads end up waiting for the few decoding threads, exposing compute and synchronization latencies.\n",
      " Based on this observation, we propose CODAG, a novel and simple ke\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "URL: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2308.05199v1.pdf\n",
      "Score: 0.353089839220047\n",
      "Published Date: 2023-08-09T00:00:00.000Z\n",
      "Author: Jiajun Huang,Sheng Di,Xiaodong Yu,Yujia Zhai,Jinyang Liu,Yafan Huang,Ken Raffenetti,Hui Zhou,Kai Zhao,Zizhong Chen,Franck Cappello,Yanfei Guo,Rajeev Thakur\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Jiajun Huang\n",
      "Sheng Di\n",
      "Xiaodong Yu\n",
      "Yujia Zhai\n",
      "Jinyang Liu\n",
      "Yafan Huang yafan-huang@uiowa.edu\n",
      "Ken Raffenetti raffenet@anl.gov\n",
      "Hui Zhou zhouh@anl.gov\n",
      "Kai Zhao kzhao@cs.fsu.edu\n",
      "Zizhong Chen chen@cs.ucr.edu\n",
      "Franck Cappello cappello@mcs.anl.gov\n",
      "Yanfei Guo yguo@anl.gov\n",
      "Rajeev Thakur thakur@anl.gov\n",
      "Argonne National Laboratory Lemont\n",
      "University of Iowa\n",
      "Iowa CityUnited States of America, United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "Argonne National Laboratory Lemont\n",
      "United States of America\n",
      "gZCCL: Compression-Accelerated Collective Communication Framework for GPU Clusters\n",
      "University of California, Riverside Riverside, United States of America Stevens Institute of Technology Hoboken, United States of America University of California, Riverside Riverside, United States of America Un\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about GPU-accelerated lossless compression for multi-byte data in high-performance computing and deep learning applications that could benefit from LZSS optimization techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.012\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.007}\n",
      "Downloaded candidate paper: 2304.07342v2.pdf\n",
      "Downloaded candidate paper: GitHub_-_hipdac-labICS23-GPULZ_GPULZ_Optimizing_LZSS_Lossless_Compression_for_Multi-byte_Data_on_Modern_GPUs.pdf\n",
      "Downloaded candidate paper: Light_Loss-Less_Data_Compression_with_GPU_Implementation.pdf\n",
      "Error downloading Adaptive loss‐less data compression method optimized for GPU decompression: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/10.1002/cpe.4283\n",
      "Downloaded candidate paper: Gorgan_BA_TCS.pdf\n",
      "Downloaded candidate paper: 2307.03760\n",
      "Downloaded candidate paper: 2308.05199v1.pdf\n",
      "Title: DZip: improved general-purpose lossless compression based on novel neural network modeling\n",
      "URL: https://arxiv.org/abs/1911.03572\n",
      "ID: https://arxiv.org/abs/1911.03572\n",
      "Score: 0.4118901491165161\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: We consider lossless compression based on statistical data modeling followed\n",
      "by prediction-based encoding, where an accurate statistical model for the input\n",
      "data leads to substantial improvements in compression. We propose DZip, a\n",
      "general-purpose compressor for sequential data that exploits the well-known\n",
      "modeling capabilities of neural networks (NNs) for prediction, followed by\n",
      "arithmetic coding. Dzip uses a novel hybrid architecture based on adaptive and\n",
      "semi-adaptive training. Unlike most NN based compressors, DZip does not require\n",
      "additional training data and is not restricted to specific data types, only\n",
      "needing the alphabet size of the input data. The proposed compressor\n",
      "outperforms general-purpose compressors such as Gzip (on average 26% reduction)\n",
      "on a variety of real datasets, achieves near-optimal compression on synthetic\n",
      "datasets, and performs close to specialized compressors for large sequence\n",
      "lengths, without any human input. The main\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/abs/1811.08162\n",
      "ID: https://arxiv.org/abs/1811.08162\n",
      "Score: 0.39063823223114014\n",
      "Published Date: 2023-02-06T00:00:00.000Z\n",
      "Author: Goyal; Mohit; Tatwawadi; Kedar; Chandak; Shubham; Ochoa; Idoia\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: Sequential data is being generated at an unprecedented pace in various forms,\n",
      "including text and genomic data. This creates the need for efficient\n",
      "compression mechanisms to enable better storage, transmission and processing of\n",
      "such data. To solve this problem, many of the existing compressors attempt to\n",
      "learn models for the data and perform prediction-based compression. Since\n",
      "neural networks are known as universal function approximators with the\n",
      "capability to learn arbitrarily complex mappings, and in practice show\n",
      "excellent performance in prediction tasks, we explore and devise methods to\n",
      "compress sequential data using neural network predictors. We combine recurrent\n",
      "neural network predictors with an arithmetic coder and losslessly compress a\n",
      "variety of synthetic, text and genomic datasets. The proposed compressor\n",
      "outperforms Gzip on the real datasets and achieves near-optimal compression for\n",
      "the synthetic datasets. The results also help understan\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: DeepZip: Lossless Data Compression Using Recurrent Neural Networks\n",
      "URL: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "ID: https://arxiv.org/pdf/1811.08162v1.pdf\n",
      "Score: 0.38720083236694336\n",
      "Published Date: 2019-03-01T00:00:00.000Z\n",
      "Author: Mohit Goyal,Kedar Tatwawadi,Shubham Chandak,Idoia Ochoa\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mohit Goyal goyal.mohit999@gmail.com \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "Kedar Tatwawadi kedart@stanford.edu \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Shubham Chandak \n",
      "Department of Electrical Engineering\n",
      "Stanford University\n",
      "CAUSA\n",
      "\n",
      "Idoia Ochoa \n",
      "Electrical and Computer Engineering\n",
      "University of Illinois\n",
      "UrbanaILUSA\n",
      "\n",
      "\n",
      "Department of Electrical Engineering\n",
      "Indian Institute of Technology\n",
      "DelhiIndia\n",
      "\n",
      "DeepZip: Lossless Data Compression using Recurrent Neural Networks\n",
      "\n",
      "Sequential data is being generated at an unprecedented pace in various forms, including text and genomic data. This creates the need for efficient compression mechanisms to enable better storage, transmission and processing of such data. To solve this problem, many of the existing compressors attempt to learn models for the data and perform predictionbased compression. Since neural networks are known as universal function approximators with the capability to learn arbit\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: \n",
      "URL: https://bellard.org/nncp/nncp.pdf\n",
      "ID: https://bellard.org/nncp/nncp.pdf\n",
      "Score: 0.37134554982185364\n",
      "Published Date: 2019-05-08T00:00:00.000Z\n",
      "Author: \n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Lossless Data Compression with Neural Networks\n",
      "Fabrice Bellard\n",
      "May 4, 2019\n",
      "Abstract\n",
      "We describe our implementation of a lossless data compressor using neu\u0002ral networks. We tuned Long Short-Term Memory and Transformer based\n",
      "models in order to achieve a fast training convergence. We evaluated the\n",
      "performance on the widely used enwik8 Hutter Prize benchmark.\n",
      "1 Introduction\n",
      "Although the best lossless data compressors are based on neural network models\n",
      "[7], they use them mostly to mix the statistics from a large number of hand coded\n",
      "models. Here we present lossless data compressors using pure neural network\n",
      "models based on Long Short-Term Memory (LSTM) and Transformer models.\n",
      "The lossless data compressor employs the traditional predictive approach: at\n",
      "each time t, the encoder uses the neural network model to compute the probability\n",
      "vector p of the next symbol value st knowing all the preceding symbols s0 up to\n",
      "st−1. The actual symbol value stis encoded using an arithmetic encoder with\n",
      "appro\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: TRACE: A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "URL: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16114v2.pdf\n",
      "Score: 0.37043696641921997\n",
      "Published Date: 2022-04-25T00:00:00.000Z\n",
      "Author: Yu Mao,Yufei Cui,Tei-Wei Kuo,Chun Jason Xue\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Yu Mao\n",
      "Yufei Cui\n",
      "Tei-Wei Kuo\n",
      "Chun Jason Xue\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science and Information Engineering\n",
      "City University of Hong\n",
      "Kong\n",
      "Department of Computer Science\n",
      "National Taiwan University\n",
      "City University of Hong\n",
      "Kong\n",
      "A Fast Transformer-based General-Purpose Lossless Compressor\n",
      "Deep-learning-based compressor has received interests recently due to much improved compression ratio. However, modern approaches suffer from long execution time. To ease this problem, this paper targets on cutting down the execution time of deep-learning-based compressors. Building historydependencies sequentially (e.g., recurrent neural networks) is responsible for long inference latency. Instead, we introduce transformer into deep learning compressors to build historydependencies in parallel. However, existing transformer is too heavy in computation and incomp\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.37147095799446106\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression\n",
      "URL: https://arxiv.org/abs/2412.16642\n",
      "ID: https://arxiv.org/abs/2412.16642\n",
      "Score: 0.37062251567840576\n",
      "Published Date: 2024-12-21T00:00:00.000Z\n",
      "Author: [Submitted on 21 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Computation and Language\n",
      "\n",
      "**arXiv:2412.16642** (cs)\n",
      "\n",
      "\\[Submitted on 21 Dec 2024\\]\n",
      "\n",
      "# Title:L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression\n",
      "\n",
      "Authors: [Junxuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Zhengxue Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng,+Z), [Yan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+Y), [Shihao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+S), [Dajiang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+D), [Guo Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+G), [Li Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+L)\n",
      "\n",
      "View a PDF of the paper titled L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression, by Junxuan Zhang and 6 other authors\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.16642) [HTML (experimental)](https://arxiv.org/html/2412.16642v1)\n",
      "\n",
      "> Abstract:Learning-based p\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: A Lossless Compression Technique for the Downlink Control Information Message\n",
      "URL: https://arxiv.org/abs/2407.16319\n",
      "ID: https://arxiv.org/abs/2407.16319\n",
      "Score: 0.37019482254981995\n",
      "Published Date: 2024-07-23T00:00:00.000Z\n",
      "Author: Liu; Bryan; Valcarce; Alvaro; Srinath; K Pavan\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "Improving the reliability and spectral efficiency of wireless systems is a key goal in wireless systems. However, most efforts have been devoted to improving data channel capacity, whereas control-plane capacity bottlenecks are often neglected. In this paper, we propose a means of improving the control-plane capacity and reliability by shrinking the bit size of a key signaling message - the 5G Downlink Control Information (DCI). In particular, a transformer model is studied as a probability distribution estimator for Arithmetic coding to achieve lossless compression. Feature engineering, neural model design, and training technique are comprehensively discussed in this paper. Both temporal and spatial correlations among DCI messages are explored by the transformer model to achieve reasonable lossless compression performance. Numerical results show that the proposed method achieves 21.7% higher compression ratio than Huffman coding in DCI compression for\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: LeCo: Lightweight Compression via Learning Serial Correlations\n",
      "URL: https://arxiv.org/abs/2306.15374v1\n",
      "ID: https://arxiv.org/abs/2306.15374v1\n",
      "Score: 0.3677283227443695\n",
      "Published Date: 2023-12-04T00:00:00.000Z\n",
      "Author: Liu; Yihao; Zeng; Xinyu; Zhang; Huanchen\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Download PDF \n",
      "Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance simultaneously. LeCo presents a general approach to this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and six real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Genetic Sequence compression using Machine Learning and Arithmetic Encoding Decoding Techniques\n",
      "URL: https://arxiv.org/pdf/2212.02864.pdf\n",
      "ID: https://arxiv.org/pdf/2212.02864.pdf\n",
      "Score: 0.38675594329833984\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: Md. Mehedi Hasan Sarkar,Adnan Ferdous Ashrafi\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Genetic Sequence compression using Machine Learning and Arithmetic Encoding Decoding Techniques\n",
      "\n",
      "\n",
      "Mehedi Hasan Sarkar mehedihasansarkar1899@gmail.com \n",
      "Department of CSE\n",
      "Department of CSE\n",
      "Stamford University Bangladesh Dhaka\n",
      "Bangladesh\n",
      "\n",
      "Adnan Ferdous \n",
      "Stamford University Bangladesh Dhaka\n",
      "Bangladesh\n",
      "\n",
      "Ashrafi \n",
      "Stamford University Bangladesh Dhaka\n",
      "Bangladesh\n",
      "\n",
      "Genetic Sequence compression using Machine Learning and Arithmetic Encoding Decoding Techniques\n",
      "Index Terms-genomecompressionmodified DeepDNADNA sequencingarithmetic compressionbio-informatics\n",
      "We live in a period where bio-informatics is rapidly expanding, a significant quantity of genomic data has been produced as a result of the advancement of high-throughput genome sequencing technology, raising concerns about the costs associated with data storage and transmission. The question of how to properly compress data from genomic sequences is still open. Previously many researcher proposed many compression method on this topic DNA Compre\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about general-purpose lossless compression for sequential data that could benefit from neural network modeling and arithmetic coding techniques\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Downloaded candidate paper: 1911.03572\n",
      "Downloaded candidate paper: 1811.08162\n",
      "Downloaded candidate paper: 1811.08162v1.pdf\n",
      "Downloaded candidate paper: nncp.pdf\n",
      "Downloaded candidate paper: 2203.16114v2.pdf\n",
      "Downloaded candidate paper: 2404.03626\n",
      "Downloaded candidate paper: 2412.16642\n",
      "Downloaded candidate paper: 2407.16319\n",
      "Downloaded candidate paper: 2306.15374v1\n",
      "Downloaded candidate paper: 2212.02864.pdf\n",
      "Title: Robust, practical and comprehensive analysis of soft compression image coding algorithms for big data\n",
      "URL: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "ID: https://www.nature.com/articles/s41598-023-29068-z.pdf?error=cookies_not_supported&code=178b1d74-aa88-4f1b-b01a-5ba756c506e8\n",
      "Score: 0.43025484681129456\n",
      "Published Date: 2023-02-02T00:00:00.000Z\n",
      "Author: Gangtao Xin\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: 1\n",
      "Vol.:(0123456789)\n",
      "Scientifc Reports | (2023) 13:1958 | https://doi.org/10.1038/s41598-023-29068-z\n",
      "www.nature.com/scientificreports\n",
      "Robust, practical \n",
      "and comprehensive analysis \n",
      "of soft compression image coding \n",
      "algorithms for big data\n",
      "Gangtao Xin1,2 & Pingyi Fan1,2*\n",
      "With the advancement of intelligent vision algorithms and devices, image reprocessing and secondary \n",
      "propagation are becoming increasingly prevalent. A large number of similar images are being \n",
      "produced rapidly and widely, resulting in the homogeneity and similarity of images. Moreover, it \n",
      "brings new challenges to compression systems, which need to exploit the potential of deep features \n",
      "and side information of images. However, traditional methods are incompetent for this issue. Soft \n",
      "compression is a novel data-driven image coding algorithm with superior performance. Compared \n",
      "with existing paradigms, it has distinctive characteristics: from hard to soft, from pixels to shapes, and \n",
      "from fxed to random. Soft compressio\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "URL: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "ID: https://arxiv.org/pdf/2112.13227v1.pdf\n",
      "Score: 0.41191521286964417\n",
      "Published Date: 2021-12-25T00:00:00.000Z\n",
      "Author: Mu Li,Kede Ma,Jinxing Li,David Zhang\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mu Li\n",
      "Member, IEEEKede Ma\n",
      "Jinxing Li\n",
      "Life Fellow, IEEEDavid Zhang\n",
      "Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression\n",
      "1Index Terms-Omnidirectional image compressionpseudocylindrical representationpseudocylindrical convolutionmap projection!\n",
      "Although equirectangular projection (ERP) is a convenient form to store omnidirectional images (also known as 360°i mages), it is neither equal-area nor conformal, thus not friendly to subsequent visual communication. In the context of image compression, ERP will over-sample and deform things and stuff near the poles, making it difficult for perceptually optimal bit allocation. In conventional 360°image compression, techniques such as region-wise packing and tiled representation are introduced to alleviate the over-sampling problem, achieving limited success. In this paper, we make one of the first attempts to learn deep neural networks for omnidirectional image compression. We first describe parametric pseudocylindrical rep\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Foveation-based Deep Video Compression without Motion Search\n",
      "URL: https://arxiv.org/pdf/2203.16490.pdf\n",
      "ID: https://arxiv.org/pdf/2203.16490.pdf\n",
      "Score: 0.41032645106315613\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: Meixu Chen,R. Webb,A. Bovik\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Foveation-based Deep Video Compression without Motion Search\n",
      "\n",
      "\n",
      "Student Member, IEEEMeixu Chen \n",
      "Richard Webb \n",
      "Fellow, IEEEAlan C Bovik \n",
      "Foveation-based Deep Video Compression without Motion Search\n",
      "1Index Terms-Foveationmotionvideo compression\n",
      "Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing, and displaying high-quality VR content. At the same time, the great potential of deep learning to advance progress on the video compression problem has driven a significant research effort. Because of the high bandwidth requirements of VR, there has also been significant interest in the use of space-variant, foveated compression protocols. We have integrated these techniques to create an endto-end deep learning video compression framework. A feature of our new com\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "URL: https://arxiv.org/pdf/2309.11661.pdf\n",
      "ID: https://arxiv.org/pdf/2309.11661.pdf\n",
      "Score: 0.4045531749725342\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Wei  Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com, Wei  Wang, Futurewei Technologies Inc, Santa Clara, CA, Yue  Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Neural Image Compression Using Masked Sparse Visual Representation\n",
      "\n",
      "Date Published: 2023-09-20\n",
      "\n",
      "Authors:\n",
      "Wei Jiang, Futurewei Technologies Inc, Santa Clara, CA, wjiang@futurewei.com\n",
      "Wei Wang, Futurewei Technologies Inc, Santa Clara, CA\n",
      "Yue Chen, Futurewei Technologies Inc, Santa Clara, CA, ychen@futurewei.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent fe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model\n",
      "URL: https://arxiv.org/abs/2402.16749\n",
      "ID: https://arxiv.org/abs/2402.16749\n",
      "Score: 0.4044540226459503\n",
      "Published Date: 2024-02-26T00:00:00.000Z\n",
      "Author: Li; Chunyi; Lu; Guo; Feng; Donghui; Wu; Haoning; Zhang; Zicheng; Xiaohong; Zhai; Guangtao; Weisi; Wenjun\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " Download PDF \n",
      " HTML (experimental) \n",
      "With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Multi-Realism Image Compression with a Conditional Generator\n",
      "URL: https://arxiv.org/pdf/2212.13824.pdf\n",
      "ID: https://arxiv.org/pdf/2212.13824.pdf\n",
      "Score: 0.4043355882167816\n",
      "Published Date: 2023-11-16T00:00:00.000Z\n",
      "Author: Eirikur  Agustsson, eirikur@google.com, Google  Research, David  Minnen, dminnen@google.com, George  Toderici, gtoderici@google.com, Fabian  Mentzer, mentzer@google.com\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Multi-Realism Image Compression with a Conditional Generator\n",
      "\n",
      "Date Published: 2023-03-30\n",
      "\n",
      "Authors:\n",
      "Eirikur Agustsson, Google Research Reykjavík, Iceland, eirikur@google.com\n",
      "David Minnen, Google Research Mountain View, USA, dminnen@google.com\n",
      "George Toderici, Google Research Mountain View, USA, gtoderici@google.com\n",
      "Fabian Mentzer, mentzer@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed represe\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "URL: https://arxiv.org/pdf/2303.03028.pdf\n",
      "ID: https://arxiv.org/pdf/2303.03028.pdf\n",
      "Score: 0.40360027551651\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: B. Damodaran,M. Balcilar,Franck Galpin,P. Hellier\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "\n",
      "Bharath Bhushan Damodaran bharath.damodaran@interdigital.com \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Muhammet Balcilar \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Franck Galpin \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "Pierre Hellier \n",
      "InterDigital, Inc\n",
      "RennesFrance\n",
      "\n",
      "RQAT-INR: Improved Implicit Neural Image Compression\n",
      "\n",
      "Deep variational autoencoders for image and video compression have gained significant attraction in the recent years, due to their potential to offer competitive or better compression rates compared to the decades long traditional codecs such as AVC, HEVC or VVC. However, because of complexity and energy consumption, these approaches are still far away from practical usage in industry. More recently, implicit neural representation (INR) based codecs have emerged, and have lower complexity and energy usage to classical approaches at decoding. However, their performances are not in par at the moment with state-of-the-art methods. In this researc\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "URL: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2309.02529v1.pdf\n",
      "Score: 0.3996758460998535\n",
      "Published Date: 2023-09-05T00:00:00.000Z\n",
      "Author: Haisheng Fu,Feng Li,Jie Liang,Yongqiang Wang,Guohe Zhang,Jingning Han\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Haisheng Fu\n",
      "Feng Liang\n",
      "Jie Liang\n",
      "Yongqiang Wang\n",
      "Guohe Zhang\n",
      "Jingning Han\n",
      "Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation\n",
      "SUBMITTED TO TRANS. JOURNAL 1\n",
      "Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the ratedistortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the tradeoff between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design an improved checkerboard context model with two separate distribution parameter estimati\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Selective compression learning of latent representations for variable-rate image compression\n",
      "URL: https://arxiv.org/pdf/2211.04104.pdf\n",
      "ID: https://arxiv.org/pdf/2211.04104.pdf\n",
      "Score: 0.3975396454334259\n",
      "Published Date: 2023-10-26T00:00:00.000Z\n",
      "Author: Jooyoung Lee,S. Jeong,Munchurl Kim\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Selective compression learning of latent representations for variable-rate image compression\n",
      "\n",
      "\n",
      "Jooyoung Lee \n",
      "School of Electrical Engineering\n",
      "Korea Advanced Institute of Science and Technology\n",
      "Korea\n",
      "\n",
      "Electronics and Telecommunications Research Institute\n",
      "Korea\n",
      "\n",
      "Seyoon Jeong \n",
      "Electronics and Telecommunications Research Institute\n",
      "Korea\n",
      "\n",
      "Munchurl Kim \n",
      "School of Electrical Engineering\n",
      "Korea Advanced Institute of Science and Technology\n",
      "Korea\n",
      "\n",
      "Selective compression learning of latent representations for variable-rate image compression\n",
      "\n",
      "Recently, many neural network-based image compression methods have shown promising results superior to the existing tool-based conventional codecs. However, most of them are often trained as separate models for different target bit rates, thus increasing the model complexity. Therefore, several studies have been conducted for learned compression that supports variable rates with single models, but they require additional network modules, layers, or inputs that \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression\n",
      "URL: https://export.arxiv.org/pdf/2303.05744v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2303.05744v1.pdf\n",
      "Score: 0.39397791028022766\n",
      "Published Date: 2023-03-10T00:00:00.000Z\n",
      "Author: Kedeng Tong,Yan Wu,Yue Li,Kai Zhang,Li Zhang,Xin Jin\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Kedeng Tong \n",
      "Shenzhen International Graduate School\n",
      "Tsinghua University\n",
      "518055ShenzhenChina\n",
      "\n",
      "Yaojun Wu \n",
      "Multimedia Lab\n",
      "Bytedance Inc\n",
      "92122San DiegoCAUSA\n",
      "\n",
      "Yue Li \n",
      "Multimedia Lab\n",
      "Bytedance Inc\n",
      "92122San DiegoCAUSA\n",
      "\n",
      "Kai Zhang \n",
      "Multimedia Lab\n",
      "Bytedance Inc\n",
      "92122San DiegoCAUSA\n",
      "\n",
      "Li Zhang \n",
      "Xin Jin \n",
      "Shenzhen International Graduate School\n",
      "Tsinghua University\n",
      "518055ShenzhenChina\n",
      "\n",
      "Multimedia Lab\n",
      "Bytedance Inc\n",
      "92122San DiegoCAUSA\n",
      "\n",
      "QVRF: A QUANTIZATION-ERROR-AWARE VARIABLE RATE FRAMEWORK FOR LEARNED IMAGE COMPRESSION\n",
      "* The work of Kedeng Tong is conducted during his internship at Bytedance Inc.Index Terms-learned image compressionvariable ratequantization regulatorentropy coding\n",
      "Learned image compression has exhibited promising compression performance, but variable bitrates over a wide range remain a challenge. State-of-the-art variable rate methods compromise the loss of model performance and require numerous additional parameters. In this paper, we present a Quantization-error-aware Variable Rate \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about image compression techniques for virtual reality, augmented reality, and metaverse applications that could benefit from soft compression algorithms with superior compression ratios\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Downloaded candidate paper: s41598-023-29068-z.pdf\n",
      "Downloaded candidate paper: 2112.13227v1.pdf\n",
      "Downloaded candidate paper: 2203.16490.pdf\n",
      "Downloaded candidate paper: 2309.11661.pdf\n",
      "Downloaded candidate paper: 2402.16749\n",
      "Downloaded candidate paper: 2212.13824.pdf\n",
      "Downloaded candidate paper: 2303.03028.pdf\n",
      "Downloaded candidate paper: 2309.02529v1.pdf\n",
      "Downloaded candidate paper: 2211.04104.pdf\n",
      "Downloaded candidate paper: 2303.05744v1.pdf\n",
      "Title: An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "URL: https://arxiv.org/abs/2412.15250\n",
      "ID: https://arxiv.org/abs/2412.15250\n",
      "Score: 0.3578655421733856\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Computation and Language\n",
      "\n",
      "**arXiv:2412.15250** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:An Enhanced Text Compression Approach Using Transformer-based Language Models\n",
      "\n",
      "Authors: [Chowdhury Mofizur Rahman](https://arxiv.org/search/cs?searchtype=author&query=Rahman,+C+M), [Mahbub E Sobhani](https://arxiv.org/search/cs?searchtype=author&query=Sobhani,+M+E), [Anika Tasnim Rodela](https://arxiv.org/search/cs?searchtype=author&query=Rodela,+A+T), [Swakkhar Shatabda](https://arxiv.org/search/cs?searchtype=author&query=Shatabda,+S)\n",
      "\n",
      "View a PDF of the paper titled An Enhanced Text Compression Approach Using Transformer-based Language Models, by Chowdhury Mofizur Rahman and Mahbub E Sobhani and Anika Tasnim Rodela and Swakkhar Shatabda\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.15250) [HTML (experimental)](https://arxiv.org/html/2412.15250v1)\n",
      "\n",
      "> Abstract:Text compression shrinks textual data while keeping crucial information, eradicating constraints on storage, bandwidth, and c\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs\n",
      "URL: https://arxiv.org/abs/2406.02376\n",
      "ID: https://arxiv.org/abs/2406.02376\n",
      "Score: 0.320854514837265\n",
      "Published Date: 2024-06-04T00:00:00.000Z\n",
      "Author: Cao; Zhiwei; Qian; Lu; Yu; Peng; Ningxin; Huang; Cheng; Shanbo; Su; Jinsong\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Training LLMs over Neurally Compressed Text\n",
      "URL: https://arxiv.org/abs/2404.03626\n",
      "ID: https://arxiv.org/abs/2404.03626\n",
      "Score: 0.3218926787376404\n",
      "Published Date: 2024-04-04T00:00:00.000Z\n",
      "Author: Lester; Brian; Lee; Jaehoon; Alemi; Alex; Pennington; Jeffrey; Roberts; Adam; Sohl-Dickstein; Jascha; Constant; Noah\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " View PDF \n",
      " HTML (experimental) \n",
      "In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text naïvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level basel\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Extending Context Window of Large Language Models via Semantic Compression\n",
      "URL: https://browse.arxiv.org/html/2312.09571v1\n",
      "ID: https://browse.arxiv.org/html/2312.09571v1\n",
      "Score: 0.3260715901851654\n",
      "Published Date: 2023-12-15T00:00:00.000Z\n",
      "Author: Correspondence to: niuxueyan3@huawei.com.\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: License: CC BY-NC-SA 4.0\n",
      "\n",
      "arXiv:2312.09571v1 \\[cs.CL\\] 15 Dec 2023\n",
      "\n",
      "# Extending Context Window of Large Language Models via Semantic Compression\n",
      "\n",
      "Weizhi Fei††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT & Xueyan Niu  ‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT\n",
      "\n",
      "††{}^{\\\\dagger}start\\_FLOATSUPERSCRIPT † end\\_FLOATSUPERSCRIPT Department of Mathematical Sciences, Tsinghua University, Beijing, China\n",
      "\n",
      "‡‡{}^{\\\\ddagger}start\\_FLOATSUPERSCRIPT ‡ end\\_FLOATSUPERSCRIPT Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.\n",
      "\n",
      "\\\\ANDPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han\n",
      "\n",
      "Huawei Technologies Co., Ltd\n",
      "Correspondence to: niuxueyan3@huawei.com.\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "URL: https://arxiv.org/abs/2412.12204\n",
      "ID: https://arxiv.org/abs/2412.12204\n",
      "Score: 0.33777424693107605\n",
      "Published Date: 2024-12-15T00:00:00.000Z\n",
      "Author: [Submitted on 15 Dec 2024]\n",
      "Image: /static/browse/0.3.4/images/arxiv-logo-fb.png\n",
      "Favicon: https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: # Computer Science > Machine Learning\n",
      "\n",
      "**arXiv:2412.12204** (cs)\n",
      "\n",
      "\\[Submitted on 15 Dec 2024\\]\n",
      "\n",
      "# Title:SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression\n",
      "\n",
      "Authors: [Jing Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Shuzhen Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+S), [Peng Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+P), [Guangxing Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao,+G), [Hui Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+H), [Xindian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+X), [Nan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+N), [Yuexian Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou,+Y)\n",
      "\n",
      "View a PDF of the paper titled SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression, by Jing Zhang and 7 other authors\n",
      "\n",
      "[View PDF](https://arxiv.org/pdf/2412.12204) [HTML (experimental)](https://ar\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Compressing Sentence Representation with Maximum Coding Rate Reduction\n",
      "URL: https://export.arxiv.org/pdf/2304.12674v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.12674v1.pdf\n",
      "Score: 0.32669198513031006\n",
      "Published Date: 2023-04-25T00:00:00.000Z\n",
      "Author: Domagoj Ševerdija,Tomislav Prusina,Antonio Jovanović,Luka Borozan,Jurica Maltar,Domagoj Matijevic\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Domagoj Ševerdija\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Tomislav Prusina\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Antonio Jovanović\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Luka Borozan\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Jurica Maltar\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Domagoj Matijević\n",
      "Department of Mathematics\n",
      "University J. J. Strossmayer of Osijek\n",
      "Croatia\n",
      "Compressing Sentence Representation with Maximum Coding Rate Reduction\n",
      "April 26, 2023Sentence embeddingsmodel distillationMaximum Coding Rate Reduc- tionsemantic retrieval\n",
      "In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Crossword: A Semantic Approach to Data Compression via Masking\n",
      "URL: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2304.01106v1.pdf\n",
      "Score: 0.35012781620025635\n",
      "Published Date: 2023-04-03T00:00:00.000Z\n",
      "Author: Mingxiao Li,Rui Jin,Liyao Xiang,Kaiming Shen,Shuguang Cui\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Mingxiao Li mingxiaoli@link.cuhk.edu.cn \n",
      "† \n",
      "Rui Jin ruijin@link.cuhk.edu.cn \n",
      "Liyao Xiang xiangliyao08@sjtu.edu.cn \n",
      "Shanghai Jiao Tong University\n",
      "China\n",
      "\n",
      "Kaiming Shen shenkaiming@cuhk.edu.cn \n",
      "Shuguang Cui shuguangcui@cuhk.edu.cn \n",
      "\n",
      "School of Science and Engineering\n",
      "FNii\n",
      "The Chinese University of Hong Kong (Shenzhen)\n",
      "China\n",
      "\n",
      "Crossword: A Semantic Approach to Data Compression via Masking\n",
      "3 Apr 2023Index Terms-Semantic source codingdata compressionword maskingcosine distanceTransformer\n",
      "The traditional methods for data compression are typically based on the symbol-level statistics, with the information source modeled as a long sequence of i.i.d. random variables or a stochastic process, thus establishing the fundamental limit as entropy for lossless compression and as mutual information for lossy compression. However, the source (including text, music, and speech) in the real world is often statistically ill-defined because of its close connection to human perception, and thus the model-driven\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Text Compression-Aided Transformer Encoding\n",
      "URL: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "ID: https://arxiv.org/pdf/2102.05951v1.pdf\n",
      "Score: 0.36277955770492554\n",
      "Published Date: 2021-01-01T00:00:00.000Z\n",
      "Author: Zuchao Li,Zhuosheng Zhang,Hai Zhao,Rui Wang,Kehai Chen,Masao Utiyama,Eiichiro Sumita\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Zuchao Li \n",
      "Zhuosheng Zhang \n",
      "Hai Zhao \n",
      "Rui Wang \n",
      "Kehai Chen \n",
      "Masao Utiyama \n",
      "Eiichiro Sumita \n",
      "Text Compression-aided Transformer Encoding\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, MARCH 2020 1 !Index Terms-Natural Language ProcessingText CompressionTransformer EncodingNeural Machine TranslationMachine Reading Comprehension\n",
      "Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical \n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Semantic Compression With Large Language Models\n",
      "URL: https://arxiv.org/abs/2304.12512\n",
      "ID: https://arxiv.org/abs/2304.12512\n",
      "Score: 0.36365997791290283\n",
      "Published Date: 2023-05-16T00:00:00.000Z\n",
      "Author: Gilbert; Henry; Sandborn; Michael; Schmidt; Douglas C; Spencer-Smith; Jesse; White; Jules\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Download PDF \n",
      " Abstract: The rise of large language models (LLMs) is revolutionizing information\n",
      "retrieval, question answering, summarization, and code generation tasks.\n",
      "However, in addition to confidently presenting factually inaccurate information\n",
      "at times (known as \"hallucinations\"), LLMs are also inherently limited by the\n",
      "number of input and output tokens that can be processed at once, making them\n",
      "potentially less effective on tasks that require processing a large set or\n",
      "continuous stream of information. A common approach to reducing the size of\n",
      "data is through lossless or lossy compression. Yet, in some cases it may not be\n",
      "strictly necessary to perfectly recover every detail from the original data, as\n",
      "long as a requisite level of semantic precision or intent is conveyed.\n",
      " This paper presents three contributions to research on LLMs. First, we\n",
      "present the results from experiments exploring the viability of approximate\n",
      "compression using LLMs, focusing specifically\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Title: Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "URL: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "ID: https://export.arxiv.org/pdf/2307.12266v1.pdf\n",
      "Score: 0.3293420374393463\n",
      "Published Date: 2023-07-23T00:00:00.000Z\n",
      "Author: Shicong Liu,Zhen Gao,Gaojie Chen,Yu Shibata,Peixiang Lu\n",
      "Image: None\n",
      "Favicon: None\n",
      "Extras: None\n",
      "Subpages: None\n",
      "Text: Shicong Liu scliu@bit.edu.cn\n",
      "Zhen Gao gaozhen16@bit.edu.cn\n",
      "Advanced Research Institute of Multidisciplinary Science\n",
      "Beijing Institute of Technology\n",
      "250307JinanChina\n",
      "Yangtze Delta Region Academy of Beijing Institute of Technology\n",
      "Beijing Institute of Technology\n",
      "314000JiaxingChina\n",
      "† ‡\n",
      "Gaojie Chen\n",
      "Institute for Communication Systems\n",
      "University of Surrey\n",
      "GuildfordUnited Kingdom\n",
      "Yu Su\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "¶\n",
      "Lu Peng\n",
      "China Mobile (Chengdu) Institute of Research and Development\n",
      "610000ChengduSichuanChina\n",
      "Field Intelligent Sensing\n",
      "MIIT Key Laboratory of Complex\n",
      "Beiing Institute of Technology\n",
      "100081BeijingChina\n",
      "Transformer-based Joint Source Channel Coding for Textual Semantic Communication\n",
      "Index Terms-Semantic communciationtransformerjoint source channel codingpretrained language model\n",
      "The Space-Air-Ground-Sea integrated network calls for more robust and secure transmission techniques against jamming. In this paper, we propose a t\n",
      "Highlights: None\n",
      "Highlight Scores: None\n",
      "Summary: None\n",
      "\n",
      "\n",
      "Autoprompt String: research papers about semantic text compression techniques that could benefit from BERT and Transformer models for preserving meaning in literature and political documents\n",
      "Resolved Search Type: neural\n",
      "CostDollars: total=0.015\n",
      "  - search: {'neural': 0.005}\n",
      "  - contents: {'text': 0.01}\n",
      "Downloaded candidate paper: 2412.15250\n",
      "Downloaded candidate paper: 2406.02376\n",
      "Downloaded candidate paper: 2404.03626\n",
      "Error downloading Extending Context Window of Large Language Models via Semantic Compression: HTTPSConnectionPool(host='browse.arxiv.org', port=443): Max retries exceeded with url: /html/2312.09571v1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)')))\n",
      "Downloaded candidate paper: 2412.12204\n",
      "Downloaded candidate paper: 2304.12674v1.pdf\n",
      "Downloaded candidate paper: 2304.01106v1.pdf\n",
      "Downloaded candidate paper: 2102.05951v1.pdf\n",
      "Downloaded candidate paper: 2304.12512\n",
      "Downloaded candidate paper: 2307.12266v1.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from exa_py import Exa\n",
    "\n",
    "def search_papers_with_exa(query_path):\n",
    "    with open(query_path, 'r', encoding='utf-8') as f:\n",
    "        query = f.read().strip()\n",
    "    \n",
    "    # Use exa to search for papers\n",
    "    try:\n",
    "        result = exa.search_and_contents(\n",
    "            query,\n",
    "            text = { \"max_characters\": 1000 },\n",
    "            category = \"research paper\",\n",
    "            num_results = 10,\n",
    "            start_published_date = \"2015-01-01T08:00:00.000Z\",\n",
    "            end_published_date = \"2025-05-25T06:59:59.999Z\"\n",
    "        )\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching with exa: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Process each query.txt file\n",
    "papers_dir = 'paper_analysis'\n",
    "for paper_folder in os.listdir(papers_dir):\n",
    "    query_path = os.path.join(papers_dir, paper_folder, 'query.txt')\n",
    "    if os.path.exists(query_path):\n",
    "        print(f\"\\nSearching for papers related to: {paper_folder}\")\n",
    "        results = search_papers_with_exa(query_path)\n",
    "        if results:\n",
    "            print(\"Search results:\")\n",
    "            print(results)\n",
    "\n",
    "\n",
    "def download_candidate_papers(results, paper_folder):\n",
    "    # Create candidates directory if it doesn't exist\n",
    "    candidates_dir = os.path.join('paper_analysis', paper_folder, 'candidates')\n",
    "    if not os.path.exists(candidates_dir):\n",
    "        os.makedirs(candidates_dir)\n",
    "        \n",
    "    for paper in results.results:\n",
    "        url = paper.url\n",
    "        title = paper.title\n",
    "        \n",
    "        try:\n",
    "            # Download the paper\n",
    "            import requests\n",
    "            from urllib.parse import urlparse\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get filename from URL or use title if URL has no filename\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            if not filename or '.' not in filename:\n",
    "                # Clean title and add .pdf extension\n",
    "                filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_'))\n",
    "                filename = filename.strip().replace(' ', '_') + '.pdf'\n",
    "                \n",
    "            filepath = os.path.join(candidates_dir, filename)\n",
    "            \n",
    "            # Actually save the downloaded content to file\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                \n",
    "            print(f\"Downloaded candidate paper: {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {title}: {str(e)}\")\n",
    "\n",
    "# Call the function for each paper folder\n",
    "for paper_folder in os.listdir('paper_analysis'):\n",
    "    query_path = os.path.join('paper_analysis', paper_folder, 'query.txt')\n",
    "    if os.path.exists(query_path):\n",
    "        results = search_papers_with_exa(query_path)\n",
    "        if results:\n",
    "            download_candidate_papers(results, paper_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
